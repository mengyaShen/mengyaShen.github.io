<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Computer Vision | Introduction]]></title>
    <url>%2Fed318fdc%2F</url>
    <content type="text"><![CDATA[一、计算视觉的经典问题Reconstruction——重建Recognition——识别(Re)organization——重组 Optical character recognition (OCR) 光学字符识别（OCR）Technology to convert scanned docs to text 扫描文档转换为文本的技术 Face detection 人脸检测 Smile detection 微笑检测 Object recognition (in supermarkets) 物体识别（超市） Vision-based biometrics 基于视觉的生物特征识别 Login without a password 无密码登录 Object recognition (in mobile phones) 物体识别（在移动电话中） 3D from images 三维图像 Human shape capture 人形捕捉 Special effects: shape capture 特殊效果：形状捕捉 Interactive Games: Kinect 互动游戏：Kinect Sports 体育运动 Medical imaging 医学影像学 AutoCars - Uber bought CMU’s lab 汽车制造商-Uber收购了CMU的实验室 Industrial robots 工业机器人 Vision in space 太空视野Vision systems (JPL) used for several tasks 用于多个任务的视觉系统（JPL） Mobile robots 移动机器人 Augmented Reality and Virtual Reality 增强现实与虚拟现实 二、计算机视觉及邻近领域 计算机视觉的贬义总结：机器学习在可视化数据中的应用。 图像处理、识别、深度学习、几何推理机器学习、图形学、计算摄影、光学机器人学、人机交互、医学影像学、神经科学 三、课程主题1.解释强度什么决定了像素的亮度和颜色？如何使用图像过滤器从图像中提取有意义的信息？ 2.对应和对齐如何在物体或场景中找到对应的点？我们如何估计它们之间的转换？ 3.分组和分段如何将像素分组成有意义的区域？ 4.分类与目标识别我们如何表现图像并对其进行分类？我们如何识别物体的类别？ 5.高级主题动作识别，三维场景和上下文，人在环视觉 6.相关知识线性代数、概率、图形课程、视觉/图像处理课程、机器学习 四、项目Projects 1-5: Structured conceptual / code——结构化概念/代码Project 6: Group challenge——团体挑战 Proj 1: Image Filtering and Hybrid Images——图像滤波和混合图像实现图像滤波以分离高频和低频。将来自不同图像的高频和低频合并以创建与比例相关的图像。 Proj 2: Local Feature Matching——局部特征匹配实现兴趣点检测、类sift局部特征描述、简单匹配算法。 Proj 3: Scene Recognition with Bag of Words——文字袋场景识别将局部特征量化为“词汇”，将图像描述为“视觉词汇”的直方图，训练分类器根据这些直方图识别场景。 Proj 3b: Object Detection with a Sliding Window——滑动窗口目标检测训练一个基于正样本和“挖掘”硬底片的人脸检测器，在多个尺度上检测人脸并抑制重复检测。 Proj 4: Convolutional Neural Nets——卷积神经网络Proj 5: Multi-view Geometry——多视图几何图形从特征点匹配恢复相机校准。计算机视觉中几乎所有测量的基础。 Proj 6: Group challenge——团体挑战改进webgazer：一个基于web的实时眼睛跟踪器。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战 | 第3章 决策树]]></title>
    <url>%2F668be595%2F</url>
    <content type="text"><![CDATA[决策树的一个重要任务是：为了数据中蕴含的知识信息。决策树可以使用不熟悉的数据集合，并从中提取除一系列规则，在这些机器根据数据集创建规则时，就是机器学习的过程。 3.1 决策树的构造 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点：可能会产生过度匹配问题。适用数据类型：数值型和标称型。 在构造决策树时，我们需要解决的第一个问题是：当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某分支上的数据属于同一类型，无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集。直到所有具有相同数据类型的数据均在一个数据子集内。 创建分支的伪代码函数createBranch()： 决策树的一般流程：(1) 收集数据：可以使用任何方法。(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。(4) 训练算法：构造树的数据结构。(5) 测试算法：使用经验树计算错误率。(6) 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。 3.3.1 信息增益 划分数据集的大原则：将无序的数据变得更加有序。 信息增益：划分数据集之前之后信息发生的变化。获得信息增益最高的特征就是最好的选择。 香农熵（熵）：集合信息的度量方式。 熵：定义为信息的期望值。 信息：符号xi的信息定义：其中p(xi)是选择该分类的概率。 信息熵：其中n是分类的数目。 基尼不纯度：度量集合无序程度的方法。简单的说就是从一个数据集中随机选取子项，度量其被错误分类到其他组里的概率。 3.1.2 划分数据集 使用ID3算法划分数据集。三组参数：待划分的数据集、划分数据集的特征、需要返回的特征的值。3.1.3 递归构建决策树 工作原理：得到原始数据集，然后基于最好的属性值划分数据集（由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分）。第一次划分之后，数据被向下传递到树分支的下一个节点，在这个节点上，再次划分数据。递归进行。递归结束的条件：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子节点或者终止块。]]></content>
      <categories>
        <category>机器学习实战学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战 | 第2章 k-临近算法]]></title>
    <url>%2F5f6e73d7%2F</url>
    <content type="text"><![CDATA[工作原理：存在一个样本数据集合（也称作训练样本集），并且样本集中每个数据都存在标签（即我们知道样本中每一数据与所属分类的对应关系）。输入没有标签的新数据后，将新数据的特征值与样本中数据对应的特征值进行比较，然后算法提取样本集中特征最相似数据（最邻近）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 kNN算法伪代码：对未知类别属性的数据集中的每个点依次执行以下操作：（1）计算已知类别数据集中的点与当前点之间的距离；（2）按照距离递增次序排序；（3）选取与当前点距离最小的k个点；（4）确定前k个点所在类别的出现频率；（5）返回前k个点出现频率最高的类别作为当前点的预测分类。]]></content>
      <categories>
        <category>机器学习实战学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西瓜书学习笔记 | 第16章 强化学习]]></title>
    <url>%2Fd6bdaab8%2F</url>
    <content type="text"><![CDATA[16.1 任务与奖赏 强化学习（再励学习）：在种瓜过程中不断摸索，总结出较好种瓜策略的抽象过程。 强化学习任务通常用马尔可夫决策过程（Markov Decision Process,简称MDP)来描述：（1）机器处于环境E中，==状态空间为X==，其中每个状态x∈X是机器感知到的环境的描述。如在种瓜任务上这就是当前瓜苗长势的描述；（2）机器能采取的动作构成了动作空间A。如种瓜过程中有浇水、施不同的肥、使用不同的农药等多种可供选择的动作；（3）若某个==动作a==∈A作用在当前状态x上，则潜在的==转移函数P==将使得环境从当前状态按某种概率转移到另一个状态。如瓜苗状态为缺水，若选择动作浇水，则瓜苗长势会发生变化，瓜苗有一定的概率恢复健康，也有一定的概率无法恢复；（4）在转移到另一个状态的同时，环境会根据潜在的==“奖赏”(reward)函数R==反馈给机器一个奖赏。如保持瓜苗健康对应奖赏+1，瓜苗凋零对应奖赏-10，最终种出了好瓜对应奖赏+100。综合起来，强化学习任务对应了四元组E=&lt;X,A,P, R&gt;，其中P:X x AxX|→IR指定了状态转移概率，R:X xAxX|→IR指定了奖赏；在有的应用中，奖赏函数可能仅与状态转移有关，即R:XxX|→IR。 给西瓜浇水问题的马尔可夫决策过程：a：动作p：转移概率r：返回的奖赏 机器要做的是通过在环境中不断地尝试而学得一个“策略”(policy)π，根据这个策略，在状态x下就能得知要执行的动作a=π(x)。策略有两种表示方法：（1）表示为函数π:X |→A，确定性策略常用这种表示；（2）概率表示π:XxA|→IR，随机性策略常用这种表示，π(x, a)为状态x下选择动作a的概率，这里必须有策略的优势取决于长期执行这一策略后得到的累积奖赏。 强化学习中，学习的目的：找到能使长期积累奖赏最大化的策略。 长期积累奖赏计算方式：（1）T步累计奖赏：（2）γ折扣累计奖赏：其中rt表示第t步获得的奖赏值，IE表示对所有随机变量求期望。 强化学习与监督学习：强化学习中的“状态”对应为监督学习中的“示例”、“动作” 对应为“标记”，“策略”对应为“分类器”(当动作是离散的)或“回归器”(当动作是连续的)。不同：在强化学习中并没有监督学习中的有标记样本(即“示例-标记”对)。换言之，没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过“反思”之前的动作是否正确来进行学习。强化学习在某种意义上可看作具有“延迟标记信息”的监督学习问题。16.2 K-摇臂赌博机16.2.1 探索与利用 最大化单步奖赏，即仅考虑一步操作。 欲最大化单步奖赏需考虑两个方面：（1）需知道每个动作带来的奖赏，（2）要执行奖赏最大的动作。 K-摇臂赌博机(K-armed bandit)：单步强化学习任务对应的一个理论模型。 若仅为获知每个摇臂的期望奖赏，则可采用“仅探索”(exploration-only)法：将所有的尝试机会平均分配给每个摇臂(即轮流按下每个摇臂)，最后以每个摇壁各白的平均叶币概率作为其奖常期望的近似估计。若仅为执行奖常最大的动作，则可采用“仅利用“（exploitation-only）法：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。“仅探索”法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；“仅利用”法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。 16.2.2 ∈-贪心 ∈-贪心法基于一个概率来对探索和利用进行折中：每次尝试时，以∈的概率进行探索，即以均匀概率随机选取一个摇臂；以1 -∈的概率进行利用，即选择当前平均奖赏最高的摇臂(若有多个，则随机选取一个)。 令Q(k)记录摇臂k的平均奖赏：若摇臂k被尝试了n次，得到的奖赏为16.2.3 Softmax 16.3 有模型学习16.3.1 策略评估16.3.2 策略改进16.3.3 策略迭代与值迭代16.4 免模型学习16.4.1 蒙特卡罗强化学习16.4.2 时序差分学习16.5 值函数近似16.6 模仿学习16.6.1 直接模仿学习16.6.2 逆强化学习]]></content>
      <categories>
        <category>西瓜书学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西瓜书学习笔记 | 第15章 规则学习]]></title>
    <url>%2Fa75704a9%2F</url>
    <content type="text"><![CDATA[15.1 基本概念 规则：机器学习中通常指语义明确、能描述数据分布所隐含的客观规律或领域概念、可写成“若……，则……”形式的逻辑规则。 规则学习（rule leaning）：是从训练数据中学习出一组能用来对未见实例进行判别的规则。 形式化地看，一条规则形如： 规则学习的优势：（1）与神经网络、支持向量机这样的“黑箱模型”相比，规则学习具有更好的可解释性，能使用户更直观地对判别过程有所了解；（2）规则学习能更自然地在学习过程中引入领域知识；（3）逻辑规则的抽象描述能力在处理一些高度复杂的AI任务时具有显著的优势。 覆盖：符合规则的样本称为被该规则覆盖。 冲突：规则集合中的每条规则都可看作一一个子模型，规则集合是这些子模型的一个集成。当同一个示例被判别结果不同的多条规则覆盖时，称发生了“冲突”(confict)，解决冲突的办法称为“冲突消解”(conflict resolution).。 常用的冲突消解策略：（1）投票法：将判别相同的规则数最多的结果作为最终结果。（2）排序法：在规则集合上定义一个顺序，在发生冲突时使用排序最前的规则，相应的规则学习过程称为“带序规则”(orderedrule)学习或“优先级规则”(priority rule)学习。（3）元规则法：根据领域知识事先设定-些“元规则”(meta-rule),，即关于规则的规则，例如“发生冲突时使用长度最小的规则”，然后根据元规则的指导来使用规则集。 规则学习算法通常会设置一条“默认规则”(default rule)，由它来处理规则集合未覆盖的样本。 从形式语言表达能力而言，规则可分为两类：（1）“命题规则“(propositionalrule)：由“原子命题”(propositionalatom)和逻辑连接词“与”(∧)、“或” (V)、“非” (﹁)和“蕴含”(←)构成的简单陈述句；（2）“一阶规则”(first- order rule)/关系型规则（relational rule）：基本成分是能描述事物的属性或关系的“原子公式”(atomic formula)。 从形式语言系统的角度看，命题规则是一阶规则的特例，一阶规则的学习要比命题规则复杂的多。 15.2 序贯覆盖 规则学习的目标：产生一个能覆盖尽可能多的样例的规则集。 最直接的做法：“序贯覆盖”(sequential covering)，即逐条归纳：在训练集上每学到一条规则，就将该规则覆盖的训练样例去除，然后以剩下的训练样例组成训练集重复上述过程，由于每次只处理一部分数据，因此也被称为“分治”(separate and-conquer)策略。 序贯覆盖法的关键：如何从训练集学出单条规则。对规则学习目标⊕，产生一条规则就是寻找最优的一组逻辑文字来构成规则体，这是一个搜索问题。 最简单的做法是从空规则“⊕←”开始，将正例类别作为规则头，再逐个遍历训练集中的每个属性及取值，尝试将其作为逻辑文字增加到规则体中，若能使当前规则体仅覆盖正例,则由此产生一条规则，然后去除已被覆盖的正例并基于剩余样本尝试生成下一条规则。 现实任务中一般有两种策略来产生规则：（1）“自顶向下”(top-down)，即从比较一般的规则开始，逐渐添加新文字以缩小规则覆盖范围，直到满足预定条件为止；亦称为“生成-测试”(generate-then-test)法，是规则逐渐“特化”(specialization)的过程。覆盖范围从大往小搜索规则，更容易产生泛化性能较好的规则。（2）“自底向上”(bottom-up)，即从比较特殊的规则开始，逐渐删除文字以扩大规则覆盖范围，直到满足条件为止;亦称为“数据驱动”(data driven)法，是规则逐渐“泛化”(generalization)的过程。覆盖范围从小往大搜索规则，适合于训练样本较少的情形。前者对噪声的鲁棒性比后者要强得多。因此，在命题规则学习中通常使用第一种策略,而第二种策略在一阶规则学习这类假设空间非常复杂的任务上实用得多。 “集束搜索”(beam search),：每轮保留最优的b个逻辑文字，在下一轮均用于构建候选集，再把候选集中最优的b个留待再下一轮使用。 15.3 剪枝优化 剪枝(pruning)：规则生成本质上是一个贪心搜索过程，需有一定的机制来缓解过拟合的风险，最常见的做法是剪枝(pruning)。 与决策树相似，剪枝可发生在规则生长过程中，即“预剪枝”，也可发生在规则产生后，即“后剪枝”。通常是基于某种性能度量指标来评估增/删逻辑文字前后的规则性能，或增/删规则前后的规则集性能,从而判断是否要进行剪枝。 剪枝还可借统计显著性检验来进行：方法：CN2算法[Clark and Niblett,1989]：在预剪枝时，假设用规则集进行预测必须显著优于直接基于训练样例集后验概率分布进行预测。CN2使用了似然率统计量(LikelihoodRatio Statistics,简称LRS)。这实际上是一种信息量指标，衡量了规则(集)覆盖样例的分布与训练集经验分布的差别：LRS越大，说明采用规则(集)进行预测与直接使用训练集正、反例比率进行猜测的差别越大；LRS越小，说明规则(集)的效果越可能仅是偶然现象。在数据量比较大的现实任务中，通常设置为在LRS很大(例如0.99)时CN2算法才停止规则(集)生长。 后剪枝最常用的策略是“减错剪枝”(Reduced Error Pruning,简称REP)：其基本做法是：将样例集划分为训练集和验证集，从训练集上学得规则集R后进行多轮剪枝，在每一轮穷举所有 可能的剪枝操作，包括删除规则中某个文字、删除规则结尾文字、删除规则尾部多个文字、删除整条规则等，然后用验证集对剪枝产生的所有候选规则集进行评估保留最好的那个规则集进行下一轮剪枝，如此继续，直到无法通过剪枝提高验证集上的性能为止。 REP：复杂度是O(m^4)IREP (Incremental REP) ：复杂度O(mlog2m)做法：在生成每条规则前，先将当前样例集划分为训练集和验证集，在训练集上生成一条规则r，立即在验证集上对其进行REP剪枝，得到规则r，将r覆盖的样例去除，在更新后的样例集上重复上述过程。REP是针对规则集进行剪枝，而IREP仅对单条规则进行剪枝，因此后者比前者更高效。 规则学习算法RIPPER： RIPPER中的后处理机 制是为了在剪枝的基础上进一步提升性能。对R中的每条规则ri,，RIPPER为它产生两个变体： 15.4 一阶规则学习 关系数据：直接描述样例间的关系。 背景知识：由原样本属性转化而来的“色泽更深”“根蒂更蜷”等原子公式。 由样本类别转化而来的关于“更好”“﹁更好”的原子公式。 FOIL (First-Order Inductive Learner) ：是著名的一阶规则学习算法，它遵循序贯覆盖框架且采用自顶向下的规则归纳策略。 FOIL使用“FOIL增益”（FOIL gain）来选择文字： 15.5 归纳逻辑程序设计 归纳逻辑程序设计(Inductive Logic Programming,简称ILP)：在一阶规则学习中引入了函数和逻辑表达式嵌套一方面，这使得机器学习系统具备了更为强大的表达能力；另一方面，ILP可看作用机器学习技术来解决基于背景知识的逻辑程序(logic program)归纳，其学得的“规则”可被PROLOG等逻辑程序设计语言直接使用。15.5.1 最小一般泛化 归纳逻辑程序设计采用==自底向上==的规则生成策略，直接将一个或多个正例所对应的具体事实(grounded fact)作为初始规则，再对规则逐步进行泛化以增加其对样例的覆盖率。泛化操作可以是将规则中的常量替换为逻辑变量，也可以是删除规则体中的某个文字。 最小一般泛化(Least General Generalization,简称LGG) ：用于将“特殊”规则转变为更“一般”的规则。15.5.2 逆归结 四种完备的逆归结操作：以规则形式p←q等价地表达pV﹁q，并假定用小写字母表示逻辑文字、大写字母表示合取式组成的逻辑子句： “置换”(substitution)是用某些项来替换逻辑表达式中的变量。 “合一”(unification)是用一种变量置换令两个或多个逻辑表达式相等。 逆归结的一大特点：能够自动发明新谓词，这些谓词可能对应于样例属性和背景知识中不存在的新知识，对知识发现与精化有重要意义。 ILP系统通常先自底向上生成一组规则，然后再结合最小一般泛化与逆归结做进一步学习。]]></content>
      <categories>
        <category>西瓜书学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西瓜书学习笔记 | 第14章 概率图模型]]></title>
    <url>%2F5028c23a%2F</url>
    <content type="text"><![CDATA[概率图模型（probabilistic model）（变量关系图）：是一类用图来表达变量相关关系的概率模型。一个结点：表示一个或一组随机变量。边：表示变量间的概率相关关系。 14.1 隐马尔科夫模型 概率模型：将学习任务归结于计算变量的概率分布。 推断：利用已知变量推测未知变量的条件分布。 “生成式”模型：考虑联合概率分布P(Y,R|O)。“判别式”模型：考虑条件分布P(Y|O)。（Y：变量集合O：可观测变量集合 R：其他变量的集合） 隐马尔科夫模型（Hidden Markov Model：简称HMM）：是结构最简单的动态贝叶斯网，有向图模型，==生成式模型==。 隐马尔科夫模型中的变量：状态变量（隐变量）、观测变量。 状态（离散空间）空间：状态变量的取值范围。 马尔科夫链（Markov chain）：系统下一时刻的状态仅有当前状态决定，不依赖于以往的任何状态。基于这种关系，所有变量的联合概率分布为： 确定一个隐马尔科夫模型需要以下三组参数：) 产生观测序列的过程： 在实际应用中，人们常关注隐马尔科夫模型的三个基本问题：14.2 马尔可夫随机场 马尔可夫随机场（Markov Random Field，简称MRF）：是典型的马尔可夫网，无向图模型，==生成式模型==。 势函数（potential functions）：亦称”因子“（factor），是定义在变量子集上的非负实函数，主要用于定义概率分布函数。 团（clique）：对于上图中节点的一个子集，若其中任意两结点间都有边连接，则该结点子集为一个“团”。 极大团（maximal clique）：在一个团中加入任何一个结点都不再形成团。不能被其他团所包含的团。 马尔可夫随机场中，多个变量之间的联合概率定义（基于团）：所有团构成的集合为C，团Q∈C。) 马尔可夫随机场中，多个变量之间的联合概率定义（基于极大团）： 分离集（separating set）：如图，从结点集A中的结点到B中的结点都必须经过结点集C中的结点，则称结点集A和B被结点集C分离，C成为“分离集”。 全局马尔可夫性（global Markov property）：给定两个变量子集的分离集，则这两个变量子集条件独立。 由全局马尔可夫性得到的两个很有用的推论：局部马尔可夫性（local Markov property）：给定某变量的邻接变量，则该变量条件独立于其他变量。成对马尔可夫性（pairwise Markov property）：给定所有其他变量，两个非邻接变量条件独立。 马尔可夫随机场中的势函数：作用：定量刻画变量集中变量之间的相关关系，它是非负函数，且在所偏好的变量取值上有较大函数值。为了满足非负性，指数函数常被用于定义势函数：14.3 条件随机场 条件随机场（Conditional Random Field，简称CRF）：是一种==判别式==无向图模型。 条件随机场试图对多个变量在给定观测值后的条件概率进行建模。在自然语言处理的词性标注任务中，观测数据为语句（即单词序列），标记为相应的词性序列，具有线性序列结构；在语法分析任务中，输出标记则是语法树，具有树形结构。 条件随机场定义： 链式条件随机场（chain-structured CRF）：条件随机场使用势函数和图结构上的团来定义条件概率P（y|x）。链式条件随机场主要包含两种关于标记变量的团：单个标记变量和相邻的标记变量。 在条件随机场中，通过选用指数势函数并引入特征函数，条件概率被定义为： 特征函数：特征函数通常是实值函数，以刻画数据的一些很可能成立或期望成立的经验特征。例：14.5（a）的词性标注，若采用转移特征函数： 条件随机场和马尔可夫随机场均使用团上的势函数定义概率，两者在形式上没有显著差别；但条件随机场处理的是条件概率，而马尔科夫随机场处理的是联合概率。 14.4 学习与推断 边际分布：指对无关变量求和或积分后得到结果。 边际化：对联合分布中其他无关变量进行积分的过程。 参数估计（参数学习问题）：对概率图模型，确定具体分布的参数。通常使用极大似然估计或最大后验概率估计求解。 推断问题的目标和关键： 概率图模型的推断方法大致可分为两类：第一类：精确推断方法——变量消去、信念传播。第二类：近似推断方法——MCMC采样、变分推断。 14.4.1 变量消去 精确推断的实质是一类动态规划算法，它利用图模型所描述的条件独立行来削减计算目标概率值所需要的计算量。 有向图模型：)) 无向图模型：上述方法对无向图模型同样适用，忽略掉图14.7（a）中的箭头，将其看作一个无向图模型，有 变量消去法有一个明显的缺点：若需计算多个边际分布，重复使用变量消去法将会造成大量的冗余计算。 14.4.2 信念传播 信念传播（Belief Propagation）算法：将变量消去法中的求和操作看作一个消息传递过程，较好地解决了求解多个边际分布时的重复计算问题。 在信念传播算法中，一个结点仅在接收到来自其他所有结点的消息后才能向另一个结点发送消息，且结点的边际分布正比于它所接收的消息的乘积，即 若图结构中没有环，则信念传播算法经过两个步骤即可完成所有消息传递，进而能计算所有变量上的边际分布： ●指定一个根结点，从所有叶结点开始向根结点传递消息，直到根结点收到 所有邻接结点的消息; ●从根结点开始向叶结点传递消息，直到所有叶结点均收到消息。例： 14.5 近似推断 近似推断方法大致可分为两大类：第一类：采样(sampling)，通过使用随机化方法完成近似；第二类：使用确定性近似完成近似推断，典型代表为变分推断(variational inference)。 14.5.1 MCMC采样 采样思路：假设我们的目标是计算函数f(x)在概率密度函数p(x)下的期望以此来近似目标期望E[f]。 概率图模型中最常用的采样技术是马尔可夫链蒙特卡罗(Markov ChainMonte Carlo,简称MCMC)方法。给定连续变量x∈X的概率密度函数p(x)，x在区间A中的概率可计算为：若有函数f:X→R，则可计算f(x)的期望 计算方法：MCMC先构造出服从p分布的独立同分布随机变量X1,2….. xN，再得到上式的无偏估计 MCMC方法的关键：通过构造“平稳分布为p的马尔可夫链”来产生样本：若马尔可夫链运行时间足够长(即收敛到平稳状态)，则此时产出的样本x近似服从于分布p。 如何判断马尔可夫链到达平稳状态呢？假定平稳马尔可夫链T的状态转移概率(即从状态x转移到状态x’的概率)为T(x’ |x)，t时刻状态的分布为p(x*)，则若在某个时刻马尔可夫链满足平稳条件则p(x)是该马尔可夫链的平稳分布，且马尔可夫链在满足该条件时已收敛到平稳状态。 MCMC方法：先设法构造一条马尔可夫链，使其收敛至平稳分布恰为待估计参数的后验分布，然后通过这条马尔可夫链来产生符合后验分布的样本，并基于这些样本来进行估计。 Metropolis Hastings (简称MH)算法（MCMC的重要代表）：它基于“拒绝采样”(reject sampling)来逼近平稳分布p。 为达到平稳状态，只需要将接受率设置为： 吉布斯采样(Gibbs sampling)：有时被视为MH算法的特例，它也使用马尔可夫链获取样本，而该马尔可夫链的平稳分布也是采样的目标分布p(x)。采样步骤：(1)随机或以某个次序选取某变量xi；(2) 根据x中除xi外的变量的现有取值,计算条件概率p(xi | x)，其中xq= {x1,W2…,Xi -1,Ti+1,…,xN} ；(3)根据p(xi I xq)对变量xi采样，用采样值代替原值。 14.5.2 变分推断 变分推断：通过使用已知简单分布来逼近需推断的复杂分布，并通过限制近似分布的类型，从而得到种局部最优、但具有确定解的近似后验分布。 盘式记法(plate notation) ：概率图模型一种简洁的表示方法。14.6 话题模型 话题模型(topic model)：是一族==生成式==有向图模型，主要用于处理离散型的数据(如文本集合)，在信息检索、自然语言处理等领域有广泛应用。 隐狄利克雷分配模型(Latent Dirichlet Allocation, 简称LDA)：话题模型的典型代表。]]></content>
      <categories>
        <category>西瓜书学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用GitHub+hexo从零开始搭建个人博客]]></title>
    <url>%2Fa13da74f%2F</url>
    <content type="text"><![CDATA[作为一个技术小白，一直想搭建一个自己的博客，但是不知道从哪里开始。偶然间知道GitHub可以搭建博客，并且是免费的。所以抱着学习的态度，开始了自己第一个博客的搭建。整个搭建过程真的很简单，网上的很多教程也都说的非常详细，所以我的这篇博客主要来记录一下我在搭建过程中参考的一些文章，并且自己遇到的一些坑，下面就开始啦~ 一、准备工作1.注册一个GitHub账号2.安装Git3.安装Node.js4.安装Hexo 二、参考文章整个搭建过程可以参考：基于hexo+github搭建一个独立博客.此博主写的非常详细，也填补了一些坑。此外还有： 1.GitHub相关：使用Github搭建属于自己的博客. 利用github搭建hexo个人静态博客网站.GitHub的注册与使用.一个小时学会Git. 2.Hexo相关：hexo安装文档.NexT使用文档.使用hexo发布文章.Hexo使用攻略-添加分类及标签.hexo高阶教程：next主题优化之加入网易云音乐、网易云跟帖、炫酷动态背景、自定义样式，打造属于你自己的定制化博客.hexo博客安装RSS插件.hexo博客常用插件及教程.hexo高级玩法. 3.SEO相关：hexo搭建个人博客之seo优化.Hexo博客之后续SEO优化. 4.Markdown相关：Markdown基本语法.最新主流 Markdown 编辑器推荐. 5.遇到的一些坑：Git之右键没有Git Bash Here的解决办法.hexo博客不蒜子统计不显示/无效.在搭建blog所遇到的问题.Hexo生成sitemap步骤及duplicated mapping key 问题解决.git warning: LF will be replaced by CRLF in 解决办法. 三、小小总结搭好博客后第一篇文章就到这里啦，其实也算不上什么技术原创文章，只是简单把自己看过的文章做个小小整理和记录。之所以搭建这个博客，主要还是想把自己的学习过程记录下来，也可以作为自己的小树洞，记录一下生活~既然有了自己的小博客，希望自己以后可以常常更新。技术小白也要更加努力的学技术，虽然现在的自己对未来还有一些迷茫，但是希望在学习过程中尽快找到方向，并且向大佬们看齐！加油^^]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Hexo</tag>
        <tag>Git</tag>
        <tag>Node</tag>
        <tag>SEO</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
</search>

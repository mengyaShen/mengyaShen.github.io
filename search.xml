<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深度学习 | 误差反向传播法]]></title>
    <url>%2F12e97368%2F</url>
    <content type="text"><![CDATA[1. 计算图 计算图将计算过程用图形表示出来。这里说的图形是数据结构图，通过多个节点和边表示（连接节点的直线称为“边”）。 正向传播（forward propagation）：从左向右进行计算，传递的是计算结果。 反向传播（backward propagation）：是从右向左进行计算，传递的是局部导数。 局部计算：计算图的特征是可以通过传递“局部计算”获得最终结果。“局部”这个词的意思是“与自己相关的某个小范围”。局部计算是指，无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。 计算图的优点：①可以进行局部计算。②计算图可以将中间的计算结果全部保存起来。③可以通过反向传播高效计算导数。综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。 2. 链式法则 链式法则是关于复合函数的导数的性质，定义如下：如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。 链式法则和计算图最左边是反向传播的结果。 3. 反向传播3.1 加法节点的反向传播加法节点的反向传播只乘以1，所以输入的值会原封不动地流向下一个节点。 3.2 乘法节点的反向传播考虑z=xy：乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。翻转值表示一种翻转关系，如图所示，正向传播时信号是x的话，反向传播时则是y；正向传播时信号是y的话，反向传播时则是x。加法的反向传播只是将上游的值传给下游，并不需要正向传播的输入信号。但是，乘法的反向传播需要正向传播时的输入信号值。因此，实现乘法节点的反向传播时，要保存正向传播的输入信号。 4. 简单层的实现我们把要实现的计算图的乘法节点称为“乘法层”（MulLayer），加法节点称为“加法层”（AddLayer）。 5. 激活函数层的实现5.1 ReLU层激活函数ReLU（Rectified Linear Unit）由下式表示：求出y关于x的导数：如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处。ReLU层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为 ON；没有电流通过的话，就将开关设为 OFF。反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，则不会有电流通过。 5.2 Sigmoid层sigmoid函数式：除了“×”和“+”节点外，还出现了新的“exp”和“/”节点。“exp”节点会进行y = exp(x)的计算，“/”节点会进行y=1/x的计算。 反向传播的流程：反向传播的输出为 ，这个值会传播给下游的节点。因此，Sigmoid层的反向传播，只根据正向传播的输出就能计算出来： 6. Affine/Softmax层的实现6.1 Affine层 神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算。矩阵的乘积运算的要点是使对应维度的元素个数一致。 仿射变换：神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”。 Affine层：将进行仿射变换的处理实现为“Affine层“。几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。6.2 批版本的Affine层前面介绍的Affine层的输入X是以单个数据为对象的。现在我们考虑N个数据一起进行正向传播的情况，也就是批版本的Affine层。6.3 Softmax-with-Loss 层 神经网络中进行的处理有推理（inference）和学习两个阶段。神经网络的推理通常不使用 Softmax层。神经网络的学习阶段则需要 Softmax层。 包含作为损失函数的交叉熵误差（cross entropy error），所以称为“Softmax-with-Loss层”。 Softmax-with-Loss层的计算图：softmax函数记为Softmax层，交叉熵误差记为Cross Entropy Error层。这里假设要进行3类分类，从前面的层接收3个输入（得分）。如图5-30所示，Softmax层将输入（a1, a2, a3）正规化，输出（y1, y2,y3）。Cross Entropy Error层接收Softmax的输出（y1, y2, y3）和教师标签（t1, t2, t3），从这些数据中输出损失L。 “简易版”的Softmax-with-Loss层的计算图：Softmax层的反向传播得到了（y1 − t1, y2 − t2, y3 − t3）这样“漂亮”的结果。由于（y1, y2, y3）是Softmax层的输出，（t1, t2, t3）是监督数据，所以（y1 − t1, y2 − t2, y3 − t3）是Softmax层的输出和教师标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。 神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax的输出）接近教师标签。7. 误差反向传播法的实现神经网络学习的步骤：前提神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为下面4个步骤。步骤1（mini-batch）从训练数据中随机选择一部分数据。步骤2（计算梯度）计算损失函数关于各个权重参数的梯度。步骤3（更新参数）将权重参数沿梯度方向进行微小的更新。步骤4（重复）重复步骤1、步骤2、步骤3。 计算梯度的两种方法：一种是基于数值微分的方法，另一种是解析性地求解数学式的方法。数值微分虽然实现简单，但是计算要耗费较多的时间。和需要花费较多时间的数值微分不同，误差反向传播法可以快速高效地计算梯度。数值微分的优点是实现简单，因此，一般情况下不太容易出错。而误差反向传播法的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确。确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）。 小结• 通过使用计算图，可以直观地把握计算过程 •计算图的节点是由局部计算构成的。局部计算构成全局计算。 •计算图的正向传播进行一般的计算。通过计算图的反向传播，可以计算各个节点的导数。 •通过将神经网络的组成元素实现为层，可以高效地计算梯度*（反向传播法）。 •通过比较数值微分和误差反向传播的结果，可以确认误差反向传播的实现是否正确（梯度确认）。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习 | 神经网络的学习]]></title>
    <url>%2Fc7c854c0%2F</url>
    <content type="text"><![CDATA[本章的“学习”是指：从训练数据中自动获取最优权重参数的过程。学习的目的：找出使损失函数的值达到最小的权重参数。方法：函数斜率的梯度法。 1 从数据中学习从数据中学习是指：由数据自动决定权重参数的值 1.1 数据驱动 利用数据来解决问题：先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。 特征量：可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。 图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量：SIFT、SURF、HOG等。使用这些特征向量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。 深度学习：端到端机器学习。从原始数据（输入）中获得目标结果（输出）。 神经网路的优点：对所有的问题都可以用同样的流程来解决。 1.2 训练数据和测试数据 首先，使用训练数据（监督数据）进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。 泛化能力：处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的目标。 过拟合：只对某个数据集过度拟合的状态。仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。 2 损失函数损失函数：神经网络学习中的指标。一般用均方误差和交叉熵误差。 2.1 均方误差yk是表示神经网络的输出，tk表示监督数据，k表示数据的维数。one-hot表示：将正确解标签表示为1，其他标签表示为0。 2.2 交叉熵误差 2.3 mini-batch学习我们从全部数据中选出一部分，作为全部数据的“近似”。神经网络的学习也是从训练数据中选出一批数据（称为mini-batch，小批量），然后对每个mini-batch进行学习。 2.4 为什么要设定损失函数在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0。 3 数值微分导数就是表示某个瞬间的变化量。为了减小数值微分中的误差，可以计算f在(x + h)和(x − h)之间的差分。中心差分：以x为中心，计算它左右两边的差分。f在(x + h)和(x − h)之间的差分。前向差分：(x + h)和x之间的差分。 4. 梯度4.1 梯度法 像 这样的由全部变量的偏导数汇总而成的向量称为梯度（gradient）。 梯度法：通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法（gradient method）。 梯度表示的是各点处的函数值减小最多的方向，因此，无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向。实际上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处。 函数的极小值、最小值以及被称为鞍点（saddle point）的地方，梯度为 0。 鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。 寻找最小值的梯度法称为梯度下降法（gradient descent method），寻找最大值的梯度法称为梯度上升法（gradient ascent method）。 公式：η表示更新量，在神经网络的学习中，称为学习率（learning rate）。是事先确定的某个值。4.2 神经网络的梯度 这里的梯度是指损失函数关于权重参数的梯度。比如，有一个只有一个形状为2 × 3的权重W的神经网络，损失函数用L表示。此时，梯度可以用 表示。数学式表示如下：两者形状相同，都为2X3的形状。5. 学习算法的实现神经网络的学习步骤：前提神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面4个步骤。步骤1（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值。步骤2（计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。步骤3（更新参数）将权重参数沿梯度方向进行微小更新。步骤4（重复）重复步骤1、步骤2、步骤3。 神经网络的学习按照上面4个步骤进行。这个方法通过梯度下降法更新参数，不过因为这里使用的数据是随机选择的mini batch数据，所以又称为随机梯度下降法（stochastic gradient descent）。 小结• 机器学习中使用的数据集分为训练数据和测试数据。 • 神经网络用训练数据进行学习，并用测试数据评价学习到的模型的泛化能力。 • 神经网络的学习以损失函数为指标，更新权重参数，以使损失函数的值减小。 • 利用某个给定的微小值的差分求导数的过程，称为数值微分。 • 利用数值微分，可以计算权重参数的梯度。 • 数值微分虽然费时间，但是实现起来很简单。稍微复杂一些的误差反向传播法可以高速地计算梯度。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习 | 神经网络]]></title>
    <url>%2F12e97368%2F</url>
    <content type="text"><![CDATA[感知机：好消息，对于复杂的函数，感知机也隐含着能够表示它的可能性。即便是计算机进行的复杂处理，感知机（理论上）也可以将其表示出来。坏消息，设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的。神经网络：为了解决感知机的坏消息。神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。 1 从感知机到神经网络1.1 神经网络的例子：图中的网络一共由 3层神经元构成，但实质上只有 2层神经元有权重，因此将其称为“2层网络”。 1.2 感知机未表示偏置：感知机数学式（式1）：b：偏置，用于控制神经元被激活的容易程度；w1和w2：各个信号的权重，用于控制各个信号的重要性。改写（式2）：（式3）：明确表示偏置： 2 激活函数 激活函数：刚刚的h(x)函数会将输入信号的总和转换为输出信号，这种函数一般称为激活函数。 激活函数的作用：决定如何来激活输入信号的总和。 将式2再改写：计算加权输入信号和偏置的总和，记为a（式4）：用h()函数将a转换为输出y（式5）： 明确显示激活函数的计算过程： “ 神经元=节点”，这里我们称a为y的节点。激活函数的计算过程： 激活函数是连接感知机和神经网络的桥梁。 感知机可分：1） 朴素感知机：指单层网络，指的是激活函数使用了阶跃函数的模型。2）多层感知机：指神经网络，即使用sigmoid函数等平滑的激活函数的多层网络。 2.1 阶跃函数h(x)表示的激活函数以阈值为界，一旦输入超过了阈值，接切换输出，这样的函数称为“阶跃函数”。感知机中使用了阶跃函数作为激活函数。 2.2 sigmoid函数（式6）：函数就是给定某个输入后，会返回某个输出的转换器。神经网络中用sigmoid函数作为激活函数，进行信号的转换，转换后的信号被传送给下一个神经元。感知机和神经网络的主要区别就在于这个激活函数。 2.3 sigmoid函数和阶跃函数的比较不同：1）平滑性：sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。2）于阶跃函数只能返回0或1，sigmoid函数可以返回0.731 …、0.880 …等实数。也就是说，感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。相同：1）它们具有相似的形状。2）两者的结构均是“输入小时，输出接近0（为0）；随着输入增大，输出向1靠近（变成1）”当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值。3），不管输入信号有多小，或者有多大，输出信号的值都在0到1之间。 2.4 非线性函数 阶跃函数和sigmoid函数均为非线性函数。 线性函数： 函数本来是输入某个值后会返回一个值的转换器。向这个转换器输入某个值后，输出值是输入值的常数倍的函数称为线性函数（用数学式表示为h(x) = cx。c为常数）。因此，线性函数是一条笔直的直线。 非线性函数：不像线性函数那样呈现出一条直线的函数。 神经网络的激活函数必须使用非线性函数。数不能使用线性函数，因为使用线性函数的话，加深神经网络的层数就没有意义了。 线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。eg：把线性函数 h(x) = cx 作为激活函数，把y(x) = h(h(h(x)))的运算对应3层神经网络。这个运算会进行y(x) = c × c × c × x的乘法运算，但是同样的处理可以由y(x) = ax（注意，a = c 3）这一次乘法运算（即没有隐藏层的神经网络）来表示。 2.5 ReLU函数ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0。ReLU函数（式7）： 3 多维数组的运算多维数组就是“数字的集合”，数字排成一列的集合、排成长方形的集合、排成三维状或者（更加一般化的）N维状的集合都称为多维数组。 神经网络的内积省略了偏置和激活函数，只有权重的神经网络： 4 3层神经网络的实现 4.1 权重的符号： 4.2 各层间信号传递的实现从输入层到第1层的第1个神经元的信号传递过程：图中增加了表示偏置的神经元“1”。注意，偏置的右下角的索引号只有一个。这是因为前一层的偏置神经元（神经元“1”）只有一个 。用数学式表示（式8）：用矩阵乘法表示（式9）：从输入层到第1层的信号传递：隐藏层的加权和（加权信号和偏置的总和）用a表示，被激活函数转换后的信号用z表示。h()表示激活函数（这里使用的是sigmoid函数）。 第1层到第2层的信号传递：第1层的输出（Z1）变成了第2层的输入 从第2层到输出层的信号传递： 输出层所用的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid函数，多元分类问题可以使用softmax函数。5 输出层的设计一般而言，回归问题用恒等函数，分类问题用softmax函数。机器学习的问题大致可以分为分类问题和回归问题。分类问题：是数据属于哪一个类别的问题。比如，区分图像中的人是男性还是女性。回归问题：是根据某个输入预测一个（连续的）数值的问题。比如，根据一个人的图像预测这个人的体重。 5.1 恒等函数和softmax函数 恒等函数：会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出。恒等函数进行的转换处理可以用一根箭头来表示。 softmax函数（式10）：输出通过箭头与所有的输入信号相连。输出层的各个神经元都受到所有输入信号的影响。 5.2 实现softmax函数时的注意事项 softmax函数在计算机运算时有一定缺陷，可能会产生”溢出问题“。指数运算值会很大，在超大值之间进行除法运算，结果会出现”不确定性“情况。 计算机处理“数”时，数值必须在 4字节或 8字节的有限数据宽度内。可以表示的数值范围是有限的。因此，会出现超大值无法表示的问题。这个问题称为溢出。 softmax函数的改进（式11）：这里的C‘可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值。 5.3 softmax函数的特征 softmax函数的输出是0.0到1.0之间的实数。并且，softmax函数的输出值的总和是1（重要性质）。 有了这个性质，我们把softmax函数的输出解释为“概率”。 一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数一般会被省略。 求解机器学习问题的步骤可以分为“学习” 和“推理”两个阶段。首先，在学习阶段进行模型的学习（这里的“学习”是指使用训练数据、自动调整参数的过程），然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax函数。在输出层使用 softmax函数是因为它和神经网络的学习有关系。 5.4 输出层的神经元数量输出层的神经元数量需要根据待解决的问题来决定。分类问题：输出层的神经元数量一般设定为类别的数量 小结本章介绍了神经网络的前向传播。 • 神经网络中的激活函数使用平滑变化的sigmoid函数或ReLU函数。 • 通过巧妙地使用NumPy多维数组，可以高效地实现神经网络。 • 机器学习的问题大体上可以分为回归问题和分类问题。 • 关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用softmax函数。 • 分类问题中，输出层的神经元的数量设置为要分类的类别数。 • 输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习 | 感知机]]></title>
    <url>%2Fc7c854c0%2F</url>
    <content type="text"><![CDATA[一、感知机是什么感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。x1、x2是输入信号，y是输出信号，w1、w2是权重（w是weight的首字母）。○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重（w1x1、w2x2）。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活”。这里将这个界限值称为阈值，用符号θ表示。 二、简单逻辑电路（单层感知机）2.1 与门参数的选择方法如：(w1, w2, θ) = (0.5, 0.5, 0.7) 2.2 与非门参数的选择方法如：(w1, w2, θ) = (−0.5, −0.5, −0.7) 2.3 或门参数的选择方法如：(w1, w2, θ) = (1.0, 1.0,−0.5) 与门、与非门、或门是单层感知机。 三、多层感知机异或门使用单层感知机无法实现异或门，需要“叠加层”。使用2层感知机。s1作为与非门的输出，把s2作为或门的输出1）第0层的两个神经元接收输入信号，并将信号发送至第1层的神经元。2）第1层的神经元将信号发送至第2层的神经元，第2层的神经元输出y。 四、权重和偏置将阈值θ换成−b，因此感知机的表示：b称为偏置，用于控制神经元被激活的容易程度。w1和w2称为权重。感知机会计算输入信号和权重的乘积，然后加上偏置，如果这个值大于0则输出1，否则输出0。 五、线性和非线性区分：能否用一条直线分割空间。可以用一条直线将○和△分开，因此该空间为线性空间。不能用一条直线将○和△分开（使用曲线可以分开），因此该空间为非线性空间。 六、小结• 感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。 • 感知机将权重和偏置设定为参数。 • 使用感知机可以表示与门和或门等逻辑电路。 • 异或门无法通过单层感知机来表示。 • 使用2层感知机可以表示异或门。 • 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。 • 多层感知机（在理论上）可以表示计算机。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computer Vision | LocalImageFeaturesAndMatching]]></title>
    <url>%2F5c655b74%2F</url>
    <content type="text"><![CDATA[一、哈里斯角的不变性Harris Corners 哈里斯角结构矩阵：2. 结构矩阵： Affine intensity change 仿射强度变化仅使用导数=&gt;强度偏移I-&gt;I+b的不变性 强度缩放：I -&gt;a I Image translation 图像翻译导数和窗函数是平移不变的。 Image rotation 图像旋转 Scaling 缩放比例所有点都将被分类为边。角点位置与缩放不相关！ 到目前为止：可以在x-y中定位，但不能缩放。 二、特征点的“比例”Automatic Scale Selection 自动刻度选择 如何找到f响应相等的补丁大小？什么是好的f？增加标度的函数响应（标度特征）。 什么是有用的特征函数f？“Blob（斑点）”探测器常见于拐角处——高斯（LoG）的拉普拉斯（二阶导数） 在位置标度空间中求局部极大值 Alternative approach 替代方法高斯差分（DoG）近似LoG： Interest points 兴趣点关键点检测：可重复且独特角点（Corners）、斑点（blobs）、稳定区域（stable regions）哈里斯（Harris），DoG 选择兴趣点检测器 为什么选择？用更多的探测器收集更多的点，以便进行更多可能的匹配。 你想要干什么？x-y精确定位：Harris良好的尺度局部化：高斯差分（Difference of Gaussian）柔性区域形状：MSER 最佳选择通常取决于应用程序harris-/hessian-laplace/dog在许多自然类别中都很有效MSER适用于建筑和印刷品 三、Local Image Descriptors 局部图像描述局部特征：主要组件 Image representations 图像表示 模板：强度、阶级等。 直方图：颜色、纹理、SIFT描述符等。 图像表示：直方图（Histograms） 联合直方图（Joint histogram）需要大量数据避免空箱子的分辨率降低 边缘直方图（Marginal histogram）需要独立功能比联合直方图更多的数据（data）/bin 聚类（Clustering）：对所有图像使用相同的聚类中心 Computing histogram distance 计算直方图距离直方图相交（假设为标准化直方图） 直方图：实现问题 量化（Quantization）网格（Grids）：快速，但仅适用于很少的维度；聚类（Clustering）：速度较慢，但可以量化更高维度的数据。 匹配（Matching）直方图相交或欧几里德可能更快；卡方检验（Chi-squared）通常效果更好；当附近的bins代表相似的值时，土方运输机的距离（Earth mover’s distance）更有利。计算直方图的目的 颜色 模型局部外观 纹理 定向梯度的局部直方图 sift：尺度不变特征变换——非常受欢迎（4万条引用）SIFT描述符格式 在本地16 x 16窗口进行计算。 根据发现的方向θ和标度σ（增益不变性）旋转和缩放窗口。 计算由一半窗口的高斯方差加权的梯度（用于平滑衰减）。SIFT矢量形成梯度方向直方图的4x4数组，按梯度大小加权。按8个方向放置X 4x4阵列=128维。保证平滑度降低光源影响SIFT 高斯尺度空间极值的差分 后处理位置插值丢弃低对比度点沿边消除点 方位估计 描述符提取动机：我们希望对空间布局有一些敏感度，但不要太多，所以直方图块给了我们这些。SIFT定向规范化计算方向直方图；选择主导方向θ；规格化：旋转到固定方向。SIFT描述符提取给定具有比例和方向的关键点： 选取与估计尺度最接近的尺度空间图像； 重新采样图像以匹配方向或从矢量中减去检测器的方向，使图像旋转不变性。局部描述符：SURFSIFT思想的快速逼近：用2D盒滤波器和积分图像进行有效计算比SIFT快6倍物体识别的等效质量局部描述符 大多数特性可以看作是模板、直方图（计数）或组合。 理想的描述符应该是：强健有特色；紧凑高效。 大多数可用的描述符关注边缘/梯度信息：捕获纹理信息；很少使用的颜色。 四、Feature Matching 特征匹配Bijective 双射：Injective 内射Surjective 满射 Euclidean distance vs. Cosine Similarity 欧氏距离与余弦相似性Euclidean distance 欧氏距离 Cosine similarity 余弦 特征匹配的标准 标准1： 计算特征空间中的距离，例如128个Sift描述符之间的欧几里德距离 匹配点到最近距离（最近邻）问题：所有的东西都匹配吗？ 标准2： 计算特征空间中的距离，例如128个Sift描述符之间的欧几里德距离 匹配点到最近距离（最近邻） 忽略任何高于阈值的内容（不匹配！） 问题：6. 门槛难挑7. 不明显的特征可能有很多相似的匹配，只有一个是正确的 最近邻距离比 Interest points 兴趣点 关键点检测（Keypoint detection）：可重复且独特角点（Corners）、斑点（blobs）、稳定区域（stable regions）哈里斯（Harris），DoG 描述符（Descriptors）：健壮和选择性方位的空间直方图SIFT]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computer Vision | InterestPointsAndCorners]]></title>
    <url>%2F9595090%2F</url>
    <content type="text"><![CDATA[Filtering——&gt;Edges——&gt;Corners滤波器——&gt;边沿——&gt;角点 Feature points 特征点也称为兴趣点（interest points）、关键点（ key points）等，通常称为“局部”特征（ ‘local’ features）。 视图之间的通信对应关系：在图像上匹配点、面片、边或区域。 特征点用于：图像对齐三维重建运动跟踪（机器人、无人机、AR）索引与数据库检索目标识别 不变局部特征检测可重复且独特的点。即，对图像变换不变性：外观变化（亮度、照度）；几何变化（平移、旋转、缩放）。 示例应用——全景接合1）检测（Detection）：找到一组与众不同的关键点。2）描述（Description）：提取每个兴趣点的特征描述作为向量。3）匹配（Matching）：计算特征向量之间的距离以找到对应关系。 好的特征：1）重复性尽管存在几何和光度变换，但在多个图像中可以找到相同的特征。2）显著性每个特征都很独特。3）紧凑性和效率比图像像素少很多特征。4）局部一个特征占据了图像相对较小的区域；对杂波和遮挡鲁棒。 Corner Detection 角点检测我们可以透过一扇小窗户来认识这一点。我们希望窗口向任何方向移动，以使强度发生很大变化。 基于自相关的角点检测 变换[u，v]：通过改变窗口w（x，y）的外观： 关键特性：在拐角区域，图像梯度有两个或多个主方向角落是可重复和独特的。 用二次曲面局部逼近E（u，v）： 一个函数f可以用它在一个点a的无穷级数表示：当我们关心窗口中心时，我们设置a=0（麦克劳林系列）。f(x)＝e^x以f(0)为中心的逼近 角点检测：数学 通过二阶泰勒展开给出了（0，0）邻域中E（u，v）的局部二次逼近： E(u，v)关于(0,0)的二阶泰勒展开式：二次近似简化为:其中M是根据图像导数计算的二阶矩矩阵： 角点作为独特的兴趣点 图像导数的2x 2矩阵（在点附近平均） 二阶矩矩阵的解释 E(u,v)的形状：E(u,v)的水平“切片”：M的对角化：因此，E(u,v)的形状为椭圆基于M特征值的图像点分类 λ1 and λ2 很小，几乎在所有方向上都是恒定的。 线性代数： Harris 角点探测器0）输入图像：我们要计算每个像素的M；1）计算图像导数（可选，先模糊）；2）用导数平方计算M分量；3）宽度为σ的高斯滤波器g()；4）计算转角5）C上选择高转角的阈值6）非极大值抑制拾取峰 位置对光度变换是不变的，对几何变换是协变的吗？不变性：图像被变换，角点位置不变；协方差：如果我们有相同图像的两个变换版本，则应在相应位置检测特征。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computer Vision | ImageFiltering]]></title>
    <url>%2F968867f0%2F</url>
    <content type="text"><![CDATA[一张图片—&gt;feature detection 特征检测（e.g.：DoG）—&gt;feature description 特征描述（e.g.：SIFT）—&gt;Matching/Indexing/Detection 匹配/索引/检测&lt;—database of local descriptors 本地描述符数据库 Image filtering 图像滤波过滤的三种观点： 空域图像滤波过滤器是数字网格的数学运算平滑、锐化、测量纹理 频域图像滤波器滤波是一种改变图像频率的方法去噪、采样、图像压缩 图像金字塔比例空间表示允许粗到细操作 图像滤波计算每个位置的局部邻域函数： h=output f=filter I=image k,l = 二维坐标 m,n = 二维坐标 一、box filter 盒式过滤器盒滤波器作用： 将每个像素替换为其邻域的平均值； 达到平滑效果（去除尖锐特征）。 图像滤波器作用： 增强图像去噪、调整大小、增加对比度等。 从图像中提取信息纹理、边缘、特征点等。 检测模式模板匹配 二、linear filters 线性滤波器没有变化按1像素左移垂直边缘（绝对值）水平边缘（绝对值）锐化滤波器——突出与当地平均水平的差异 三、Gaussian filter 高斯滤波器邻近像素的加权贡献 高斯滤波器的可分性可分性示例： 2D convolution (center location only)二维卷积（仅中心位置）The filter factors into a product of 1D filters:过滤因子为1的乘积：Perform convolution along rows:执行卷积长行：Followed by convolution along the remaining column:然后对其余列进行卷积：为什么分离性在实践中有用？ MXN图像，PXQ过滤器二维卷积：MNPQ乘加可分二维：MN（P+Q）乘加 加速=PQ/（P+Q）9x9过滤器大约快4.5倍 过滤器应该有多大？ 边缘处的值应接近零 高斯人有无限的范围… 高斯的经验法则：将滤波器半宽设为3σ左右]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看远端地址 git remote -v时报错 fatal:not a git repository(or any of the parent directories):.git]]></title>
    <url>%2Ff3333876%2F</url>
    <content type="text"><![CDATA[查看git远端地址时： 1$ git remote –v 报错： 1fatal: not a git repository (or any of the parent directories): .git 提示说没有.git这样一个目录解决办法： 1$ git init ok啦！]]></content>
      <categories>
        <category>Hexo使用过程中出现的一些坑</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署hexo时出错：fatal: Could not read from remote repository.Error: Spawn failed]]></title>
    <url>%2F708c56a%2F</url>
    <content type="text"><![CDATA[在部署hexo执行下面代码时： 1$ hexo d 出现错误： 123456789On branch masternothing to commit, working tree cleangit@git.coding.net: Permission denied (publickey).fatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists.FATAL Something&apos;s wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.htmlError: Spawn failed 查了一些资料后发现是因为在设置百度主动推送的时候，站点配置文件中deploy:字段写错了。正确的应为： 123456deploy:- type: git repo: git@github.com:mengyaShen/mengyaShen.github.io.git branch: master- type: baidu_url_submitter 重新部署，成功。]]></content>
      <categories>
        <category>Hexo使用过程中出现的一些坑</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Computer Vision | Introduction]]></title>
    <url>%2Fed318fdc%2F</url>
    <content type="text"><![CDATA[一、计算视觉的经典问题Reconstruction——重建Recognition——识别(Re)organization——重组 Optical character recognition (OCR) 光学字符识别（OCR）Technology to convert scanned docs to text 扫描文档转换为文本的技术 Face detection 人脸检测 Smile detection 微笑检测 Object recognition (in supermarkets) 物体识别（超市） Vision-based biometrics 基于视觉的生物特征识别 Login without a password 无密码登录 Object recognition (in mobile phones) 物体识别（在移动电话中） 3D from images 三维图像 Human shape capture 人形捕捉 Special effects: shape capture 特殊效果：形状捕捉 Interactive Games: Kinect 互动游戏：Kinect Sports 体育运动 Medical imaging 医学影像学 AutoCars - Uber bought CMU’s lab 汽车制造商-Uber收购了CMU的实验室 Industrial robots 工业机器人 Vision in space 太空视野Vision systems (JPL) used for several tasks 用于多个任务的视觉系统（JPL） Mobile robots 移动机器人 Augmented Reality and Virtual Reality 增强现实与虚拟现实 二、计算机视觉及邻近领域 计算机视觉的贬义总结：机器学习在可视化数据中的应用。 图像处理、识别、深度学习、几何推理机器学习、图形学、计算摄影、光学机器人学、人机交互、医学影像学、神经科学 三、课程主题1.解释强度什么决定了像素的亮度和颜色？如何使用图像过滤器从图像中提取有意义的信息？ 2.对应和对齐如何在物体或场景中找到对应的点？我们如何估计它们之间的转换？ 3.分组和分段如何将像素分组成有意义的区域？ 4.分类与目标识别我们如何表现图像并对其进行分类？我们如何识别物体的类别？ 5.高级主题动作识别，三维场景和上下文，人在环视觉 6.相关知识线性代数、概率、图形课程、视觉/图像处理课程、机器学习 四、项目Projects 1-5: Structured conceptual / code——结构化概念/代码Project 6: Group challenge——团体挑战 Proj 1: Image Filtering and Hybrid Images——图像滤波和混合图像实现图像滤波以分离高频和低频。将来自不同图像的高频和低频合并以创建与比例相关的图像。 Proj 2: Local Feature Matching——局部特征匹配实现兴趣点检测、类sift局部特征描述、简单匹配算法。 Proj 3: Scene Recognition with Bag of Words——文字袋场景识别将局部特征量化为“词汇”，将图像描述为“视觉词汇”的直方图，训练分类器根据这些直方图识别场景。 Proj 3b: Object Detection with a Sliding Window——滑动窗口目标检测训练一个基于正样本和“挖掘”硬底片的人脸检测器，在多个尺度上检测人脸并抑制重复检测。 Proj 4: Convolutional Neural Nets——卷积神经网络Proj 5: Multi-view Geometry——多视图几何图形从特征点匹配恢复相机校准。计算机视觉中几乎所有测量的基础。 Proj 6: Group challenge——团体挑战改进webgazer：一个基于web的实时眼睛跟踪器。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战 | 第3章 决策树]]></title>
    <url>%2F668be595%2F</url>
    <content type="text"><![CDATA[决策树的一个重要任务是：为了数据中蕴含的知识信息。决策树可以使用不熟悉的数据集合，并从中提取除一系列规则，在这些机器根据数据集创建规则时，就是机器学习的过程。 3.1 决策树的构造 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点：可能会产生过度匹配问题。适用数据类型：数值型和标称型。 在构造决策树时，我们需要解决的第一个问题是：当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某分支上的数据属于同一类型，无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集。直到所有具有相同数据类型的数据均在一个数据子集内。 创建分支的伪代码函数createBranch()： 决策树的一般流程：(1) 收集数据：可以使用任何方法。(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。(4) 训练算法：构造树的数据结构。(5) 测试算法：使用经验树计算错误率。(6) 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。 3.1.1 信息增益 划分数据集的大原则：将无序的数据变得更加有序。 信息增益：划分数据集之前之后信息发生的变化。获得信息增益最高的特征就是最好的选择。 香农熵（熵）：集合信息的度量方式。 熵：定义为信息的期望值。 信息：符号xi的信息定义：其中p(xi)是选择该分类的概率。 信息熵：其中n是分类的数目。 基尼不纯度：度量集合无序程度的方法。简单的说就是从一个数据集中随机选取子项，度量其被错误分类到其他组里的概率。 程序清单3-1 计算给定数据集的香农熵12345678910111213141516171819202122232425from math import logfrom numpy import *import operatorimport matplotlib.pyplot as plt# 为所有可能分类创建字典def cal_entropy(data): entries_num = len(data) label_count = &#123;&#125; for vec in data: cur_label = vec[-1] label_count[cur_label] = label_count.get(cur_label,0)+1 Entropy =0.0 # 以2为底求对数 for key in label_count: prob =float(label_count[key])/entries_num Entropy += prob*math.log(prob,2) return (0-Entropy)# 定义自己的数据集def createData(): data = [[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']] labels = ['no sufacing','flippers'] return data,labels 执行： 12345678&gt;import trees&gt;myDat,labels = trees.createData()&gt;myDat&gt;trees.cal_entropy(myDat)# 增加第三个名为maybe的分类，测试熵的变化：&gt;myDat[0][-1] = 'maybe'&gt;myDat&gt;trees.cal_entropy(myDat) 3.1.2 划分数据集 使用ID3算法划分数据集。三组参数：待划分的数据集、划分数据集的特征、需要返回的特征的值。 程序清单3-2 按照给定特征划分数据集1234567891011121314151617def Split_Data(dataset, axis, value): ''' 使用传入的axis以及value划分数据集 axis代表在每个列表中的第X位，value为用来划分的特征值 ''' new_subset = [] # 利用循环将不符合value的特征值划分入另一集合 # 相当于将value单独提取出来（或作为叶节点） for vec in dataset: if vec[axis] == value: feature_split = vec[:axis] feature_split.extend(vec[axis + 1:]) new_subset.append(feature_split) # extend将vec中的元素一一纳入feature_split # append则将feature_split作为列表结合进目标集合 return new_subset 执行： 12345678&gt;import trees&gt;myDat,labels = trees.createData()&gt;myDat[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]&gt;trees.split_data(myDat,0,1)[[1, 'yes'], [1, 'yes'], [0, 'no']]&gt;trees.split_data(myDat,0,0)[[1, 'no'], [1, 'no']] Python语言不用考虑内存分配问题。Python语言在函数中传递的是列表的引用，在函 数内部对列表对象的修改，将会影响该列表对象的整个生存周期。为了消除这个不良影响，我们需要在函数的开始声明一个新列表对象。程序清单3-3 选择最好的数据集划分方式123456789101112131415161718192021222324252627282930def Split_by_entropy(dataset): ''' 使用熵原则进行数据集划分 @信息增益:info_gain = old -new @最优特征：best_feature @类别集合：uniVal ''' feature_num = len(dataset[0]) - 1 ent_old = cal_entropy(dataset) best_gain = 0.0 best_feature = -1 for i in range(feature_num): feature_list = [x[i] for x in dataset] # 将dataSet中的数据先按行依次放入x中，然后取得x中的x[i]元素，放入列表feature_list中 uniVal = set(feature_list) ent_new = 0.0 # 使用set剔除重复项，保留该特征对应的不同取值 for value in uniVal: sub_set = split_data(dataset, i, value) prob = len(sub_set) / float(len(dataset)) # 使用熵计算函数求出划分后的熵值 ent_new += prob * (0 - cal_entropy(sub_set)) # 由ent_old - ent_new选出划分对应的最优特征 Info_gain = ent_old - ent_new if (Info_gain &gt; best_gain): best_gain = Info_gain best_feature = i return best_feature 执行： 123456&gt;import trees&gt;myDat,labels=trees.createData()&gt;trees.Split_by_entropy(myDat)1&gt;myDat[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] 3.1.3 递归构建决策树 工作原理：得到原始数据集，然后基于最好的属性值划分数据集（由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分）。第一次划分之后，数据被向下传递到树分支的下一个节点，在这个节点上，再次划分数据。递归进行。递归结束的条件：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子节点或者终止块。如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用==多数表决==的方法决定该叶子节点的分类。 多数表决法： 123456789101112def Majority_vote(classList): ''' 使用多数表决法：若集合中属于第K类的节点最多，则此分支集合划分为第K类 ''' classcount = &#123;&#125; for vote in classList: classcount[vote] = classcount.get(vote,0) + 1 sorted_count = sorted(classcount.items(), key = operator.itemgetter(1),\ reverse = True) # 获取每一类出现的节点数（没出现默认为0）并进行排序 # 返回最大项的KEY所对应的类别 return sorted_count[0][0] 程序清单3-4 创建树的函数代码123456789101112131415161718192021222324252627282930313233def Create_Tree(dataset, labels): # 类别完全相同则停止继续划分 classList = [x[-1] for x in dataset] if classList.count(classList[0]) == len(classList): return classList[0] # 遍历完所有特征值时返回出现次数最多的 if len(dataset[0]) == 1: return Majority_vote(classList) best_feature = Split_by_entropy(dataset) best_labels = labels[best_feature] # 得到列表包含的所有属性值 myTree = &#123;best_labels: &#123;&#125;&#125; # 此位置书上写的有误，书上为del(labels[bestFeat]) # 相当于操作原始列表内容，导致原始列表内容发生改变 # 按此运行程序，报错'no surfacing'is not in list # 以下代码已改正 # 复制当前特征标签列表，防止改变原始列表的内容 subLabels = labels[:] # 删除属性列表中当前分类数据集特征 del (subLabels[best_feature]) # 使用列表推导式生成该特征对应的列 f_val = [x[best_feature] for x in dataset] uni_val = set(f_val) for value in uni_val: # 递归创建子树并返回 myTree[best_labels][value] = Create_Tree(split_data(dataset \ , best_feature, value), subLabels) return myTree 执行： 12345&gt;import trees&gt;myDat,labels = trees.createData()&gt;myTree = trees.Create_Tree(myDat,labels)&gt;myTree&#123;'flippers': &#123;0: 'no', 1: &#123;'no sufacing': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125; 3.2 在 Python 中使用 Matplotlib 注解绘制树形图Matplotlib提供了一个注解工具==annotations==，非常有用，它可以在数据图形上添加文本注释。 程序清单3-5 使用文本注解绘制树节点我们要知道：1）有多少个叶节点，以便确定x轴的长度；2）树有多少层，以便确定y轴的高度。 1234567891011#定义文本框和箭头格式decisionNode = dict(boxstyle="sawtooth", fc="0.8")# 创建字典。 boxstyle=”sawtooth” 表示注解框的边缘是波浪线，fc=”0.8” 是颜色深度leafNode = dict(boxstyle="round4", fc="0.8")arrow_args = dict(arrowstyle="&lt;-") # 箭头样式#绘制带箭头的注释def plotNode(nodeTxt, centerPt, parentPt, nodeType):# centerPt:节点中心坐标 parentPt:起点坐标 createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction',xytext=centerPt, textcoords='axes fraction',va="center",ha="center", bbox=nodeType, arrowprops=arrow_args )# 参考annotate说明文档 程序清单3-6 获取叶节点的数目和树的层数12345678910111213141516171819202122232425262728293031323334353637383940def Num_of_leaf(myTree): '''计算此树的叶子节点数目，输入为我们前面得到的树（字典）''' num_leaf = 0 # 初始化 first_node = myTree.keys() first_node = list(first_node)[0] # 获得第一个key值（根节点） 'no surfacing' # python 3X 中： mytree.keys() 返回 :dict_keys([’ ‘])是类似于列表但又不是列表的东东，它是个字典的key值的一个视图（view），所以改写为本句方法。 second_dict = myTree[first_node] # 获得value值 &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125; # Python3中使用LIST转换firstnode，原书使用[0]直接索引只能用于Python2 # 对于树，每次判断value是否为字典，若为字典则进行递归，否则累加器+1 for key in second_dict.keys(): # 键值：0 和 1 if type(second_dict[key]).__name__ =='dict': # 判断如果里面的一个value是否还是dict num_leaf += Num_of_leaf(second_dict[key]) # 递归调用 else: num_leaf += 1 return num_leafdef Depth_of_tree(myTree): '''计算此树的总深度''' depth = 0 first_node = myTree.keys() first_node = list(first_node)[0] second_dict = myTree[first_node] for key in second_dict.keys(): if type(second_dict[key]).__name__ == 'dict': pri_depth = 1 + Depth_of_tree(second_dict[key]) else: pri_depth = 1 # 对于树，每次判断value是否为字典，若为字典则进行递归，否则计数器+1 if pri_depth &gt; depth: depth = pri_depth return depth def retrieveTree(i): ''' 保存了树的测试数据 ''' listOfTrees =[&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;,&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: &#123;'head': &#123;0: 'no', 1: 'yes'&#125;&#125;, 1: 'no'&#125;&#125;&#125;&#125;] return listOfTrees[i] 程序清单3-7 plotTree函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def plotmidtext(cntrpt, parentpt, txtstring): '''作用是计算tree的中间位置 cntrpt起始位置,parentpt终止位置,txtstring：文本标签信息 (在两个节点之间的线上写上字) ''' xmid = (parentpt[0] - cntrpt[0]) / 2.0 + cntrpt[0] # cntrPt 起点坐标 子节点坐标 # parentPt 结束坐标 父节点坐标 ymid = (parentpt[1] - cntrpt[1]) / 2.0 + cntrpt[1] # 找到x和y的中间位置 createPlot.ax1.text(xmid, ymid, txtstring) # text() 的使用def plottree(mytree, parentpt, nodetxt): # 画树 numleafs = Num_of_leaf(mytree) depth = Depth_of_tree(mytree) firststr = list(mytree.keys())[0] cntrpt = (plottree.xoff + (1.0 + float(numleafs)) / 2.0 / plottree.totalw, plottree.yoff) # 计算子节点的坐标 plotmidtext(cntrpt, parentpt, nodetxt) # 绘制线上的文字 plotNode(firststr, cntrpt, parentpt, decisionNode) # 绘制节点 seconddict = mytree[firststr] plottree.yoff = plottree.yoff - 1.0 / plottree.totald # 每绘制一次图，将y的坐标减少1.0/plottree.totald，间接保证y坐标上深度的 for key in seconddict.keys(): if type(seconddict[key]).__name__ == 'dict': plottree(seconddict[key], cntrpt, str(key)) else: plottree.xoff = plottree.xoff + 1.0 / plottree.totalw plotNode(seconddict[key], (plottree.xoff, plottree.yoff), cntrpt, leafNode) plotmidtext((plottree.xoff, plottree.yoff), cntrpt, str(key)) plottree.yoff = plottree.yoff + 1.0 / plottree.totalddef createPlot(intree): # 类似于Matlab的figure，定义一个画布(暂且这么称呼吧)，背景为白色 fig = plt.figure(1, facecolor='white') fig.clf() # 把画布清空 axprops = dict(xticks=[], yticks=[]) # createPlot.ax1为全局变量，绘制图像的句柄，subplot为定义了一个绘图， # 111表示figure中的图有1行1列，即1个，最后的1代表第一个图 # frameon表示是否绘制坐标轴矩形 createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) plottree.totalw = float(Num_of_leaf(intree)) plottree.totald = float(Depth_of_tree(intree)) plottree.xoff = -0.6 / plottree.totalw;plottree.yoff = 1.2; plottree(intree, (0.5, 1.0), '') plt.show() 执行： 12345678&gt;import treesBackend TkAgg is interactive backend. Turning interactive mode on. #???&gt;myTree=trees.retrieveTree(0)&gt;trees.createPlot(myTree)&gt;myTree['no surfacing'][3]='maybe'&gt;myTree&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;, 3: 'maybe'&#125;&#125;&gt;trees.createPlot(myTree) 为啥没有根节点呢？？ 3.3 测试和存储分类器使用决策树构建分类器，以及实际应用中如何存储分类器。 3.3.1 测试算法：使用决策树执行分类]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战 | 第2章 k-临近算法]]></title>
    <url>%2F5f6e73d7%2F</url>
    <content type="text"><![CDATA[==以下代码全部基于python3== 一、k-临近算法概述 工作原理：存在一个样本数据集合（也称作训练样本集），并且样本集中每个数据都存在标签（即我们知道样本中每一数据与所属分类的对应关系）。输入没有标签的新数据后，将新数据的特征值与样本中数据对应的特征值进行比较，然后算法提取样本集中特征最相似数据（最邻近）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 kNN算法伪代码：对未知类别属性的数据集中的每个点依次执行以下操作：（1）计算已知类别数据集中的点与当前点之间的距离；（2）按照距离递增次序排序；（3）选取与当前点距离最小的k个点；（4）确定前k个点所在类别的出现频率；（5）返回前k个点出现频率最高的类别作为当前点的预测分类。 python3 下kNN算法实现： 1234567891011121314151617181920212223242526272829303132from numpy import *import operator def createDataSet(): group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group,labelsdef classify_KNN(test_X, train_set, labels, K): rows = train_set.shape[0] diff = tile(test_X,(rows,1)) - train_set # 这一行利用tile函数将输入样本实例转化为与训练集同尺寸的矩阵 # 便之后的矩阵减法运算 # .tile若输入一个二元祖，第一个数表示复制的行数，第二个数表示对inx的重复的次数 # .shape[0]代表行数，.shape[1]代表列数 sqDistance = (diff ** 2).sum(axis=1) Distance = sqDistance ** 0.5 sorted_Distance = Distance.argsort() # 对每个训练样本与输入的测试样本求欧几里得距离，即点之间的范数 # 随后按距离由小到大进行排序 # 当axis为0时,是压缩行,即将每一列的元素相加,将矩阵压缩为一行 # 当axis为1时,是压缩列,即将每一行的元素相加,将矩阵压缩为一列 # 如果axis为整数元组（x，y），则是求出axis=x和axis=y情况下得到的和 classCount = &#123;&#125; for i in range(K): vote_label = labels[sorted_Distance[i]] classCount[vote_label] = classCount.get(vote_label,0) +1 sortedClassCount = sorted(classCount.items() , key = operator.itemgetter(1),reverse=True) return sortedClassCount[0][0] 运行： 1234&gt;import kNN&gt;group,labels = kNN.createDataSet()&gt;kNN.classify_KNN([0,0],group,labels,3)'B' 测试分类器最常用的方法：错误率——分类器给出错误结果的次数除以测试执行的总数。 二、示例：使用 k-近邻算法改进约会网站的配对效果1. 将文本记录转换为NumPy的解析程序123456789101112131415161718def file_parse_matrix(filename): with open(filename) as fp: Arr_lines = fp.readlines() number = len(Arr_lines) # 初始化数据为m行3列（飞行里程，游戏时间，冰淇淋数） # 标签单独创建一个向量保存 return_mat = zeros((number, 3)) label_vec = [] index = 0 for line in Arr_lines: line = line.strip() listFromLine = line.split('\t') # 按换行符分割数据 # 将文本数据前三行存入数据矩阵，第四行存入标签向量 return_mat[index, :] = listFromLine[0:3] label_vec.append(listFromLine[-1]) index += 1 return return_mat, label_vec 运行： 123&gt;datingDataMat,datingLabels=kNN.file_parse_matrix('datingTestSet.txt')&gt;datingDataMat&gt;&gt;&gt;datingLabels 2. 分析数据：使用 Matplotlib 创建散点图123456&gt;import matplotlib&gt;import matplotlib.pyplot as plt&gt;fig = plt.figure()&gt;ax = fig.add_subplot(111)&gt;ax.scatter(datingDataMat[:,1], datingDataMat[:,2])&gt;plt.show() //这段代码运行的时候有一点问题，等待后续更正 3. 准备数据：归一化数值数值归一化：处理不同取值范围的特征值，如将取值范围处理为0到1或者-1到1之间。公式：newValue = (oldValue-min)/(max-min)其中min和max分别是数据集中的最小特征值和最大特征值。函数Norm_feature()，可以自动将数字特征值转化为0到1的区间： 1234567891011def Norm_feature(data_set): minVal = data_set.min(0) maxVal = data_set.max(0) ranges = maxVal - minVal # 计算极差 # 下一步将初始化一个与原始数据矩阵同尺寸的矩阵 # 利用tile函数实现扩充向量，并进行元素间的对位运算 norm_set = zeros(shape(data_set)) rows = data_set.shape[0] norm_set = (data_set - tile(minVal, (rows, 1))) / tile(ranges, (rows,1)) return norm_set, ranges, minVal 运行： 1&gt;data_normed, ranges, minV = kNN.Norm_feature(datingDataMat) 4. 测试算法：作为完整程序验证分类器 书上的测试函数没有参数，是自适应函数 此处传入分割参数以及测试集，可以修改测试数值（使用书上的0.1作为分割率） 1234567891011121314151617181920def Test_accuray(split_ratio, test_set, test_label): norm_test, ranges, Min = Norm_feature(test_set) rows = norm_test.shape[0] rows_test = int(rows * split_ratio) error = 0 for i in range(rows_test): Result = classify_KNN(norm_test[i,:], norm_test[rows_test:rows], \ test_label[rows_test:rows], 3) # 参数1表示从测试集（此处约会数据是随机的因此抽取前10%即可）中抽取一个实例 # 参数2，3，4使用后90%作为训练数据，为输入的实例进行投票并分类，K=3 print("the classifier came with: %s, the real answer is :%s " \ % (Result, test_label[i])) if(Result != test_label[i]) : error += 1 # print(type(error)) #for test print("the accuracy is %f | the error_rate is %f " % \ (1- (float(error) /float(rows_test)),(float(error) /float(rows_test)))) 运行： 1&gt;kNN.Test_accuray(0.1,datingDataMat,datingLabels) 三、 示例：手写识别系统1. 准备数据：将图像转换为测试向量实际图像存储在源代码的两个子目录内：目录trainingDigits中包含了大约2000个例子，每个数字大约有200个样本；目录testDigits中包含了大约900个测试数据。我们使用目录trainingDigits中的数据训练分类器，使用目录testDigits中的数据测试分类器的效果。两组数据没有重叠。为了使用前面两个例子的分类器，我们必须将图像格式化处理为一个向量。 123456789101112from os import listdir #从os模块中导入函数listdir，它可以列出给定目录的文件名。def img2vec(filename): '''this is to...将32X32的图像转化为1X1024的行向量''' returnvec = zeros((1, 1024)) with open(filename) as fp: for i in range(32): line = fp.readline() for j in range(32): returnvec[0, 32 * i + j] = int(line[j]) # returnVEC按32进位，j代表每位的32个元素 return returnvec 运行： 123&gt;testVector = kNN.img2vec('digits/testDigits/0_13.txt')&gt;testVector[0,0:31]&gt;testVector[0,32:36] 2.测试算法：使用 k-近邻算法识别手写数字12345678910111213141516171819202122232425262728293031def HandWritingTest(train_dir, test_dir): labels = [] File_list = listdir(train_dir) # 将目录内的文件按名字放入列表，使用函数解析为数字 m = len(File_list) train_mat = zeros((m, 1024)) for i in range(m): fname = File_list[i] fstr = fname.split('.')[0] fnumber = int(fstr.split('_')[0]) # 比如'digits/testDigits/0_13.txt'，被拆分为0,13,txt # 此处0即为标签数字 labels.append(fnumber) train_mat[i, :] = img2vec('%s/%s' % (train_dir, fname)) # labels is label_vec，同之前的KNN代码相同，存储标签 test_File_list = listdir(test_dir) error = 0.0 test_m = len(test_File_list) for i in range(test_m): fname = test_File_list[i] fstr = fname.split('.')[0] fnumber = int(fstr.split('_')[0]) vec_test = img2vec('digits/testDigits/%s' % fname) Result = classify_KNN(vec_test, train_mat, labels, 3) print("the classifier came with: %d, the real answer is :%d " \ % (Result, fnumber)) if (Result != fnumber): error += 1 # 这部分和Test模块相同，直接copy过来就好 print("the accuracy is %f | the error_rate is %f " % \ (1 - (float(error) / float(test_m)), (float(error) / float(test_m)))) 运行： 1&gt;kNN.HandWritingTest('digits/trainingDigits', 'digits/testDigits/')]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西瓜书学习笔记 | 第16章 强化学习]]></title>
    <url>%2Fd6bdaab8%2F</url>
    <content type="text"><![CDATA[16.1 任务与奖赏 强化学习（再励学习）：在种瓜过程中不断摸索，总结出较好种瓜策略的抽象过程。 强化学习任务通常用马尔可夫决策过程（Markov Decision Process,简称MDP)来描述：（1）机器处于环境E中，==状态空间为X==，其中每个状态x∈X是机器感知到的环境的描述。如在种瓜任务上这就是当前瓜苗长势的描述；（2）机器能采取的动作构成了动作空间A。如种瓜过程中有浇水、施不同的肥、使用不同的农药等多种可供选择的动作；（3）若某个==动作a==∈A作用在当前状态x上，则潜在的==转移函数P==将使得环境从当前状态按某种概率转移到另一个状态。如瓜苗状态为缺水，若选择动作浇水，则瓜苗长势会发生变化，瓜苗有一定的概率恢复健康，也有一定的概率无法恢复；（4）在转移到另一个状态的同时，环境会根据潜在的==“奖赏”(reward)函数R==反馈给机器一个奖赏。如保持瓜苗健康对应奖赏+1，瓜苗凋零对应奖赏-10，最终种出了好瓜对应奖赏+100。综合起来，强化学习任务对应了四元组E=&lt;X,A,P, R&gt;，其中P:X x AxX|→IR指定了状态转移概率，R:X xAxX|→IR指定了奖赏；在有的应用中，奖赏函数可能仅与状态转移有关，即R:XxX|→IR。 给西瓜浇水问题的马尔可夫决策过程：a：动作p：转移概率r：返回的奖赏 机器要做的是通过在环境中不断地尝试而学得一个“策略”(policy)π，根据这个策略，在状态x下就能得知要执行的动作a=π(x)。策略有两种表示方法：（1）表示为函数π:X |→A，确定性策略常用这种表示；（2）概率表示π:XxA|→IR，随机性策略常用这种表示，π(x, a)为状态x下选择动作a的概率，这里必须有策略的优势取决于长期执行这一策略后得到的累积奖赏。 强化学习中，学习的目的：找到能使长期积累奖赏最大化的策略。 长期积累奖赏计算方式：（1）T步累计奖赏：（2）γ折扣累计奖赏：其中rt表示第t步获得的奖赏值，IE表示对所有随机变量求期望。 强化学习与监督学习：强化学习中的“状态”对应为监督学习中的“示例”、“动作” 对应为“标记”，“策略”对应为“分类器”(当动作是离散的)或“回归器”(当动作是连续的)。不同：在强化学习中并没有监督学习中的有标记样本(即“示例-标记”对)。换言之，没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过“反思”之前的动作是否正确来进行学习。强化学习在某种意义上可看作具有“延迟标记信息”的监督学习问题。16.2 K-摇臂赌博机16.2.1 探索与利用 最大化单步奖赏，即仅考虑一步操作。 欲最大化单步奖赏需考虑两个方面：（1）需知道每个动作带来的奖赏，（2）要执行奖赏最大的动作。 K-摇臂赌博机(K-armed bandit)：单步强化学习任务对应的一个理论模型。 若仅为获知每个摇臂的期望奖赏，则可采用“仅探索”(exploration-only)法：将所有的尝试机会平均分配给每个摇臂(即轮流按下每个摇臂)，最后以每个摇壁各白的平均叶币概率作为其奖常期望的近似估计。若仅为执行奖常最大的动作，则可采用“仅利用“（exploitation-only）法：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。“仅探索”法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；“仅利用”法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。 16.2.2 ∈-贪心 ∈-贪心法基于一个概率来对探索和利用进行折中：每次尝试时，以∈的概率进行探索，即以均匀概率随机选取一个摇臂；以1 -∈的概率进行利用，即选择当前平均奖赏最高的摇臂(若有多个，则随机选取一个)。 令Q(k)记录摇臂k的平均奖赏：若摇臂k被尝试了n次，得到的奖赏为16.2.3 Softmax 16.3 有模型学习16.3.1 策略评估16.3.2 策略改进16.3.3 策略迭代与值迭代16.4 免模型学习16.4.1 蒙特卡罗强化学习16.4.2 时序差分学习16.5 值函数近似16.6 模仿学习16.6.1 直接模仿学习16.6.2 逆强化学习]]></content>
      <categories>
        <category>西瓜书学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西瓜书学习笔记 | 第15章 规则学习]]></title>
    <url>%2Fa75704a9%2F</url>
    <content type="text"><![CDATA[15.1 基本概念 规则：机器学习中通常指语义明确、能描述数据分布所隐含的客观规律或领域概念、可写成“若……，则……”形式的逻辑规则。 规则学习（rule leaning）：是从训练数据中学习出一组能用来对未见实例进行判别的规则。 形式化地看，一条规则形如： 规则学习的优势：（1）与神经网络、支持向量机这样的“黑箱模型”相比，规则学习具有更好的可解释性，能使用户更直观地对判别过程有所了解；（2）规则学习能更自然地在学习过程中引入领域知识；（3）逻辑规则的抽象描述能力在处理一些高度复杂的AI任务时具有显著的优势。 覆盖：符合规则的样本称为被该规则覆盖。 冲突：规则集合中的每条规则都可看作一一个子模型，规则集合是这些子模型的一个集成。当同一个示例被判别结果不同的多条规则覆盖时，称发生了“冲突”(confict)，解决冲突的办法称为“冲突消解”(conflict resolution).。 常用的冲突消解策略：（1）投票法：将判别相同的规则数最多的结果作为最终结果。（2）排序法：在规则集合上定义一个顺序，在发生冲突时使用排序最前的规则，相应的规则学习过程称为“带序规则”(orderedrule)学习或“优先级规则”(priority rule)学习。（3）元规则法：根据领域知识事先设定-些“元规则”(meta-rule),，即关于规则的规则，例如“发生冲突时使用长度最小的规则”，然后根据元规则的指导来使用规则集。 规则学习算法通常会设置一条“默认规则”(default rule)，由它来处理规则集合未覆盖的样本。 从形式语言表达能力而言，规则可分为两类：（1）“命题规则“(propositionalrule)：由“原子命题”(propositionalatom)和逻辑连接词“与”(∧)、“或” (V)、“非” (﹁)和“蕴含”(←)构成的简单陈述句；（2）“一阶规则”(first- order rule)/关系型规则（relational rule）：基本成分是能描述事物的属性或关系的“原子公式”(atomic formula)。 从形式语言系统的角度看，命题规则是一阶规则的特例，一阶规则的学习要比命题规则复杂的多。 15.2 序贯覆盖 规则学习的目标：产生一个能覆盖尽可能多的样例的规则集。 最直接的做法：“序贯覆盖”(sequential covering)，即逐条归纳：在训练集上每学到一条规则，就将该规则覆盖的训练样例去除，然后以剩下的训练样例组成训练集重复上述过程，由于每次只处理一部分数据，因此也被称为“分治”(separate and-conquer)策略。 序贯覆盖法的关键：如何从训练集学出单条规则。对规则学习目标⊕，产生一条规则就是寻找最优的一组逻辑文字来构成规则体，这是一个搜索问题。 最简单的做法是从空规则“⊕←”开始，将正例类别作为规则头，再逐个遍历训练集中的每个属性及取值，尝试将其作为逻辑文字增加到规则体中，若能使当前规则体仅覆盖正例,则由此产生一条规则，然后去除已被覆盖的正例并基于剩余样本尝试生成下一条规则。 现实任务中一般有两种策略来产生规则：（1）“自顶向下”(top-down)，即从比较一般的规则开始，逐渐添加新文字以缩小规则覆盖范围，直到满足预定条件为止；亦称为“生成-测试”(generate-then-test)法，是规则逐渐“特化”(specialization)的过程。覆盖范围从大往小搜索规则，更容易产生泛化性能较好的规则。（2）“自底向上”(bottom-up)，即从比较特殊的规则开始，逐渐删除文字以扩大规则覆盖范围，直到满足条件为止;亦称为“数据驱动”(data driven)法，是规则逐渐“泛化”(generalization)的过程。覆盖范围从小往大搜索规则，适合于训练样本较少的情形。前者对噪声的鲁棒性比后者要强得多。因此，在命题规则学习中通常使用第一种策略,而第二种策略在一阶规则学习这类假设空间非常复杂的任务上实用得多。 “集束搜索”(beam search),：每轮保留最优的b个逻辑文字，在下一轮均用于构建候选集，再把候选集中最优的b个留待再下一轮使用。 15.3 剪枝优化 剪枝(pruning)：规则生成本质上是一个贪心搜索过程，需有一定的机制来缓解过拟合的风险，最常见的做法是剪枝(pruning)。 与决策树相似，剪枝可发生在规则生长过程中，即“预剪枝”，也可发生在规则产生后，即“后剪枝”。通常是基于某种性能度量指标来评估增/删逻辑文字前后的规则性能，或增/删规则前后的规则集性能,从而判断是否要进行剪枝。 剪枝还可借统计显著性检验来进行：方法：CN2算法[Clark and Niblett,1989]：在预剪枝时，假设用规则集进行预测必须显著优于直接基于训练样例集后验概率分布进行预测。CN2使用了似然率统计量(LikelihoodRatio Statistics,简称LRS)。这实际上是一种信息量指标，衡量了规则(集)覆盖样例的分布与训练集经验分布的差别：LRS越大，说明采用规则(集)进行预测与直接使用训练集正、反例比率进行猜测的差别越大；LRS越小，说明规则(集)的效果越可能仅是偶然现象。在数据量比较大的现实任务中，通常设置为在LRS很大(例如0.99)时CN2算法才停止规则(集)生长。 后剪枝最常用的策略是“减错剪枝”(Reduced Error Pruning,简称REP)：其基本做法是：将样例集划分为训练集和验证集，从训练集上学得规则集R后进行多轮剪枝，在每一轮穷举所有 可能的剪枝操作，包括删除规则中某个文字、删除规则结尾文字、删除规则尾部多个文字、删除整条规则等，然后用验证集对剪枝产生的所有候选规则集进行评估保留最好的那个规则集进行下一轮剪枝，如此继续，直到无法通过剪枝提高验证集上的性能为止。 REP：复杂度是O(m^4)IREP (Incremental REP) ：复杂度O(mlog2m)做法：在生成每条规则前，先将当前样例集划分为训练集和验证集，在训练集上生成一条规则r，立即在验证集上对其进行REP剪枝，得到规则r，将r覆盖的样例去除，在更新后的样例集上重复上述过程。REP是针对规则集进行剪枝，而IREP仅对单条规则进行剪枝，因此后者比前者更高效。 规则学习算法RIPPER： RIPPER中的后处理机 制是为了在剪枝的基础上进一步提升性能。对R中的每条规则ri,，RIPPER为它产生两个变体： 15.4 一阶规则学习 关系数据：直接描述样例间的关系。 背景知识：由原样本属性转化而来的“色泽更深”“根蒂更蜷”等原子公式。 由样本类别转化而来的关于“更好”“﹁更好”的原子公式。 FOIL (First-Order Inductive Learner) ：是著名的一阶规则学习算法，它遵循序贯覆盖框架且采用自顶向下的规则归纳策略。 FOIL使用“FOIL增益”（FOIL gain）来选择文字： 15.5 归纳逻辑程序设计 归纳逻辑程序设计(Inductive Logic Programming,简称ILP)：在一阶规则学习中引入了函数和逻辑表达式嵌套一方面，这使得机器学习系统具备了更为强大的表达能力；另一方面，ILP可看作用机器学习技术来解决基于背景知识的逻辑程序(logic program)归纳，其学得的“规则”可被PROLOG等逻辑程序设计语言直接使用。15.5.1 最小一般泛化 归纳逻辑程序设计采用==自底向上==的规则生成策略，直接将一个或多个正例所对应的具体事实(grounded fact)作为初始规则，再对规则逐步进行泛化以增加其对样例的覆盖率。泛化操作可以是将规则中的常量替换为逻辑变量，也可以是删除规则体中的某个文字。 最小一般泛化(Least General Generalization,简称LGG) ：用于将“特殊”规则转变为更“一般”的规则。15.5.2 逆归结 四种完备的逆归结操作：以规则形式p←q等价地表达pV﹁q，并假定用小写字母表示逻辑文字、大写字母表示合取式组成的逻辑子句： “置换”(substitution)是用某些项来替换逻辑表达式中的变量。 “合一”(unification)是用一种变量置换令两个或多个逻辑表达式相等。 逆归结的一大特点：能够自动发明新谓词，这些谓词可能对应于样例属性和背景知识中不存在的新知识，对知识发现与精化有重要意义。 ILP系统通常先自底向上生成一组规则，然后再结合最小一般泛化与逆归结做进一步学习。]]></content>
      <categories>
        <category>西瓜书学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西瓜书学习笔记 | 第14章 概率图模型]]></title>
    <url>%2F5028c23a%2F</url>
    <content type="text"><![CDATA[概率图模型（probabilistic model）（变量关系图）：是一类用图来表达变量相关关系的概率模型。一个结点：表示一个或一组随机变量。边：表示变量间的概率相关关系。 14.1 隐马尔科夫模型 概率模型：将学习任务归结于计算变量的概率分布。 推断：利用已知变量推测未知变量的条件分布。 “生成式”模型：考虑联合概率分布P(Y,R|O)。“判别式”模型：考虑条件分布P(Y|O)。（Y：变量集合O：可观测变量集合 R：其他变量的集合） 隐马尔科夫模型（Hidden Markov Model：简称HMM）：是结构最简单的动态贝叶斯网，有向图模型，==生成式模型==。 隐马尔科夫模型中的变量：状态变量（隐变量）、观测变量。 状态（离散空间）空间：状态变量的取值范围。 马尔科夫链（Markov chain）：系统下一时刻的状态仅有当前状态决定，不依赖于以往的任何状态。基于这种关系，所有变量的联合概率分布为： 确定一个隐马尔科夫模型需要以下三组参数：) 产生观测序列的过程： 在实际应用中，人们常关注隐马尔科夫模型的三个基本问题：14.2 马尔可夫随机场 马尔可夫随机场（Markov Random Field，简称MRF）：是典型的马尔可夫网，无向图模型，==生成式模型==。 势函数（potential functions）：亦称”因子“（factor），是定义在变量子集上的非负实函数，主要用于定义概率分布函数。 团（clique）：对于上图中节点的一个子集，若其中任意两结点间都有边连接，则该结点子集为一个“团”。 极大团（maximal clique）：在一个团中加入任何一个结点都不再形成团。不能被其他团所包含的团。 马尔可夫随机场中，多个变量之间的联合概率定义（基于团）：所有团构成的集合为C，团Q∈C。) 马尔可夫随机场中，多个变量之间的联合概率定义（基于极大团）： 分离集（separating set）：如图，从结点集A中的结点到B中的结点都必须经过结点集C中的结点，则称结点集A和B被结点集C分离，C成为“分离集”。 全局马尔可夫性（global Markov property）：给定两个变量子集的分离集，则这两个变量子集条件独立。 由全局马尔可夫性得到的两个很有用的推论：局部马尔可夫性（local Markov property）：给定某变量的邻接变量，则该变量条件独立于其他变量。成对马尔可夫性（pairwise Markov property）：给定所有其他变量，两个非邻接变量条件独立。 马尔可夫随机场中的势函数：作用：定量刻画变量集中变量之间的相关关系，它是非负函数，且在所偏好的变量取值上有较大函数值。为了满足非负性，指数函数常被用于定义势函数：14.3 条件随机场 条件随机场（Conditional Random Field，简称CRF）：是一种==判别式==无向图模型。 条件随机场试图对多个变量在给定观测值后的条件概率进行建模。在自然语言处理的词性标注任务中，观测数据为语句（即单词序列），标记为相应的词性序列，具有线性序列结构；在语法分析任务中，输出标记则是语法树，具有树形结构。 条件随机场定义： 链式条件随机场（chain-structured CRF）：条件随机场使用势函数和图结构上的团来定义条件概率P（y|x）。链式条件随机场主要包含两种关于标记变量的团：单个标记变量和相邻的标记变量。 在条件随机场中，通过选用指数势函数并引入特征函数，条件概率被定义为： 特征函数：特征函数通常是实值函数，以刻画数据的一些很可能成立或期望成立的经验特征。例：14.5（a）的词性标注，若采用转移特征函数： 条件随机场和马尔可夫随机场均使用团上的势函数定义概率，两者在形式上没有显著差别；但条件随机场处理的是条件概率，而马尔科夫随机场处理的是联合概率。 14.4 学习与推断 边际分布：指对无关变量求和或积分后得到结果。 边际化：对联合分布中其他无关变量进行积分的过程。 参数估计（参数学习问题）：对概率图模型，确定具体分布的参数。通常使用极大似然估计或最大后验概率估计求解。 推断问题的目标和关键： 概率图模型的推断方法大致可分为两类：第一类：精确推断方法——变量消去、信念传播。第二类：近似推断方法——MCMC采样、变分推断。 14.4.1 变量消去 精确推断的实质是一类动态规划算法，它利用图模型所描述的条件独立行来削减计算目标概率值所需要的计算量。 有向图模型：)) 无向图模型：上述方法对无向图模型同样适用，忽略掉图14.7（a）中的箭头，将其看作一个无向图模型，有 变量消去法有一个明显的缺点：若需计算多个边际分布，重复使用变量消去法将会造成大量的冗余计算。 14.4.2 信念传播 信念传播（Belief Propagation）算法：将变量消去法中的求和操作看作一个消息传递过程，较好地解决了求解多个边际分布时的重复计算问题。 在信念传播算法中，一个结点仅在接收到来自其他所有结点的消息后才能向另一个结点发送消息，且结点的边际分布正比于它所接收的消息的乘积，即 若图结构中没有环，则信念传播算法经过两个步骤即可完成所有消息传递，进而能计算所有变量上的边际分布： ●指定一个根结点，从所有叶结点开始向根结点传递消息，直到根结点收到 所有邻接结点的消息; ●从根结点开始向叶结点传递消息，直到所有叶结点均收到消息。例： 14.5 近似推断 近似推断方法大致可分为两大类：第一类：采样(sampling)，通过使用随机化方法完成近似；第二类：使用确定性近似完成近似推断，典型代表为变分推断(variational inference)。 14.5.1 MCMC采样 采样思路：假设我们的目标是计算函数f(x)在概率密度函数p(x)下的期望以此来近似目标期望E[f]。 概率图模型中最常用的采样技术是马尔可夫链蒙特卡罗(Markov ChainMonte Carlo,简称MCMC)方法。给定连续变量x∈X的概率密度函数p(x)，x在区间A中的概率可计算为：若有函数f:X→R，则可计算f(x)的期望 计算方法：MCMC先构造出服从p分布的独立同分布随机变量X1,2….. xN，再得到上式的无偏估计 MCMC方法的关键：通过构造“平稳分布为p的马尔可夫链”来产生样本：若马尔可夫链运行时间足够长(即收敛到平稳状态)，则此时产出的样本x近似服从于分布p。 如何判断马尔可夫链到达平稳状态呢？假定平稳马尔可夫链T的状态转移概率(即从状态x转移到状态x’的概率)为T(x’ |x)，t时刻状态的分布为p(x*)，则若在某个时刻马尔可夫链满足平稳条件则p(x)是该马尔可夫链的平稳分布，且马尔可夫链在满足该条件时已收敛到平稳状态。 MCMC方法：先设法构造一条马尔可夫链，使其收敛至平稳分布恰为待估计参数的后验分布，然后通过这条马尔可夫链来产生符合后验分布的样本，并基于这些样本来进行估计。 Metropolis Hastings (简称MH)算法（MCMC的重要代表）：它基于“拒绝采样”(reject sampling)来逼近平稳分布p。 为达到平稳状态，只需要将接受率设置为： 吉布斯采样(Gibbs sampling)：有时被视为MH算法的特例，它也使用马尔可夫链获取样本，而该马尔可夫链的平稳分布也是采样的目标分布p(x)。采样步骤：(1)随机或以某个次序选取某变量xi；(2) 根据x中除xi外的变量的现有取值,计算条件概率p(xi | x)，其中xq= {x1,W2…,Xi -1,Ti+1,…,xN} ；(3)根据p(xi I xq)对变量xi采样，用采样值代替原值。 14.5.2 变分推断 变分推断：通过使用已知简单分布来逼近需推断的复杂分布，并通过限制近似分布的类型，从而得到种局部最优、但具有确定解的近似后验分布。 盘式记法(plate notation) ：概率图模型一种简洁的表示方法。14.6 话题模型 话题模型(topic model)：是一族==生成式==有向图模型，主要用于处理离散型的数据(如文本集合)，在信息检索、自然语言处理等领域有广泛应用。 隐狄利克雷分配模型(Latent Dirichlet Allocation, 简称LDA)：话题模型的典型代表。]]></content>
      <categories>
        <category>西瓜书学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用GitHub+hexo从零开始搭建个人博客]]></title>
    <url>%2Fa13da74f%2F</url>
    <content type="text"><![CDATA[作为一个技术小白，一直想搭建一个自己的博客，但是不知道从哪里开始。偶然间知道GitHub可以搭建博客，并且是免费的。所以抱着学习的态度，开始了自己第一个博客的搭建。整个搭建过程真的很简单，网上的很多教程也都说的非常详细，所以我的这篇博客主要来记录一下我在搭建过程中参考的一些文章，并且自己遇到的一些坑，下面就开始啦~ 一、准备工作1.注册一个GitHub账号2.安装Git3.安装Node.js4.安装Hexo 二、参考文章整个搭建过程可以参考：基于hexo+github搭建一个独立博客.此博主写的非常详细，也填补了一些坑。此外还有： 1.GitHub相关：使用Github搭建属于自己的博客. 利用github搭建hexo个人静态博客网站.GitHub的注册与使用.一个小时学会Git. 2.Hexo相关：hexo安装文档.NexT使用文档.使用hexo发布文章.Hexo使用攻略-添加分类及标签.hexo高阶教程：next主题优化之加入网易云音乐、网易云跟帖、炫酷动态背景、自定义样式，打造属于你自己的定制化博客.hexo博客安装RSS插件.hexo博客常用插件及教程.hexo高级玩法. 3.SEO相关：hexo搭建个人博客之seo优化.Hexo博客之后续SEO优化. 4.Markdown相关：Markdown基本语法.最新主流 Markdown 编辑器推荐. 5.遇到的一些坑：Git之右键没有Git Bash Here的解决办法.hexo博客不蒜子统计不显示/无效.在搭建blog所遇到的问题.Hexo生成sitemap步骤及duplicated mapping key 问题解决.git warning: LF will be replaced by CRLF in 解决办法. 三、小小总结搭好博客后第一篇文章就到这里啦，其实也算不上什么技术原创文章，只是简单把自己看过的文章做个小小整理和记录。之所以搭建这个博客，主要还是想把自己的学习过程记录下来，也可以作为自己的小树洞，记录一下生活~既然有了自己的小博客，希望自己以后可以常常更新。技术小白也要更加努力的学技术，虽然现在的自己对未来还有一些迷茫，但是希望在学习过程中尽快找到方向，并且向大佬们看齐！加油^^]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Hexo</tag>
        <tag>Git</tag>
        <tag>Node</tag>
        <tag>SEO</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
</search>

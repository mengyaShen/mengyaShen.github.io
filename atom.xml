<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shenmy&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://mengyashen.github.io/"/>
  <updated>2019-11-09T09:33:26.079Z</updated>
  <id>http://mengyashen.github.io/</id>
  
  <author>
    <name>Shen Mengya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习 | 感知机</title>
    <link href="http://mengyashen.github.io/c7c854c0/"/>
    <id>http://mengyashen.github.io/c7c854c0/</id>
    <published>2019-11-09T08:54:40.000Z</published>
    <updated>2019-11-09T09:33:26.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、感知机是什么"><a href="#一、感知机是什么" class="headerlink" title="一、感知机是什么"></a>一、感知机是什么</h1><p>感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。<br><img src="https://img-blog.csdnimg.cn/20191109162350612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="两个输入的感知机"><br>x1、x2是输入信号，y是输出信号，w1、w2是权重（w是weight的首字母）。○称为“神经元”或者“节点”。<br>输入信号被送往神经元时，会被分别乘以固定的权重（w1x1、w2x2）。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活”。这里将这个界限值称为阈值，用符号θ表示。<br><img src="https://img-blog.csdnimg.cn/20191109162325229.png" alt="感知机数学公式"></p><h1 id="二、简单逻辑电路（单层感知机）"><a href="#二、简单逻辑电路（单层感知机）" class="headerlink" title="二、简单逻辑电路（单层感知机）"></a>二、简单逻辑电路（单层感知机）</h1><h2 id="2-1-与门"><a href="#2-1-与门" class="headerlink" title="2.1 与门"></a>2.1 与门</h2><p><img src="https://img-blog.csdnimg.cn/2019110916254426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="与门"><br>参数的选择方法如：(w1, w2, θ) = (0.5, 0.5, 0.7)</p><h2 id="2-2-与非门"><a href="#2-2-与非门" class="headerlink" title="2.2 与非门"></a>2.2 与非门</h2><p><img src="https://img-blog.csdnimg.cn/20191109162630100.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="与非门"><br>参数的选择方法如：(w1, w2, θ) = (−0.5, −0.5, −0.7)</p><h2 id="2-3-或门"><a href="#2-3-或门" class="headerlink" title="2.3 或门"></a>2.3 或门</h2><p><img src="https://img-blog.csdnimg.cn/20191109162708271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="或门"><br>参数的选择方法如：(w1, w2, θ) = (1.0, 1.0,−0.5)</p><p>与门、与非门、或门是单层感知机。</p><h1 id="三、多层感知机"><a href="#三、多层感知机" class="headerlink" title="三、多层感知机"></a>三、多层感知机</h1><h2 id="异或门"><a href="#异或门" class="headerlink" title="异或门"></a>异或门</h2><p><img src="https://img-blog.csdnimg.cn/20191109163555571.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="异或门"><br>使用单层感知机无法实现异或门，需要“叠加层”。使用2层感知机。<br><img src="https://img-blog.csdnimg.cn/2019110916393365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="异或门"><br>s1作为与非门的输出，把s2作为或门的输出<br><img src="https://img-blog.csdnimg.cn/2019110916390665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="2层感知机"><br>1）第0层的两个神经元接收输入信号，并将信号发送至第1层的神经元。<br>2）第1层的神经元将信号发送至第2层的神经元，第2层的神经元输出y。</p><h1 id="四、权重和偏置"><a href="#四、权重和偏置" class="headerlink" title="四、权重和偏置"></a>四、权重和偏置</h1><p>将阈值θ换成−b，因此感知机的表示：<br><img src="https://img-blog.csdnimg.cn/20191109164539519.png" alt="感知机的表示"><br>b称为偏置，用于控制神经元被激活的容易程度。w1和w2称为权重。<br>感知机会计算输入信号和权重的乘积，然后加上偏置，如果这个值大于0则输出1，否则输出0。</p><h1 id="五、线性和非线性"><a href="#五、线性和非线性" class="headerlink" title="五、线性和非线性"></a>五、线性和非线性</h1><p>区分：能否用一条直线分割空间。<br><img src="https://img-blog.csdnimg.cn/20191109165919718.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="线性空间"><br>可以用一条直线将○和△分开，因此该空间为线性空间。<br><img src="https://img-blog.csdnimg.cn/20191109164959384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="非线性空间"><br>不能用一条直线将○和△分开（使用曲线可以分开），因此该空间为非线性空间。</p><h1 id="六、小结"><a href="#六、小结" class="headerlink" title="六、小结"></a>六、小结</h1><pre><code>• 感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。• 感知机将权重和偏置设定为参数。• 使用感知机可以表示与门和或门等逻辑电路。• 异或门无法通过单层感知机来表示。• 使用2层感知机可以表示异或门。• 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。• 多层感知机（在理论上）可以表示计算机。</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、感知机是什么&quot;&gt;&lt;a href=&quot;#一、感知机是什么&quot; class=&quot;headerlink&quot; title=&quot;一、感知机是什么&quot;&gt;&lt;/a&gt;一、感知机是什么&lt;/h1&gt;&lt;p&gt;感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。&lt;br&gt;&lt;img src=
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://mengyashen.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://mengyashen.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Computer Vision | LocalImageFeaturesAndMatching</title>
    <link href="http://mengyashen.github.io/5c655b74/"/>
    <id>http://mengyashen.github.io/5c655b74/</id>
    <published>2019-10-22T06:27:05.000Z</published>
    <updated>2019-10-22T06:30:23.359Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、哈里斯角的不变性"><a href="#一、哈里斯角的不变性" class="headerlink" title="一、哈里斯角的不变性"></a>一、哈里斯角的不变性</h1><h2 id="Harris-Corners-哈里斯角"><a href="#Harris-Corners-哈里斯角" class="headerlink" title="Harris Corners 哈里斯角"></a>Harris Corners 哈里斯角</h2><p><img src="https://img-blog.csdnimg.cn/20191021200846726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="current window"><br>结构矩阵：<br><img src="https://img-blog.csdnimg.cn/20191021201247779.png" alt="在这里插入图片描述"><br>2.  <img src="https://img-blog.csdnimg.cn/20191021201354598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="current window"><br>结构矩阵：<br><img src="https://img-blog.csdnimg.cn/20191021201435682.png" alt="在这里插入图片描述"></p><h2 id="Affine-intensity-change-仿射强度变化"><a href="#Affine-intensity-change-仿射强度变化" class="headerlink" title="Affine intensity change 仿射强度变化"></a>Affine intensity change 仿射强度变化</h2><p><img src="https://img-blog.csdnimg.cn/2019102120170865.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191021201720923.png" alt="在这里插入图片描述"><br>仅使用导数=&gt;强度偏移I-&gt;I+b的不变性</p><p>强度缩放：I -&gt;a I<br><img src="https://img-blog.csdnimg.cn/20191021202626697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="Image-translation-图像翻译"><a href="#Image-translation-图像翻译" class="headerlink" title="Image translation 图像翻译"></a>Image translation 图像翻译</h2><p>导数和窗函数是平移不变的。<br><img src="https://img-blog.csdnimg.cn/20191021202503646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="Image-rotation-图像旋转"><a href="#Image-rotation-图像旋转" class="headerlink" title="Image rotation 图像旋转"></a>Image rotation 图像旋转</h2><p><img src="https://img-blog.csdnimg.cn/20191021202950478.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="图像旋转"></p><h2 id="Scaling-缩放比例"><a href="#Scaling-缩放比例" class="headerlink" title="Scaling 缩放比例"></a>Scaling 缩放比例</h2><p><img src="https://img-blog.csdnimg.cn/20191021203032145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="缩放比例"><br>所有点都将被分类为边。<br>角点位置与缩放不相关！</p><p>到目前为止：可以在x-y中定位，但不能缩放。</p><h1 id="二、特征点的“比例”"><a href="#二、特征点的“比例”" class="headerlink" title="二、特征点的“比例”"></a>二、特征点的“比例”</h1><h2 id="Automatic-Scale-Selection-自动刻度选择"><a href="#Automatic-Scale-Selection-自动刻度选择" class="headerlink" title="Automatic Scale Selection 自动刻度选择"></a>Automatic Scale Selection 自动刻度选择</h2><p><img src="https://img-blog.csdnimg.cn/20191021203558875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="自动刻度选择"></p><h3 id="如何找到f响应相等的补丁大小？什么是好的f？"><a href="#如何找到f响应相等的补丁大小？什么是好的f？" class="headerlink" title="如何找到f响应相等的补丁大小？什么是好的f？"></a>如何找到f响应相等的补丁大小？什么是好的f？</h3><p>增加标度的函数响应（标度特征）。<br><img src="https://img-blog.csdnimg.cn/20191021205049306.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191021205121394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191021205141972.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191021204921440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191021205203446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="什么是有用的特征函数f？"><a href="#什么是有用的特征函数f？" class="headerlink" title="什么是有用的特征函数f？"></a>什么是有用的特征函数f？</h3><p><img src="https://img-blog.csdnimg.cn/20191021205257986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>“Blob（斑点）”探测器常见于拐角处<br>——高斯（LoG）的拉普拉斯（二阶导数）<br><img src="https://img-blog.csdnimg.cn/20191021205652416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="Blob"></p><h3 id="在位置标度空间中求局部极大值"><a href="#在位置标度空间中求局部极大值" class="headerlink" title="在位置标度空间中求局部极大值"></a>在位置标度空间中求局部极大值</h3><p><img src="https://img-blog.csdnimg.cn/20191021210305791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在位置标度空间中求局部极大值"></p><h2 id="Alternative-approach-替代方法"><a href="#Alternative-approach-替代方法" class="headerlink" title="Alternative approach 替代方法"></a>Alternative approach 替代方法</h2><p>高斯差分（DoG）近似LoG：<br><img src="https://img-blog.csdnimg.cn/20191021210031173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="高斯差分"><br><img src="https://img-blog.csdnimg.cn/20191021210156874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="高斯差分"><br><img src="https://img-blog.csdnimg.cn/20191021210547282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="高斯差分"></p><h2 id="Interest-points-兴趣点"><a href="#Interest-points-兴趣点" class="headerlink" title="Interest points 兴趣点"></a>Interest points 兴趣点</h2><p>关键点检测：可重复且独特<br>角点（Corners）、斑点（blobs）、稳定区域（stable regions）<br>哈里斯（Harris），DoG</p><h2 id="选择兴趣点检测器"><a href="#选择兴趣点检测器" class="headerlink" title="选择兴趣点检测器"></a>选择兴趣点检测器</h2><ol><li>为什么选择？<br>用更多的探测器收集更多的点，以便进行更多可能的匹配。</li><li>你想要干什么？<br>x-y精确定位：Harris<br>良好的尺度局部化：高斯差分（Difference of Gaussian）<br>柔性区域形状：MSER</li><li>最佳选择通常取决于应用程序<br>harris-/hessian-laplace/dog在许多自然类别中都很有效<br>MSER适用于建筑和印刷品</li></ol><h1 id="三、Local-Image-Descriptors-局部图像描述"><a href="#三、Local-Image-Descriptors-局部图像描述" class="headerlink" title="三、Local Image Descriptors 局部图像描述"></a>三、Local Image Descriptors 局部图像描述</h1><p>局部特征：主要组件<br><img src="https://img-blog.csdnimg.cn/2019102211154235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="局部图像描述"></p><h2 id="Image-representations-图像表示"><a href="#Image-representations-图像表示" class="headerlink" title="Image representations 图像表示"></a>Image representations 图像表示</h2><ol><li>模板：强度、阶级等。</li><li>直方图：颜色、纹理、SIFT描述符等。</li></ol><h2 id="图像表示：直方图（Histograms）"><a href="#图像表示：直方图（Histograms）" class="headerlink" title="图像表示：直方图（Histograms）"></a>图像表示：直方图（Histograms）</h2><p><img src="https://img-blog.csdnimg.cn/2019102121183541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="直方图"></p><ol><li><p>联合直方图（Joint histogram）<br>需要大量数据<br>避免空箱子的分辨率降低</p></li><li><p>边缘直方图（Marginal histogram）<br>需要独立功能<br>比联合直方图更多的数据（data）/bin</p></li><li><p>聚类（Clustering）：对所有图像使用相同的聚类中心<br><img src="https://img-blog.csdnimg.cn/20191021212134675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li></ol><h2 id="Computing-histogram-distance-计算直方图距离"><a href="#Computing-histogram-distance-计算直方图距离" class="headerlink" title="Computing histogram distance 计算直方图距离"></a>Computing histogram distance 计算直方图距离</h2><p><img src="https://img-blog.csdnimg.cn/20191022103257694.png" alt="计算直方图距离"><br>直方图相交（假设为标准化直方图）<br><img src="https://img-blog.csdnimg.cn/20191022103427942.png" alt="直方图相交"></p><h2 id="直方图：实现问题"><a href="#直方图：实现问题" class="headerlink" title="直方图：实现问题"></a>直方图：实现问题</h2><ol><li>量化（Quantization）<br>网格（Grids）：快速，但仅适用于很少的维度；<br>聚类（Clustering）：速度较慢，但可以量化更高维度的数据。</li><li>匹配（Matching）<br>直方图相交或欧几里德可能更快；<br>卡方检验（Chi-squared）通常效果更好；<br>当附近的bins代表相似的值时，土方运输机的距离（Earth mover’s distance）更有利。<h2 id="计算直方图的目的"><a href="#计算直方图的目的" class="headerlink" title="计算直方图的目的"></a>计算直方图的目的</h2></li><li>颜色</li><li>模型局部外观</li><li>纹理</li><li>定向梯度的局部直方图</li><li>sift：尺度不变特征变换——非常受欢迎（4万条引用）<br><img src="https://img-blog.csdnimg.cn/20191022104547377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="image gradients"><h2 id="SIFT描述符格式"><a href="#SIFT描述符格式" class="headerlink" title="SIFT描述符格式"></a>SIFT描述符格式</h2></li><li>在本地16 x 16窗口进行计算。</li><li>根据发现的方向θ和标度σ（增益不变性）旋转和缩放窗口。</li><li>计算由一半窗口的高斯方差加权的梯度（用于平滑衰减）。<br><img src="https://img-blog.csdnimg.cn/20191022104915414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="SIFT"><h2 id="SIFT矢量形成"><a href="#SIFT矢量形成" class="headerlink" title="SIFT矢量形成"></a>SIFT矢量形成</h2>梯度方向直方图的4x4数组，按梯度大小加权。<br>按8个方向放置X 4x4阵列=128维。<br><img src="https://img-blog.csdnimg.cn/20191022105048113.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="SIFT矢量"><br>保证平滑度<br><img src="https://img-blog.csdnimg.cn/20191022105743138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="保证平滑度"><br>降低光源影响<h2 id="SIFT"><a href="#SIFT" class="headerlink" title="SIFT"></a>SIFT</h2></li><li>高斯尺度空间极值的差分</li><li>后处理<br>位置插值<br>丢弃低对比度点<br>沿边消除点</li><li>方位估计</li><li>描述符提取<br>动机：我们希望对空间布局有一些敏感度，但不要太多，所以直方图块给了我们这些。<h2 id="SIFT定向规范化"><a href="#SIFT定向规范化" class="headerlink" title="SIFT定向规范化"></a>SIFT定向规范化</h2>计算方向直方图；<br>选择主导方向θ；<br>规格化：旋转到固定方向。<h2 id="SIFT描述符提取"><a href="#SIFT描述符提取" class="headerlink" title="SIFT描述符提取"></a>SIFT描述符提取</h2>给定具有比例和方向的关键点：</li><li>选取与估计尺度最接近的尺度空间图像；</li><li>重新采样图像以匹配方向或从矢量中减去检测器的方向，使图像旋转不变性。<br><img src="https://img-blog.csdnimg.cn/20191022110433517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="SIFT描述符提取"><h2 id="局部描述符：SURF"><a href="#局部描述符：SURF" class="headerlink" title="局部描述符：SURF"></a>局部描述符：SURF</h2>SIFT思想的快速逼近：用2D盒滤波器和积分图像进行有效计算比SIFT快6倍物体识别的等效质量<h2 id="局部描述符"><a href="#局部描述符" class="headerlink" title="局部描述符"></a>局部描述符</h2></li><li>大多数特性可以看作是模板、直方图（计数）或组合。</li><li>理想的描述符应该是：<br>强健有特色；<br>紧凑高效。</li><li>大多数可用的描述符关注边缘/梯度信息：<br>捕获纹理信息；<br>很少使用的颜色。</li></ol><h1 id="四、Feature-Matching-特征匹配"><a href="#四、Feature-Matching-特征匹配" class="headerlink" title="四、Feature Matching 特征匹配"></a>四、Feature Matching 特征匹配</h1><p><img src="https://img-blog.csdnimg.cn/20191022111630357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="特征匹配"><br>Bijective 双射：<br><img src="https://img-blog.csdnimg.cn/20191022134506371.png" alt="双射"><br>Injective 内射<br><img src="https://img-blog.csdnimg.cn/201910221345432.png" alt="内射"><br>Surjective 满射<br><img src="https://img-blog.csdnimg.cn/20191022134554286.png" alt="满射"></p><h2 id="Euclidean-distance-vs-Cosine-Similarity-欧氏距离与余弦相似性"><a href="#Euclidean-distance-vs-Cosine-Similarity-欧氏距离与余弦相似性" class="headerlink" title="Euclidean distance vs. Cosine Similarity 欧氏距离与余弦相似性"></a>Euclidean distance vs. Cosine Similarity 欧氏距离与余弦相似性</h2><h3 id="Euclidean-distance-欧氏距离"><a href="#Euclidean-distance-欧氏距离" class="headerlink" title="Euclidean distance 欧氏距离"></a>Euclidean distance 欧氏距离</h3><p><img src="https://img-blog.csdnimg.cn/20191022135010814.png" alt="Euclidean distance"><br><img src="https://img-blog.csdnimg.cn/20191022135019159.png" alt="Euclidean distance"></p><h3 id="Cosine-similarity-余弦"><a href="#Cosine-similarity-余弦" class="headerlink" title="Cosine similarity 余弦"></a>Cosine similarity 余弦</h3><p><img src="https://img-blog.csdnimg.cn/20191022135055580.png" alt="余弦"><br><img src="https://img-blog.csdnimg.cn/20191022135059893.png" alt="余弦"><br><img src="https://img-blog.csdnimg.cn/20191022135114689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="余弦"></p><h2 id="特征匹配的标准"><a href="#特征匹配的标准" class="headerlink" title="特征匹配的标准"></a>特征匹配的标准</h2><p> 标准1：</p><ol><li>计算特征空间中的距离，例如128个Sift描述符之间的欧几里德距离</li><li>匹配点到最近距离（最近邻）<br>问题：<br>所有的东西都匹配吗？</li></ol><p>标准2：</p><ol start="3"><li>计算特征空间中的距离，例如128个Sift描述符之间的欧几里德距离</li><li>匹配点到最近距离（最近邻）</li><li>忽略任何高于阈值的内容（不匹配！）</li></ol><p>问题：<br>6. 门槛难挑<br>7. 不明显的特征可能有很多相似的匹配，只有一个是正确的</p><h2 id="最近邻距离比"><a href="#最近邻距离比" class="headerlink" title="最近邻距离比"></a>最近邻距离比</h2><p><img src="https://img-blog.csdnimg.cn/20191022140852350.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="最近邻距离比"><br><img src="https://img-blog.csdnimg.cn/20191022141102861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="最近邻距离比"></p><h1 id="Interest-points-兴趣点-1"><a href="#Interest-points-兴趣点-1" class="headerlink" title="Interest points 兴趣点"></a>Interest points 兴趣点</h1><ol><li>关键点检测（Keypoint detection）：可重复且独特<br>角点（Corners）、斑点（blobs）、稳定区域（stable regions）<br>哈里斯（Harris），DoG</li><li>描述符（Descriptors）：健壮和选择性<br>方位的空间直方图<br>SIFT</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、哈里斯角的不变性&quot;&gt;&lt;a href=&quot;#一、哈里斯角的不变性&quot; class=&quot;headerlink&quot; title=&quot;一、哈里斯角的不变性&quot;&gt;&lt;/a&gt;一、哈里斯角的不变性&lt;/h1&gt;&lt;h2 id=&quot;Harris-Corners-哈里斯角&quot;&gt;&lt;a href=&quot;#Ha
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://mengyashen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://mengyashen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Computer Vision | InterestPointsAndCorners</title>
    <link href="http://mengyashen.github.io/9595090/"/>
    <id>http://mengyashen.github.io/9595090/</id>
    <published>2019-10-21T10:28:30.000Z</published>
    <updated>2019-10-22T02:17:44.107Z</updated>
    
    <content type="html"><![CDATA[<p>Filtering——&gt;Edges——&gt;Corners<br>滤波器——&gt;边沿——&gt;角点</p><h1 id="Feature-points-特征点"><a href="#Feature-points-特征点" class="headerlink" title="Feature points 特征点"></a>Feature points 特征点</h1><p>也称为兴趣点（interest points）、关键点（ key points）等，通常称为“局部”特征（ ‘local’ features）。</p><ol><li>视图之间的通信<br>对应关系：在图像上匹配点、面片、边或区域。</li><li>特征点用于：<br>图像对齐<br>三维重建<br>运动跟踪（机器人、无人机、AR）<br>索引与数据库检索<br>目标识别</li><li>不变局部特征<br>检测可重复且独特的点。即，对图像变换不变性：<br>外观变化（亮度、照度）；<br>几何变化（平移、旋转、缩放）。</li><li>示例应用——全景接合<br>1）检测（Detection）：找到一组与众不同的关键点。<br>2）描述（Description）：提取每个兴趣点的特征描述作为向量。<br>3）匹配（Matching）：计算特征向量之间的距离以找到对应关系。</li><li>好的特征：<br>1）重复性<br>尽管存在几何和光度变换，但在多个图像中可以找到相同的特征。<br>2）显著性<br>每个特征都很独特。<br>3）紧凑性和效率<br>比图像像素少很多特征。<br>4）局部<br>一个特征占据了图像相对较小的区域；对杂波和遮挡鲁棒。</li></ol><h1 id="Corner-Detection-角点检测"><a href="#Corner-Detection-角点检测" class="headerlink" title="Corner Detection 角点检测"></a>Corner Detection 角点检测</h1><p>我们可以透过一扇小窗户来认识这一点。<br>我们希望窗口向任何方向移动，以使强度发生很大变化。<br><img src="https://img-blog.csdnimg.cn/20191021170444176.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="基于自相关的角点检测"><a href="#基于自相关的角点检测" class="headerlink" title="基于自相关的角点检测"></a>基于自相关的角点检测</h2><ol><li><p>变换[u，v]：通过改变窗口w（x，y）的外观：<br><img src="https://img-blog.csdnimg.cn/20191021170758603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="基于自相关的角点检测"><br><img src="https://img-blog.csdnimg.cn/20191021170830103.png" alt="窗口函数"></p></li><li><p>关键特性：在拐角区域，图像梯度有两个或多个主方向角落是可重复和独特的。</p></li><li><p>用二次曲面局部逼近E（u，v）：<br><img src="https://img-blog.csdnimg.cn/20191021173029342.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="用二次曲面局部逼近E（u，v）"></p></li><li><p>一个函数f可以用它在一个点a的无穷级数表示：<br><img src="https://img-blog.csdnimg.cn/20191021173315297.png" alt="无穷级数"><br>当我们关心窗口中心时，我们设置a=0（麦克劳林系列）。<br>f(x)＝e^x以f(0)为中心的逼近<br><img src="https://img-blog.csdnimg.cn/2019102117351932.png" alt="f(0)"></p></li></ol><h2 id="角点检测：数学"><a href="#角点检测：数学" class="headerlink" title="角点检测：数学"></a>角点检测：数学</h2><ol start="5"><li><p>通过二阶泰勒展开给出了（0，0）邻域中E（u，v）的局部二次逼近：<br><img src="https://img-blog.csdnimg.cn/20191021174002998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="E（u，v）的局部二次逼近"></p></li><li><p>E(u，v)关于(0,0)的二阶泰勒展开式：<br><img src="https://img-blog.csdnimg.cn/20191021174213500.png" alt="E(u，v)"><br><img src="https://img-blog.csdnimg.cn/20191021174131367.png" alt="E(u，v)关于(0,0)的二阶泰勒展开式"><br><img src="https://img-blog.csdnimg.cn/20191021174136319.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="E(u，v)关于(0,0)的二阶泰勒展开式"><br><img src="https://img-blog.csdnimg.cn/20191021174315863.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="E(u，v)关于(0,0)的二阶泰勒展开式"><br>二次近似简化为:<br><img src="https://img-blog.csdnimg.cn/20191021174347305.png" alt="二次近似"><br><img src="https://img-blog.csdnimg.cn/20191021174351550.png" alt="二次近似"><br>其中M是根据图像导数计算的二阶矩矩阵：<br><img src="https://img-blog.csdnimg.cn/20191021174417618.png" alt="M"><br><img src="https://img-blog.csdnimg.cn/20191021174425553.png" alt="M"></p></li></ol><h2 id="角点作为独特的兴趣点"><a href="#角点作为独特的兴趣点" class="headerlink" title="角点作为独特的兴趣点"></a>角点作为独特的兴趣点</h2><ol start="7"><li>图像导数的2x 2矩阵（在点附近平均）<br><img src="https://img-blog.csdnimg.cn/20191021174802506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="矩阵"></li></ol><h2 id="二阶矩矩阵的解释"><a href="#二阶矩矩阵的解释" class="headerlink" title="二阶矩矩阵的解释"></a>二阶矩矩阵的解释</h2><ol start="8"><li>E(u,v)的形状：<br><img src="https://img-blog.csdnimg.cn/20191021174957168.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>E(u,v)的水平“切片”：<img src="https://img-blog.csdnimg.cn/20191021175019808.png" alt="E(u,v)"><br><img src="https://img-blog.csdnimg.cn/20191021175026323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="E(u,v)的水平“切片”"><br>M的对角化：<br><img src="https://img-blog.csdnimg.cn/20191021175903515.png" alt="M的对角化"><br>因此，E(u,v)的形状为椭圆<br><img src="https://img-blog.csdnimg.cn/20191021175919188.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h2 id="基于M特征值的图像点分类"><a href="#基于M特征值的图像点分类" class="headerlink" title="基于M特征值的图像点分类"></a>基于M特征值的图像点分类</h2></li><li>λ1 and λ2 很小，几乎在所有方向上都是恒定的。<br><img src="https://img-blog.csdnimg.cn/20191021180253238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="图像点分类"></li><li><img src="https://img-blog.csdnimg.cn/20191021180455532.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="图像点分类"><br><img src="https://img-blog.csdnimg.cn/20191021181006356.png" alt="C"><br>线性代数：<br><img src="https://img-blog.csdnimg.cn/2019102118105049.png" alt="线性代数"><br><img src="https://img-blog.csdnimg.cn/20191021181054344.png" alt="线性代数"><br><img src="https://img-blog.csdnimg.cn/20191021181131589.png" alt="在这里插入图片描述"></li></ol><h2 id="Harris-角点探测器"><a href="#Harris-角点探测器" class="headerlink" title="Harris 角点探测器"></a>Harris 角点探测器</h2><p>0）输入图像：我们要计算每个像素的M；<br><img src="https://img-blog.csdnimg.cn/20191021182240147.png" alt="Harris 角点探测器"><br>1）计算图像导数（可选，先模糊）；<br><img src="https://img-blog.csdnimg.cn/20191021182245879.png" alt="Harris 角点探测器"><br>2）用导数平方计算M分量；<br><img src="https://img-blog.csdnimg.cn/20191021184317821.png" alt="Harris 角点探测器"><br>3）宽度为σ的高斯滤波器g()；<br><img src="https://img-blog.csdnimg.cn/20191021182331318.png" alt="Harris 角点探测器"><br>4）计算转角<br><img src="https://img-blog.csdnimg.cn/20191021181909803.png" alt="C"><br><img src="https://img-blog.csdnimg.cn/20191021181918399.png" alt="C"><br>5）C上选择高转角的阈值<br>6）非极大值抑制拾取峰<br><img src="https://img-blog.csdnimg.cn/20191021182344996.png" alt="Harris 角点探测器"></p><p>位置对光度变换是不变的，对几何变换是协变的吗？<br>不变性：图像被变换，角点位置不变；<br>协方差：如果我们有相同图像的两个变换版本，则应在相应位置检测特征。<br><img src="https://img-blog.csdnimg.cn/20191021182615936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="Harris 角点探测器"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Filtering——&amp;gt;Edges——&amp;gt;Corners&lt;br&gt;滤波器——&amp;gt;边沿——&amp;gt;角点&lt;/p&gt;
&lt;h1 id=&quot;Feature-points-特征点&quot;&gt;&lt;a href=&quot;#Feature-points-特征点&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://mengyashen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://mengyashen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Computer Vision | ImageFiltering</title>
    <link href="http://mengyashen.github.io/968867f0/"/>
    <id>http://mengyashen.github.io/968867f0/</id>
    <published>2019-10-21T08:24:04.000Z</published>
    <updated>2019-10-21T08:31:58.420Z</updated>
    
    <content type="html"><![CDATA[<p> 一张图片—&gt;feature detection 特征检测（e.g.：DoG）—&gt;feature description 特征描述（e.g.：SIFT）—&gt;Matching/Indexing/Detection 匹配/索引/检测&lt;—database of local  descriptors 本地描述符数据库</p><h1 id="Image-filtering-图像滤波"><a href="#Image-filtering-图像滤波" class="headerlink" title="Image filtering 图像滤波"></a>Image filtering 图像滤波</h1><p>过滤的三种观点：</p><ol><li>空域图像滤波<br>过滤器是数字网格的数学运算<br>平滑、锐化、测量纹理<ol start="2"><li>频域图像滤波器<br>滤波是一种改变图像频率的方法<br>去噪、采样、图像压缩</li><li>图像金字塔<br>比例空间表示允许粗到细操作</li></ol></li></ol><p>图像滤波<br>计算每个位置的局部邻域函数：<br><img src="https://img-blog.csdnimg.cn/20191021154040633.png" alt="局部邻域函数"><br>        h=output    f=filter    I=image<br>        k,l = 二维坐标   m,n = 二维坐标</p><h2 id="一、box-filter-盒式过滤器"><a href="#一、box-filter-盒式过滤器" class="headerlink" title="一、box filter 盒式过滤器"></a>一、box filter 盒式过滤器</h2><p><img src="https://img-blog.csdnimg.cn/20191021154452465.png" alt="box filter"><br><img src="https://img-blog.csdnimg.cn/20191021154457545.png" alt="box filter"><br><img src="https://img-blog.csdnimg.cn/20191021154720446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>盒滤波器作用：</p><ol><li>将每个像素替换为其邻域的平均值；</li><li>达到平滑效果（去除尖锐特征）。</li></ol><p>图像滤波器作用：</p><ol><li>增强图像<br>去噪、调整大小、增加对比度等。</li><li>从图像中提取信息<br>纹理、边缘、特征点等。</li><li>检测模式<br>模板匹配</li></ol><h2 id="二、linear-filters-线性滤波器"><a href="#二、linear-filters-线性滤波器" class="headerlink" title="二、linear filters 线性滤波器"></a>二、linear filters 线性滤波器</h2><p>没有变化<br><img src="https://img-blog.csdnimg.cn/20191021155654409.png" alt="线性滤波器"><br>按1像素左移<br><img src="https://img-blog.csdnimg.cn/20191021155709314.png" alt="线性滤波器"><br>垂直边缘（绝对值）<br><img src="https://img-blog.csdnimg.cn/20191021155753112.png" alt="线性滤波器"><br>水平边缘（绝对值）<br><img src="https://img-blog.csdnimg.cn/20191021155920820.png" alt="线性滤波器"><br>锐化滤波器——突出与当地平均水平的差异<br><img src="https://img-blog.csdnimg.cn/20191021160050769.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="线性滤波器"></p><h2 id="三、Gaussian-filter-高斯滤波器"><a href="#三、Gaussian-filter-高斯滤波器" class="headerlink" title="三、Gaussian filter 高斯滤波器"></a>三、Gaussian filter 高斯滤波器</h2><p>邻近像素的加权贡献<br><img src="https://img-blog.csdnimg.cn/20191021160608192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="高斯滤波器"></p><h3 id="高斯滤波器的可分性"><a href="#高斯滤波器的可分性" class="headerlink" title="高斯滤波器的可分性"></a>高斯滤波器的可分性</h3><p><img src="https://img-blog.csdnimg.cn/20191021160729979.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="高斯滤波器的可分性"><br>可分性示例：</p><p>2D convolution(center location only)<br>二维卷积（仅中心位置）<br><img src="https://img-blog.csdnimg.cn/20191021161210473.png" alt="可分性示例"><br>The filter factorsinto a product of 1Dfilters:<br>过滤因子为1的乘积：<br><img src="https://img-blog.csdnimg.cn/20191021161225137.png" alt="可分性示例"><br>Perform convolutionalong rows:<br>执行卷积长行：<br>Followed by convolutionalong the remaining column:<br>然后对其余列进行卷积：<br><img src="https://img-blog.csdnimg.cn/20191021161233292.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="可分性示例"><br>为什么分离性在实践中有用？</p><ol><li>MXN图像，PXQ过滤器<br>二维卷积：MNPQ乘加<br>可分二维：MN（P+Q）乘加</li><li>加速=PQ/（P+Q）<br>9x9过滤器大约快4.5倍</li></ol><p>过滤器应该有多大？</p><ol><li>边缘处的值应接近零</li><li>高斯人有无限的范围…</li><li>高斯的经验法则：将滤波器半宽设为3σ左右</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 一张图片—&amp;gt;feature detection 特征检测（e.g.：DoG）—&amp;gt;feature description 特征描述（e.g.：SIFT）—&amp;gt;Matching/Indexing/Detection 匹配/索引/检测&amp;lt;—database 
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://mengyashen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://mengyashen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>查看远端地址 git remote -v时报错 fatal:not a git repository(or any of the parent directories):.git</title>
    <link href="http://mengyashen.github.io/f3333876/"/>
    <id>http://mengyashen.github.io/f3333876/</id>
    <published>2019-10-21T05:14:20.000Z</published>
    <updated>2019-10-21T05:15:45.856Z</updated>
    
    <content type="html"><![CDATA[<p>查看git远端地址时：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  git remote –v</span><br></pre></td></tr></table></figure><p>报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fatal: not a git repository (or any of the parent directories): .git</span><br></pre></td></tr></table></figure><p>提示说没有.git这样一个目录<br>解决办法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br></pre></td></tr></table></figure><p>ok啦！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;查看git远端地址时：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;c
      
    
    </summary>
    
    
      <category term="Hexo使用过程中出现的一些坑" scheme="http://mengyashen.github.io/categories/Hexo%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/"/>
    
    
      <category term="git" scheme="http://mengyashen.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>部署hexo时出错：fatal: Could not read from remote repository.Error: Spawn failed</title>
    <link href="http://mengyashen.github.io/708c56a/"/>
    <id>http://mengyashen.github.io/708c56a/</id>
    <published>2019-10-21T05:04:59.000Z</published>
    <updated>2019-10-21T05:15:45.857Z</updated>
    
    <content type="html"><![CDATA[<p>在部署hexo执行下面代码时：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><p>出现错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">On branch master</span><br><span class="line">nothing to commit, working tree clean</span><br><span class="line">git@git.coding.net: Permission denied (publickey).</span><br><span class="line">fatal: Could not read from remote repository.</span><br><span class="line"></span><br><span class="line">Please make sure you have the correct access rights</span><br><span class="line">and the repository exists.</span><br><span class="line">FATAL Something&apos;s wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.html</span><br><span class="line">Error: Spawn failed</span><br></pre></td></tr></table></figure><p>查了一些资料后发现是因为在设置百度主动推送的时候，站点配置文件中deploy:字段写错了。<br>正确的应为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">- type: git</span><br><span class="line">  repo: </span><br><span class="line">      git@github.com:mengyaShen/mengyaShen.github.io.git</span><br><span class="line">  branch: master</span><br><span class="line">- type: baidu_url_submitter</span><br></pre></td></tr></table></figure><p>重新部署，成功。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在部署hexo执行下面代码时：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td clas
      
    
    </summary>
    
    
      <category term="Hexo使用过程中出现的一些坑" scheme="http://mengyashen.github.io/categories/Hexo%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/"/>
    
    
      <category term="Hexo" scheme="http://mengyashen.github.io/tags/Hexo/"/>
    
      <category term="git" scheme="http://mengyashen.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Computer Vision | Introduction</title>
    <link href="http://mengyashen.github.io/ed318fdc/"/>
    <id>http://mengyashen.github.io/ed318fdc/</id>
    <published>2019-10-21T02:51:47.000Z</published>
    <updated>2019-10-21T03:03:20.906Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、计算视觉的经典问题"><a href="#一、计算视觉的经典问题" class="headerlink" title="一、计算视觉的经典问题"></a>一、计算视觉的经典问题</h1><p>Reconstruction——重建<br>Recognition——识别<br>(Re)organization——重组</p><ol><li><p>Optical character recognition (OCR) 光学字符识别（OCR）<br>Technology to convert scanned docs to text 扫描文档转换为文本的技术</p></li><li><p>Face detection 人脸检测</p></li><li><p>Smile detection 微笑检测</p></li><li><p>Object recognition (in supermarkets) 物体识别（超市）</p></li><li><p>Vision-based biometrics 基于视觉的生物特征识别</p></li><li><p>Login without a password 无密码登录</p></li><li><p>Object recognition (in mobile phones) 物体识别（在移动电话中）</p></li><li><p>3D from images 三维图像</p></li><li><p>Human shape capture 人形捕捉</p></li><li><p>Special effects:  shape capture 特殊效果：形状捕捉</p></li><li><p>Interactive Games: Kinect 互动游戏：Kinect</p></li><li><p>Sports 体育运动</p></li><li><p>Medical imaging 医学影像学</p></li><li><p>AutoCars - Uber bought CMU’s lab 汽车制造商-Uber收购了CMU的实验室</p></li><li><p>Industrial robots 工业机器人</p></li><li><p>Vision in space 太空视野<br>Vision systems (JPL) used for several tasks 用于多个任务的视觉系统（JPL）</p></li><li><p>Mobile robots 移动机器人</p></li><li><p>Augmented Reality and Virtual Reality 增强现实与虚拟现实</p><h2 id="二、计算机视觉及邻近领域"><a href="#二、计算机视觉及邻近领域" class="headerlink" title="二、计算机视觉及邻近领域"></a>二、计算机视觉及邻近领域</h2></li><li><p>计算机视觉的贬义总结：<br>机器学习在可视化数据中的应用。<br><img src="https://img-blog.csdnimg.cn/20191021102225457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li><li><p><img src="https://img-blog.csdnimg.cn/20191021103117537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="Scope of CSCI 1430"><br>图像处理、识别、深度学习、几何推理<br>机器学习、图形学、计算摄影、光学<br>机器人学、人机交互、医学影像学、神经科学 </p></li></ol><h1 id="三、课程主题"><a href="#三、课程主题" class="headerlink" title="三、课程主题"></a>三、课程主题</h1><h2 id="1-解释强度"><a href="#1-解释强度" class="headerlink" title="1.解释强度"></a>1.解释强度</h2><p>什么决定了像素的亮度和颜色？<br>如何使用图像过滤器从图像中提取有意义的信息？</p><h2 id="2-对应和对齐"><a href="#2-对应和对齐" class="headerlink" title="2.对应和对齐"></a>2.对应和对齐</h2><p>如何在物体或场景中找到对应的点？<br>我们如何估计它们之间的转换？</p><h2 id="3-分组和分段"><a href="#3-分组和分段" class="headerlink" title="3.分组和分段"></a>3.分组和分段</h2><p>如何将像素分组成有意义的区域？</p><h2 id="4-分类与目标识别"><a href="#4-分类与目标识别" class="headerlink" title="4.分类与目标识别"></a>4.分类与目标识别</h2><p>我们如何表现图像并对其进行分类？<br>我们如何识别物体的类别？</p><h2 id="5-高级主题"><a href="#5-高级主题" class="headerlink" title="5.高级主题"></a>5.高级主题</h2><p>动作识别，三维场景和上下文，人在环视觉</p><h2 id="6-相关知识"><a href="#6-相关知识" class="headerlink" title="6.相关知识"></a>6.相关知识</h2><p>线性代数、概率、图形课程、视觉/图像处理课程、机器学习</p><h1 id="四、项目"><a href="#四、项目" class="headerlink" title="四、项目"></a>四、项目</h1><p>Projects 1-5: Structured conceptual / code——结构化概念/代码<br>Project 6: Group challenge——团体挑战</p><h2 id="Proj-1-Image-Filtering-and-Hybrid-Images——图像滤波和混合图像"><a href="#Proj-1-Image-Filtering-and-Hybrid-Images——图像滤波和混合图像" class="headerlink" title="Proj 1: Image Filtering and Hybrid Images——图像滤波和混合图像"></a>Proj 1: Image Filtering and Hybrid Images——图像滤波和混合图像</h2><p>实现图像滤波以分离高频和低频。<br>将来自不同图像的高频和低频合并以创建与比例相关的图像。</p><h2 id="Proj-2-Local-Feature-Matching——局部特征匹配"><a href="#Proj-2-Local-Feature-Matching——局部特征匹配" class="headerlink" title="Proj 2: Local Feature Matching——局部特征匹配"></a>Proj 2: Local Feature Matching——局部特征匹配</h2><p>实现兴趣点检测、类sift局部特征描述、简单匹配算法。</p><h2 id="Proj-3-Scene-Recognition-with-Bag-of-Words——文字袋场景识别"><a href="#Proj-3-Scene-Recognition-with-Bag-of-Words——文字袋场景识别" class="headerlink" title="Proj 3: Scene Recognition with Bag of Words——文字袋场景识别"></a>Proj 3: Scene Recognition with Bag of Words——文字袋场景识别</h2><p>将局部特征量化为“词汇”，将图像描述为“视觉词汇”的直方图，训练分类器根据这些直方图识别场景。</p><h2 id="Proj-3b-Object-Detection-with-a-Sliding-Window——滑动窗口目标检测"><a href="#Proj-3b-Object-Detection-with-a-Sliding-Window——滑动窗口目标检测" class="headerlink" title="Proj 3b: Object Detection with a Sliding Window——滑动窗口目标检测"></a>Proj 3b: Object Detection with a Sliding Window——滑动窗口目标检测</h2><p>训练一个基于正样本和“挖掘”硬底片的人脸检测器，在多个尺度上检测人脸并抑制重复检测。</p><h2 id="Proj-4-Convolutional-Neural-Nets——卷积神经网络"><a href="#Proj-4-Convolutional-Neural-Nets——卷积神经网络" class="headerlink" title="Proj 4: Convolutional Neural Nets——卷积神经网络"></a>Proj 4: Convolutional Neural Nets——卷积神经网络</h2><h2 id="Proj-5-Multi-view-Geometry——多视图几何图形"><a href="#Proj-5-Multi-view-Geometry——多视图几何图形" class="headerlink" title="Proj 5: Multi-view Geometry——多视图几何图形"></a>Proj 5: Multi-view Geometry——多视图几何图形</h2><p>从特征点匹配恢复相机校准。<br>计算机视觉中几乎所有测量的基础。</p><h2 id="Proj-6-Group-challenge——团体挑战"><a href="#Proj-6-Group-challenge——团体挑战" class="headerlink" title="Proj 6: Group challenge——团体挑战"></a>Proj 6: Group challenge——团体挑战</h2><p>改进webgazer：一个基于web的实时眼睛跟踪器。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、计算视觉的经典问题&quot;&gt;&lt;a href=&quot;#一、计算视觉的经典问题&quot; class=&quot;headerlink&quot; title=&quot;一、计算视觉的经典问题&quot;&gt;&lt;/a&gt;一、计算视觉的经典问题&lt;/h1&gt;&lt;p&gt;Reconstruction——重建&lt;br&gt;Recognition—
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://mengyashen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://mengyashen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战 | 第3章 决策树</title>
    <link href="http://mengyashen.github.io/668be595/"/>
    <id>http://mengyashen.github.io/668be595/</id>
    <published>2019-10-18T02:58:26.000Z</published>
    <updated>2019-10-29T11:31:32.779Z</updated>
    
    <content type="html"><![CDATA[<p>决策树的一个重要任务是：为了数据中蕴含的知识信息。<br>决策树可以使用不熟悉的数据集合，并从中提取除一系列规则，在这些机器根据数据集创建规则时，就是机器学习的过程。</p><h1 id="3-1-决策树的构造"><a href="#3-1-决策树的构造" class="headerlink" title="3.1 决策树的构造"></a>3.1 决策树的构造</h1><ol><li>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。<br>缺点：可能会产生过度匹配问题。<br>适用数据类型：数值型和标称型。</li><li>在构造决策树时，我们需要解决的第一个问题是：当前数据集上哪个特征在划分数据分类时起决定性作用。<br>为了找到决定性特征，划分出最好的结果，我们必须评估每个特征。<br>完成测试之后，原始数据集就被划分为几个数据子集。<br>这些数据子集会分布在第一个决策点的所有分支上。<br>如果某分支上的数据属于同一类型，无需进一步对数据集进行分割。<br>如果数据子集内的数据不属于同一类型，则需要重复划分数据子集。<br>直到所有具有相同数据类型的数据均在一个数据子集内。</li><li>创建分支的伪代码函数createBranch()：<br><img src="https://img-blog.csdnimg.cn/20191018101538240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="创建分支的伪代码函数"></li><li>决策树的一般流程：<br>(1) 收集数据：可以使用任何方法。<br>(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。<br>(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。<br>(4) 训练算法：构造树的数据结构。<br>(5) 测试算法：使用经验树计算错误率。<br>(6) 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据<br>的内在含义。</li></ol><h2 id="3-1-1-信息增益"><a href="#3-1-1-信息增益" class="headerlink" title="3.1.1 信息增益"></a>3.1.1 信息增益</h2><ol start="5"><li>划分数据集的大原则：将无序的数据变得更加有序。</li><li>信息增益：划分数据集之前之后信息发生的变化。获得信息增益最高的特征就是最好的选择。</li><li>香农熵（熵）：集合信息的度量方式。</li><li>熵：定义为信息的期望值。</li><li>信息：符号xi的信息定义：<br><img src="https://img-blog.csdnimg.cn/20191018102957793.png" alt="信息"><br>其中p(xi)是选择该分类的概率。</li><li>信息熵：<img src="https://img-blog.csdnimg.cn/20191018103300857.png" alt="信息熵"><br>其中n是分类的数目。</li><li>基尼不纯度：度量集合无序程度的方法。简单的说就是从一个数据集中随机选取子项，度量其被错误分类到其他组里的概率。</li></ol><h3 id="程序清单3-1-计算给定数据集的香农熵"><a href="#程序清单3-1-计算给定数据集的香农熵" class="headerlink" title="程序清单3-1 计算给定数据集的香农熵"></a>程序清单3-1 计算给定数据集的香农熵</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"># 为所有可能分类创建字典</span><br><span class="line">def cal_entropy(data):</span><br><span class="line">    entries_num = len(data)</span><br><span class="line">    label_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vec <span class="keyword">in</span> data:</span><br><span class="line">        cur_label = vec[<span class="number">-1</span>]</span><br><span class="line">        label_count[cur_label] = label_count.get(cur_label,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">    Entropy =<span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    # 以2为底求对数</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> label_count:</span><br><span class="line">        prob =float(label_count[key])/entries_num</span><br><span class="line">        Entropy += prob*math.log(prob,<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">0</span>-Entropy)</span><br><span class="line"></span><br><span class="line"># 定义自己的数据集</span><br><span class="line">def createData():</span><br><span class="line">    data = [[<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="string">'no'</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no sufacing'</span>,<span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> data,labels</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">import</span> trees</span><br><span class="line">&gt;myDat,labels = trees.createData()</span><br><span class="line">&gt;myDat</span><br><span class="line">&gt;trees.cal_entropy(myDat)</span><br><span class="line"># 增加第三个名为maybe的分类，测试熵的变化：</span><br><span class="line">&gt;myDat[<span class="number">0</span>][<span class="number">-1</span>] = <span class="string">'maybe'</span></span><br><span class="line">&gt;myDat</span><br><span class="line">&gt;trees.cal_entropy(myDat)</span><br></pre></td></tr></table></figure><h2 id="3-1-2-划分数据集"><a href="#3-1-2-划分数据集" class="headerlink" title="3.1.2 划分数据集"></a>3.1.2 划分数据集</h2><ol start="12"><li>使用ID3算法划分数据集。<br>三组参数：待划分的数据集、划分数据集的特征、需要返回的特征的值。</li></ol><h3 id="程序清单3-2-按照给定特征划分数据集"><a href="#程序清单3-2-按照给定特征划分数据集" class="headerlink" title="程序清单3-2 按照给定特征划分数据集"></a>程序清单3-2 按照给定特征划分数据集</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def Split_Data(dataset, axis, value):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    使用传入的axis以及value划分数据集</span></span><br><span class="line"><span class="string">    axis代表在每个列表中的第X位，value为用来划分的特征值</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    new_subset = []</span><br><span class="line">    # 利用循环将不符合value的特征值划分入另一集合</span><br><span class="line">    # 相当于将value单独提取出来（或作为叶节点）</span><br><span class="line">    <span class="keyword">for</span> vec <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">if</span> vec[axis] == value:</span><br><span class="line">            feature_split = vec[:axis]</span><br><span class="line">            feature_split.extend(vec[axis + <span class="number">1</span>:])</span><br><span class="line">            new_subset.append(feature_split)</span><br><span class="line">    # extend将vec中的元素一一纳入feature_split</span><br><span class="line">    # append则将feature_split作为列表结合进目标集合</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> new_subset</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">import</span> trees</span><br><span class="line">&gt;myDat,labels = trees.createData()</span><br><span class="line">&gt;myDat</span><br><span class="line">[[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br><span class="line">&gt;trees.split_data(myDat,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">[[<span class="number">1</span>, <span class="string">'yes'</span>], [<span class="number">1</span>, <span class="string">'yes'</span>], [<span class="number">0</span>, <span class="string">'no'</span>]]</span><br><span class="line">&gt;trees.split_data(myDat,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">[[<span class="number">1</span>, <span class="string">'no'</span>], [<span class="number">1</span>, <span class="string">'no'</span>]]</span><br></pre></td></tr></table></figure><pre><code>Python语言不用考虑内存分配问题。Python语言在函数中传递的是列表的引用，在函        数内部对列表对象的修改，将会影响该列表对象的整个生存周期。为了消除这个不良影响，我们需要在函数的开始声明一个新列表对象。</code></pre><h3 id="程序清单3-3-选择最好的数据集划分方式"><a href="#程序清单3-3-选择最好的数据集划分方式" class="headerlink" title="程序清单3-3 选择最好的数据集划分方式"></a>程序清单3-3 选择最好的数据集划分方式</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def Split_by_entropy(dataset):</span><br><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    使用熵原则进行数据集划分</span></span><br><span class="line"><span class="string">    @信息增益:info_gain = old -new</span></span><br><span class="line"><span class="string">    @最优特征：best_feature</span></span><br><span class="line"><span class="string">    @类别集合：uniVal</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    feature_num = len(dataset[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    ent_old = cal_entropy(dataset)</span><br><span class="line">    best_gain = <span class="number">0.0</span></span><br><span class="line">    best_feature = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_num):</span><br><span class="line">        feature_list = [x[i] <span class="keyword">for</span> x <span class="keyword">in</span> dataset]</span><br><span class="line">        # 将dataSet中的数据先按行依次放入x中，然后取得x中的x[i]元素，放入列表feature_list中</span><br><span class="line">        uniVal = <span class="keyword">set</span>(feature_list)</span><br><span class="line">        ent_new = 0.0</span><br><span class="line">        # 使用<span class="keyword">set</span>剔除重复项，保留该特征对应的不同取值</span><br><span class="line">        for value in uniVal:</span><br><span class="line">            sub_set = split_data(dataset, i, value)</span><br><span class="line">            prob = len(sub_set) / float(len(dataset))</span><br><span class="line">            # 使用熵计算函数求出划分后的熵值</span><br><span class="line">            ent_new += prob * (0 - cal_entropy(sub_set))</span><br><span class="line"></span><br><span class="line">        # 由ent_old - ent_new选出划分对应的最优特征</span><br><span class="line">        Info_gain = ent_old - ent_new</span><br><span class="line">        if (Info_gain &gt; best_gain):</span><br><span class="line">            best_gain = Info_gain</span><br><span class="line">            best_feature = i</span><br><span class="line"></span><br><span class="line">    return best_feature</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">import</span> trees</span><br><span class="line">&gt;myDat,labels=trees.createData()</span><br><span class="line">&gt;trees.Split_by_entropy(myDat)</span><br><span class="line"><span class="number">1</span></span><br><span class="line">&gt;myDat</span><br><span class="line">[[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br></pre></td></tr></table></figure><h2 id="3-1-3-递归构建决策树"><a href="#3-1-3-递归构建决策树" class="headerlink" title="3.1.3 递归构建决策树"></a>3.1.3 递归构建决策树</h2><ol start="13"><li>工作原理：得到原始数据集，然后基于最好的属性值划分数据集（由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分）。第一次划分之后，数据被向下传递到树分支的下一个节点，在这个节点上，再次划分数据。递归进行。<br>递归结束的条件：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子节点或者终止块。<br>如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用==多数表决==的方法决定该叶子节点的分类。</li></ol><p>多数表决法：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def Majority_vote(classList):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">    使用多数表决法：若集合中属于第K类的节点最多，则此分支集合划分为第K类</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    classcount = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        classcount[vote] = classcount.get(vote,<span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    sorted_count = sorted(classcount.items(), key = operator.itemgetter(<span class="number">1</span>),\</span><br><span class="line">                          reverse = True)</span><br><span class="line">    # 获取每一类出现的节点数（没出现默认为0）并进行排序</span><br><span class="line">    # 返回最大项的KEY所对应的类别</span><br><span class="line">    <span class="keyword">return</span> sorted_count[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="程序清单3-4-创建树的函数代码"><a href="#程序清单3-4-创建树的函数代码" class="headerlink" title="程序清单3-4 创建树的函数代码"></a>程序清单3-4 创建树的函数代码</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def Create_Tree(dataset, labels):</span><br><span class="line">    # 类别完全相同则停止继续划分</span><br><span class="line">    classList = [x[<span class="number">-1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> dataset]</span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    # 遍历完所有特征值时返回出现次数最多的</span><br><span class="line">    <span class="keyword">if</span> len(dataset[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> Majority_vote(classList)</span><br><span class="line">    best_feature = Split_by_entropy(dataset)</span><br><span class="line">    best_labels = labels[best_feature]</span><br><span class="line"></span><br><span class="line">    # 得到列表包含的所有属性值</span><br><span class="line">    myTree = &#123;<span class="attr">best_labels</span>: &#123;&#125;&#125;</span><br><span class="line">    # 此位置书上写的有误，书上为del(labels[bestFeat])</span><br><span class="line">    # 相当于操作原始列表内容，导致原始列表内容发生改变</span><br><span class="line">    # 按此运行程序，报错'no surfacing'is not in list</span><br><span class="line">    # 以下代码已改正</span><br><span class="line"></span><br><span class="line">    # 复制当前特征标签列表，防止改变原始列表的内容</span><br><span class="line">    subLabels = labels[:]</span><br><span class="line">    # 删除属性列表中当前分类数据集特征</span><br><span class="line">    del (subLabels[best_feature])</span><br><span class="line"></span><br><span class="line">    # 使用列表推导式生成该特征对应的列</span><br><span class="line">    f_val = [x[best_feature] <span class="keyword">for</span> x <span class="keyword">in</span> dataset]</span><br><span class="line">    uni_val = <span class="keyword">set</span>(f_val)</span><br><span class="line">    for value in uni_val:</span><br><span class="line">        # 递归创建子树并返回</span><br><span class="line">        myTree[best_labels][value] = Create_Tree(split_data(dataset \</span><br><span class="line">                                                            , best_feature, value), subLabels)</span><br><span class="line"></span><br><span class="line">    return myTree</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">import</span> trees</span><br><span class="line">&gt;myDat,labels = trees.createData()</span><br><span class="line">&gt;myTree = trees.Create_Tree(myDat,labels)</span><br><span class="line">&gt;myTree</span><br><span class="line">&#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'no sufacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure><h1 id="3-2-在-Python-中使用-Matplotlib-注解绘制树形图"><a href="#3-2-在-Python-中使用-Matplotlib-注解绘制树形图" class="headerlink" title="3.2 在 Python 中使用 Matplotlib 注解绘制树形图"></a>3.2 在 Python 中使用 Matplotlib 注解绘制树形图</h1><p>Matplotlib提供了一个注解工具==annotations==，非常有用，它可以在数据图形上添加文本注释。</p><h3 id="程序清单3-5-使用文本注解绘制树节点"><a href="#程序清单3-5-使用文本注解绘制树节点" class="headerlink" title="程序清单3-5 使用文本注解绘制树节点"></a>程序清单3-5 使用文本注解绘制树节点</h3><p>我们要知道：<br>1）有多少个叶节点，以便确定x轴的长度；<br>2）树有多少层，以便确定y轴的高度。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#定义文本框和箭头格式</span><br><span class="line">decisionNode = dict(boxstyle=<span class="string">"sawtooth"</span>, fc=<span class="string">"0.8"</span>)</span><br><span class="line"># 创建字典。 boxstyle=”sawtooth” 表示注解框的边缘是波浪线，fc=”0.8” 是颜色深度</span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"round4"</span>, fc=<span class="string">"0.8"</span>)</span><br><span class="line">arrow_args = dict(arrowstyle="&lt;-") # 箭头样式</span><br><span class="line"></span><br><span class="line">#绘制带箭头的注释</span><br><span class="line">def plotNode(nodeTxt, centerPt, parentPt, nodeType):</span><br><span class="line"># centerPt:节点中心坐标  parentPt:起点坐标</span><br><span class="line">     createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords=<span class="string">'axes fraction'</span>,xytext=centerPt, textcoords=<span class="string">'axes fraction'</span>,va=<span class="string">"center"</span>,ha=<span class="string">"center"</span>, bbox=nodeType, arrowprops=arrow_args )</span><br><span class="line"># 参考annotate说明文档</span><br></pre></td></tr></table></figure><h3 id="程序清单3-6-获取叶节点的数目和树的层数"><a href="#程序清单3-6-获取叶节点的数目和树的层数" class="headerlink" title="程序清单3-6 获取叶节点的数目和树的层数"></a>程序清单3-6 获取叶节点的数目和树的层数</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">def Num_of_leaf(myTree): </span><br><span class="line">    <span class="string">''</span><span class="string">'计算此树的叶子节点数目，输入为我们前面得到的树（字典）'</span><span class="string">''</span></span><br><span class="line">    num_leaf = 0  # 初始化</span><br><span class="line">    first_node = myTree.keys() </span><br><span class="line">    first_node = list(first_node)[0]  # 获得第一个key值（根节点） 'no surfacing'</span><br><span class="line">    # python 3X 中： mytree.keys() 返回 :dict_keys([’ ‘])是类似于列表但又不是列表的东东，它是个字典的key值的一个视图（view），所以改写为本句方法。</span><br><span class="line">    </span><br><span class="line">    second_dict = myTree[first_node]  # 获得value值 &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;</span><br><span class="line">    # Python3中使用LIST转换firstnode，原书使用[0]直接索引只能用于Python2</span><br><span class="line">    # 对于树，每次判断value是否为字典，若为字典则进行递归，否则累加器+1</span><br><span class="line">    </span><br><span class="line">    for key in second_dict.keys():  # 键值：0 和 1</span><br><span class="line">        if type(second_dict[key]).__name__ =='dict':  # 判断如果里面的一个value是否还是dict</span><br><span class="line">            num_leaf += Num_of_leaf(second_dict[key])  # 递归调用</span><br><span class="line">        <span class="keyword">else</span>: num_leaf += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> num_leaf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def Depth_of_tree(myTree):</span><br><span class="line">    <span class="string">''</span><span class="string">'计算此树的总深度'</span><span class="string">''</span></span><br><span class="line">    depth = <span class="number">0</span></span><br><span class="line">    first_node = myTree.keys()</span><br><span class="line">    first_node = list(first_node)[<span class="number">0</span>]</span><br><span class="line">    second_dict = myTree[first_node]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> second_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(second_dict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            pri_depth = <span class="number">1</span> + Depth_of_tree(second_dict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pri_depth = <span class="number">1</span></span><br><span class="line">        # 对于树，每次判断value是否为字典，若为字典则进行递归，否则计数器+1</span><br><span class="line">        <span class="keyword">if</span> pri_depth &gt; depth: depth = pri_depth</span><br><span class="line">    <span class="keyword">return</span> depth</span><br><span class="line">    </span><br><span class="line">def retrieveTree(i):</span><br><span class="line">    <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">   保存了树的测试数据</span></span><br><span class="line"><span class="string">     '</span><span class="string">''</span></span><br><span class="line">    listOfTrees =[&#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;&#125;&#125;,&#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: &#123;<span class="string">'head'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;, <span class="number">1</span>: <span class="string">'no'</span>&#125;&#125;&#125;&#125;]</span><br><span class="line">    <span class="keyword">return</span> listOfTrees[i]</span><br></pre></td></tr></table></figure><h3 id="程序清单3-7-plotTree函数"><a href="#程序清单3-7-plotTree函数" class="headerlink" title="程序清单3-7 plotTree函数"></a>程序清单3-7 plotTree函数</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def plotmidtext(cntrpt, parentpt, txtstring):</span><br><span class="line">    <span class="string">''</span><span class="string">'作用是计算tree的中间位置</span></span><br><span class="line"><span class="string">    cntrpt起始位置,parentpt终止位置,txtstring：文本标签信息</span></span><br><span class="line"><span class="string">    (在两个节点之间的线上写上字)</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    xmid = (parentpt[<span class="number">0</span>] - cntrpt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrpt[<span class="number">0</span>]</span><br><span class="line">    # cntrPt 起点坐标 子节点坐标</span><br><span class="line">    # parentPt 结束坐标 父节点坐标</span><br><span class="line">    ymid = (parentpt[1] - cntrpt[1]) / 2.0 + cntrpt[1]  # 找到x和y的中间位置</span><br><span class="line">    createPlot.ax1.text(xmid, ymid, txtstring)  # text() 的使用</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plottree(mytree, parentpt, nodetxt):  # 画树</span><br><span class="line">    numleafs = Num_of_leaf(mytree)</span><br><span class="line">    depth = Depth_of_tree(mytree)</span><br><span class="line">    firststr = list(mytree.keys())[<span class="number">0</span>]</span><br><span class="line">    cntrpt = (plottree.xoff + (<span class="number">1.0</span> + float(numleafs)) / <span class="number">2.0</span> / plottree.totalw, plottree.yoff)</span><br><span class="line">    # 计算子节点的坐标</span><br><span class="line">    plotmidtext(cntrpt, parentpt, nodetxt)  # 绘制线上的文字</span><br><span class="line">    plotNode(firststr, cntrpt, parentpt, decisionNode)  # 绘制节点</span><br><span class="line">    seconddict = mytree[firststr]</span><br><span class="line">    plottree.yoff = plottree.yoff - <span class="number">1.0</span> / plottree.totald</span><br><span class="line">    # 每绘制一次图，将y的坐标减少1.0/plottree.totald，间接保证y坐标上深度的</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> seconddict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(seconddict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            plottree(seconddict[key], cntrpt, str(key))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            plottree.xoff = plottree.xoff + <span class="number">1.0</span> / plottree.totalw</span><br><span class="line">            plotNode(seconddict[key], (plottree.xoff, plottree.yoff), cntrpt, leafNode)</span><br><span class="line">            plotmidtext((plottree.xoff, plottree.yoff), cntrpt, str(key))</span><br><span class="line">    plottree.yoff = plottree.yoff + <span class="number">1.0</span> / plottree.totald</span><br><span class="line"></span><br><span class="line">def createPlot(intree):</span><br><span class="line">    # 类似于Matlab的figure，定义一个画布(暂且这么称呼吧)，背景为白色</span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()  # 把画布清空</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    # createPlot.ax1为全局变量，绘制图像的句柄，subplot为定义了一个绘图，</span><br><span class="line">    # 111表示figure中的图有1行1列，即1个，最后的1代表第一个图</span><br><span class="line">    # frameon表示是否绘制坐标轴矩形</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=False, **axprops)</span><br><span class="line"></span><br><span class="line">    plottree.totalw = float(Num_of_leaf(intree))</span><br><span class="line">    plottree.totald = float(Depth_of_tree(intree))</span><br><span class="line">    plottree.xoff = <span class="number">-0.6</span> / plottree.totalw;plottree.yoff = <span class="number">1.2</span>;</span><br><span class="line">    plottree(intree, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">''</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">import</span> trees</span><br><span class="line">Backend TkAgg is interactive backend. Turning interactive mode on. #???</span><br><span class="line">&gt;myTree=trees.retrieveTree(<span class="number">0</span>)</span><br><span class="line">&gt;trees.createPlot(myTree)</span><br><span class="line">&gt;myTree[<span class="string">'no surfacing'</span>][<span class="number">3</span>]=<span class="string">'maybe'</span></span><br><span class="line">&gt;myTree</span><br><span class="line">&#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;, <span class="number">3</span>: <span class="string">'maybe'</span>&#125;&#125;</span><br><span class="line">&gt;trees.createPlot(myTree)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2019102821053413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="tree"><br>为啥没有根节点呢？？</p><h1 id="3-3-测试和存储分类器"><a href="#3-3-测试和存储分类器" class="headerlink" title="3.3 测试和存储分类器"></a>3.3 测试和存储分类器</h1><p>使用决策树构建分类器，以及实际应用中如何存储分类器。</p><h2 id="3-3-1-测试算法：使用决策树执行分类"><a href="#3-3-1-测试算法：使用决策树执行分类" class="headerlink" title="3.3.1 测试算法：使用决策树执行分类"></a>3.3.1 测试算法：使用决策树执行分类</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;决策树的一个重要任务是：为了数据中蕴含的知识信息。&lt;br&gt;决策树可以使用不熟悉的数据集合，并从中提取除一系列规则，在这些机器根据数据集创建规则时，就是机器学习的过程。&lt;/p&gt;
&lt;h1 id=&quot;3-1-决策树的构造&quot;&gt;&lt;a href=&quot;#3-1-决策树的构造&quot; class=&quot;
      
    
    </summary>
    
    
      <category term="机器学习实战" scheme="http://mengyashen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="机器学习" scheme="http://mengyashen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://mengyashen.github.io/tags/python/"/>
    
      <category term="Matplotlib" scheme="http://mengyashen.github.io/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战 | 第2章 k-临近算法</title>
    <link href="http://mengyashen.github.io/5f6e73d7/"/>
    <id>http://mengyashen.github.io/5f6e73d7/</id>
    <published>2019-10-18T02:55:06.000Z</published>
    <updated>2019-10-23T13:37:14.807Z</updated>
    
    <content type="html"><![CDATA[<p>==以下代码全部基于python3==</p><h1 id="一、k-临近算法概述"><a href="#一、k-临近算法概述" class="headerlink" title="一、k-临近算法概述"></a>一、k-临近算法概述</h1><ol><li><p>工作原理：存在一个样本数据集合（也称作训练样本集），并且样本集中每个数据都存在标签（即我们知道样本中每一数据与所属分类的对应关系）。输入没有标签的新数据后，将新数据的特征值与样本中数据对应的特征值进行比较，然后算法提取样本集中特征最相似数据（最邻近）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。</p></li><li><p>kNN算法伪代码：<br>对未知类别属性的数据集中的每个点依次执行以下操作：<br>（1）计算已知类别数据集中的点与当前点之间的距离；<br>（2）按照距离递增次序排序；<br>（3）选取与当前点距离最小的k个点；<br>（4）确定前k个点所在类别的出现频率；<br>（5）返回前k个点出现频率最高的类别作为当前点的预测分类。</p></li><li><p>python3 下kNN算法实现：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> operator </span><br><span class="line"></span><br><span class="line">def createDataSet():</span><br><span class="line">    group = array([[<span class="number">1.0</span>,<span class="number">1.1</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]])</span><br><span class="line">    labels = [<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group,labels</span><br><span class="line"></span><br><span class="line">def classify_KNN(test_X, train_set, labels, K):</span><br><span class="line">    rows = train_set.shape[<span class="number">0</span>]</span><br><span class="line">    diff = tile(test_X,(rows,<span class="number">1</span>)) - train_set</span><br><span class="line">    # 这一行利用tile函数将输入样本实例转化为与训练集同尺寸的矩阵</span><br><span class="line">    # 便之后的矩阵减法运算</span><br><span class="line"></span><br><span class="line">    # .tile若输入一个二元祖，第一个数表示复制的行数，第二个数表示对inx的重复的次数</span><br><span class="line">    # .shape[0]代表行数，.shape[1]代表列数</span><br><span class="line">   </span><br><span class="line">    sqDistance = (diff ** <span class="number">2</span>).sum(axis=<span class="number">1</span>)</span><br><span class="line">    Distance = sqDistance ** <span class="number">0.5</span></span><br><span class="line">    sorted_Distance = Distance.argsort()</span><br><span class="line"># 对每个训练样本与输入的测试样本求欧几里得距离，即点之间的范数</span><br><span class="line">    # 随后按距离由小到大进行排序</span><br><span class="line">    # 当axis为0时,是压缩行,即将每一列的元素相加,将矩阵压缩为一行</span><br><span class="line">    # 当axis为1时,是压缩列,即将每一行的元素相加,将矩阵压缩为一列</span><br><span class="line">    # 如果axis为整数元组（x，y），则是求出axis=x和axis=y情况下得到的和</span><br><span class="line">    </span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">        vote_label = labels[sorted_Distance[i]]</span><br><span class="line">        classCount[vote_label] = classCount.get(vote_label,<span class="number">0</span>) +<span class="number">1</span></span><br><span class="line">        sortedClassCount = sorted(classCount.items() , key = operator.itemgetter(<span class="number">1</span>),reverse=True)</span><br><span class="line">        <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure></li></ol><p>运行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">import</span> kNN</span><br><span class="line">&gt;group,labels = kNN.createDataSet()</span><br><span class="line">&gt;kNN.classify_KNN([<span class="number">0</span>,<span class="number">0</span>],group,labels,<span class="number">3</span>)</span><br><span class="line"><span class="string">'B'</span></span><br></pre></td></tr></table></figure><ol start="4"><li>测试分类器最常用的方法：错误率——分类器给出错误结果的次数除以测试执行的总数。</li></ol><h1 id="二、示例：使用-k-近邻算法改进约会网站的配对效果"><a href="#二、示例：使用-k-近邻算法改进约会网站的配对效果" class="headerlink" title="二、示例：使用 k-近邻算法改进约会网站的配对效果"></a>二、示例：使用 k-近邻算法改进约会网站的配对效果</h1><h2 id="1-将文本记录转换为NumPy的解析程序"><a href="#1-将文本记录转换为NumPy的解析程序" class="headerlink" title="1. 将文本记录转换为NumPy的解析程序"></a>1. 将文本记录转换为NumPy的解析程序</h2><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def file_parse_matrix(filename):</span><br><span class="line">    <span class="keyword">with</span> open(filename) <span class="keyword">as</span> fp:</span><br><span class="line">        Arr_lines = fp.readlines()</span><br><span class="line">        number = len(Arr_lines)</span><br><span class="line">        # 初始化数据为m行3列（飞行里程，游戏时间，冰淇淋数）</span><br><span class="line">        # 标签单独创建一个向量保存</span><br><span class="line">        return_mat = zeros((number, <span class="number">3</span>))</span><br><span class="line">        label_vec = []</span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> Arr_lines:</span><br><span class="line">            line = line.strip()</span><br><span class="line">            listFromLine = line.split('\t')  # 按换行符分割数据</span><br><span class="line">            # 将文本数据前三行存入数据矩阵，第四行存入标签向量</span><br><span class="line">            return_mat[index, :] = listFromLine[<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">            label_vec.append(listFromLine[<span class="number">-1</span>])</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> return_mat, label_vec</span><br></pre></td></tr></table></figure><p>运行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;datingDataMat,datingLabels=kNN.file_parse_matrix(<span class="string">'datingTestSet.txt'</span>)</span><br><span class="line">&gt;datingDataMat</span><br><span class="line">&gt;&gt;&gt;datingLabels</span><br></pre></td></tr></table></figure><h2 id="2-分析数据：使用-Matplotlib-创建散点图"><a href="#2-分析数据：使用-Matplotlib-创建散点图" class="headerlink" title="2. 分析数据：使用 Matplotlib 创建散点图"></a>2. 分析数据：使用 Matplotlib 创建散点图</h2><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">import</span> matplotlib</span><br><span class="line">&gt;<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">&gt;fig = plt.figure()</span><br><span class="line">&gt;ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">&gt;ax.scatter(datingDataMat[:,<span class="number">1</span>], datingDataMat[:,<span class="number">2</span>])</span><br><span class="line">&gt;plt.show()</span><br></pre></td></tr></table></figure><p>//这段代码运行的时候有一点问题，等待后续更正</p><h2 id="3-准备数据：归一化数值"><a href="#3-准备数据：归一化数值" class="headerlink" title="3. 准备数据：归一化数值"></a>3. 准备数据：归一化数值</h2><p>数值归一化：处理不同取值范围的特征值，如将取值范围处理为0到1或者-1到1之间。<br>公式：newValue = (oldValue-min)/(max-min)<br>其中min和max分别是数据集中的最小特征值和最大特征值。<br>函数Norm_feature()，可以自动将数字特征值转化为0到1的区间：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def Norm_feature(data_set):</span><br><span class="line">    minVal = data_set.min(<span class="number">0</span>)</span><br><span class="line">    maxVal = data_set.max(<span class="number">0</span>)</span><br><span class="line">    ranges = maxVal - minVal     # 计算极差</span><br><span class="line">    # 下一步将初始化一个与原始数据矩阵同尺寸的矩阵</span><br><span class="line">    # 利用tile函数实现扩充向量，并进行元素间的对位运算</span><br><span class="line">    norm_set = zeros(shape(data_set))</span><br><span class="line">    rows = data_set.shape[<span class="number">0</span>]</span><br><span class="line">    norm_set = (data_set - tile(minVal, (rows, <span class="number">1</span>))) / tile(ranges, (rows,<span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> norm_set, ranges, minVal</span><br></pre></td></tr></table></figure><p>运行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;data_normed, ranges, minV = kNN.Norm_feature(datingDataMat)</span><br></pre></td></tr></table></figure><h2 id="4-测试算法：作为完整程序验证分类器"><a href="#4-测试算法：作为完整程序验证分类器" class="headerlink" title="4. 测试算法：作为完整程序验证分类器"></a>4. 测试算法：作为完整程序验证分类器</h2><p> 书上的测试函数没有参数，是自适应函数<br> 此处传入分割参数以及测试集，可以修改测试数值（使用书上的0.1作为分割率）</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def Test_accuray(split_ratio, test_set, test_label):</span><br><span class="line">    norm_test, ranges, Min = Norm_feature(test_set)</span><br><span class="line">    rows = norm_test.shape[<span class="number">0</span>]</span><br><span class="line">    rows_test = int(rows * split_ratio)</span><br><span class="line">    </span><br><span class="line">    error  = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(rows_test):</span><br><span class="line">        Result = classify_KNN(norm_test[i,:], norm_test[rows_test:rows], \</span><br><span class="line">                              test_label[rows_test:rows], <span class="number">3</span>)</span><br><span class="line">        # 参数1表示从测试集（此处约会数据是随机的因此抽取前10%即可）中抽取一个实例</span><br><span class="line">        # 参数2，3，4使用后90%作为训练数据，为输入的实例进行投票并分类，K=3</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">"the classifier came with: %s, the real answer is :%s "</span> \</span><br><span class="line">                 % (Result, test_label[i]))</span><br><span class="line">        <span class="keyword">if</span>(Result != test_label[i]) : error += <span class="number">1</span></span><br><span class="line">        # print(type(error)) #for test</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">"the accuracy is %f | the error_rate is %f "</span> % \</span><br><span class="line">          (<span class="number">1</span>- (float(error) /float(rows_test)),(float(error) /float(rows_test))))</span><br></pre></td></tr></table></figure><p>运行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;kNN.Test_accuray(<span class="number">0.1</span>,datingDataMat,datingLabels)</span><br></pre></td></tr></table></figure><h1 id="三、-示例：手写识别系统"><a href="#三、-示例：手写识别系统" class="headerlink" title="三、 示例：手写识别系统"></a>三、 示例：手写识别系统</h1><h2 id="1-准备数据：将图像转换为测试向量"><a href="#1-准备数据：将图像转换为测试向量" class="headerlink" title="1. 准备数据：将图像转换为测试向量"></a>1. 准备数据：将图像转换为测试向量</h2><p>实际图像存储在源代码的两个子目录内：目录trainingDigits中包含了大约2000个例子，每个数字大约有200个样本；目录testDigits中包含了大约900个测试数据。我们使用目录trainingDigits中的数据训练分类器，使用目录testDigits中的数据测试分类器的效果。两组数据没有重叠。<br>为了使用前面两个例子的分类器，我们必须将图像格式化处理为一个向量。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from os import listdir #从os模块中导入函数listdir，它可以列出给定目录的文件名。</span><br><span class="line">def img2vec(filename):</span><br><span class="line">    <span class="string">''</span><span class="string">'this is to...将32X32的图像转化为1X1024的行向量'</span><span class="string">''</span></span><br><span class="line">    returnvec = zeros((<span class="number">1</span>, <span class="number">1024</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(filename) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            line = fp.readline()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">                returnvec[<span class="number">0</span>, <span class="number">32</span> * i + j] = int(line[j])</span><br><span class="line">    # returnVEC按32进位，j代表每位的32个元素</span><br><span class="line">    <span class="keyword">return</span> returnvec</span><br></pre></td></tr></table></figure><p>运行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;testVector = kNN.img2vec(<span class="string">'digits/testDigits/0_13.txt'</span>)</span><br><span class="line">&gt;testVector[<span class="number">0</span>,<span class="number">0</span>:<span class="number">31</span>]</span><br><span class="line">&gt;testVector[<span class="number">0</span>,<span class="number">32</span>:<span class="number">36</span>]</span><br></pre></td></tr></table></figure><h2 id="2-测试算法：使用-k-近邻算法识别手写数字"><a href="#2-测试算法：使用-k-近邻算法识别手写数字" class="headerlink" title="2.测试算法：使用 k-近邻算法识别手写数字"></a>2.测试算法：使用 k-近邻算法识别手写数字</h2><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def HandWritingTest(train_dir, test_dir):</span><br><span class="line">    labels = []</span><br><span class="line">    File_list = listdir(train_dir)</span><br><span class="line">    # 将目录内的文件按名字放入列表，使用函数解析为数字</span><br><span class="line">    m = len(File_list)</span><br><span class="line">    train_mat = zeros((m, <span class="number">1024</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        fname = File_list[i]</span><br><span class="line">        fstr = fname.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        fnumber = int(fstr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        # 比如'digits/testDigits/0_13.txt'，被拆分为0,13,txt</span><br><span class="line">        # 此处0即为标签数字</span><br><span class="line">        labels.append(fnumber)</span><br><span class="line">        train_mat[i, :] = img2vec(<span class="string">'%s/%s'</span> % (train_dir, fname))</span><br><span class="line">    # labels is label_vec，同之前的KNN代码相同，存储标签</span><br><span class="line"></span><br><span class="line">    test_File_list = listdir(test_dir)</span><br><span class="line">    error = <span class="number">0.0</span></span><br><span class="line">    test_m = len(test_File_list)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(test_m):</span><br><span class="line">        fname = test_File_list[i]</span><br><span class="line">        fstr = fname.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        fnumber = int(fstr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        vec_test = img2vec(<span class="string">'digits/testDigits/%s'</span> % fname)</span><br><span class="line">        Result = classify_KNN(vec_test, train_mat, labels, <span class="number">3</span>)</span><br><span class="line">        print(<span class="string">"the classifier came with: %d, the real answer is :%d "</span> \</span><br><span class="line">              % (Result, fnumber))</span><br><span class="line">        <span class="keyword">if</span> (Result != fnumber): error += <span class="number">1</span></span><br><span class="line">    # 这部分和Test模块相同，直接copy过来就好</span><br><span class="line">    print(<span class="string">"the accuracy is %f | the error_rate is %f "</span> % \</span><br><span class="line">          (<span class="number">1</span> - (float(error) / float(test_m)), (float(error) / float(test_m))))</span><br></pre></td></tr></table></figure><p>运行：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;kNN.HandWritingTest(<span class="string">'digits/trainingDigits'</span>, <span class="string">'digits/testDigits/'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;==以下代码全部基于python3==&lt;/p&gt;
&lt;h1 id=&quot;一、k-临近算法概述&quot;&gt;&lt;a href=&quot;#一、k-临近算法概述&quot; class=&quot;headerlink&quot; title=&quot;一、k-临近算法概述&quot;&gt;&lt;/a&gt;一、k-临近算法概述&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;工作
      
    
    </summary>
    
    
      <category term="机器学习实战" scheme="http://mengyashen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="机器学习" scheme="http://mengyashen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://mengyashen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书学习笔记 | 第16章 强化学习</title>
    <link href="http://mengyashen.github.io/d6bdaab8/"/>
    <id>http://mengyashen.github.io/d6bdaab8/</id>
    <published>2019-10-14T11:48:31.000Z</published>
    <updated>2019-10-14T12:23:39.913Z</updated>
    
    <content type="html"><![CDATA[<h1 id="16-1-任务与奖赏"><a href="#16-1-任务与奖赏" class="headerlink" title="16.1 任务与奖赏"></a>16.1 任务与奖赏</h1><ol><li>强化学习（再励学习）：在种瓜过程中不断摸索，总结出较好种瓜策略的抽象过程。<br><img src="https://img-blog.csdnimg.cn/20191014142804871.png" alt="强化学习">    </li><li>强化学习任务通常用马尔可夫决策过程（Markov Decision Process,简称MDP)来描述：<br>（1）机器处于环境E中，==状态空间为X==，其中每个状态x∈X是机器感知到的环境的描述。如在种瓜任务上这就是当前瓜苗长势的描述；<br>（2）机器能采取的动作构成了动作空间A。如种瓜过程中有浇水、施不同的肥、使用不同的农药等多种可供选择的动作；<br>（3）若某个==动作a==∈A作用在当前状态x上，则潜在的==转移函数P==将使得环境从当前状态按某种概率转移到另一个状态。如瓜苗状态为缺水，若选择动作浇水，则瓜苗长势会发生变化，瓜苗有一定的概率恢复健康，也有一定的概率无法恢复；<br>（4）在转移到另一个状态的同时，环境会根据潜在的==“奖赏”(reward)函数R==反馈给机器一个奖赏。如保持瓜苗健康对应奖赏+1，瓜苗凋零对应奖赏-10，最终种出了好瓜对应奖赏+100。<br>综合起来，强化学习任务对应了四元组E=&lt;X,A,P, R&gt;，其中P:X x AxX|→IR指定了状态转移概率，R:X xAxX|→IR指定了奖赏；在有的应用中，奖赏函数可能仅与状态转移有关，即R:XxX|→IR。</li><li>给西瓜浇水问题的马尔可夫决策过程：<br><img src="https://img-blog.csdnimg.cn/20191014150308750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="给西瓜浇水问题的马尔可夫决策过程"><br>a：动作<br>p：转移概率<br>r：返回的奖赏</li><li>机器要做的是通过在环境中不断地尝试而学得一个“策略”(policy)π，根据这个策略，在状态x下就能得知要执行的动作a=π(x)。<br>策略有两种表示方法：<br>（1）表示为函数π:X |→A，确定性策略常用这种表示；<br>（2）概率表示π:XxA|→IR，随机性策略常用这种表示，π(x, a)为状态x下选择动作a的概率，这里必须有<br><img src="https://img-blog.csdnimg.cn/20191014151105696.png" alt="策略表示方法"><br>策略的优势取决于长期执行这一策略后得到的累积奖赏。</li><li>强化学习中，学习的目的：找到能使长期积累奖赏最大化的策略。</li><li>长期积累奖赏计算方式：<br>（1）T步累计奖赏：<br><img src="https://img-blog.csdnimg.cn/20191014161926423.png" alt="T步累计奖赏"><br>（2）γ折扣累计奖赏：<br><img src="https://img-blog.csdnimg.cn/20191014161937296.png" alt="γ折扣累计奖赏"><br>其中rt表示第t步获得的奖赏值，IE表示对所有随机变量求期望。</li><li>强化学习与监督学习：<br>强化学习中的“状态”对应为监督学习中的“示例”、“动作” 对应为“标记”，“策略”对应为“分类器”(当动作是离散的)或“回归器”(当动作是连续的)。<br>不同：在强化学习中并没有监督学习中的有标记样本(即“示例-标记”对)。<br>换言之，没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过“反思”之前的动作是否正确来进行学习。<br>强化学习在某种意义上可看作具有“延迟标记信息”的监督学习问题。<h1 id="16-2-K-摇臂赌博机"><a href="#16-2-K-摇臂赌博机" class="headerlink" title="16.2 K-摇臂赌博机"></a>16.2 K-摇臂赌博机</h1><h2 id="16-2-1-探索与利用"><a href="#16-2-1-探索与利用" class="headerlink" title="16.2.1 探索与利用"></a>16.2.1 探索与利用</h2></li><li>最大化单步奖赏，即仅考虑一步操作。</li><li>欲最大化单步奖赏需考虑两个方面：<br>（1）需知道每个动作带来的奖赏，<br>（2）要执行奖赏最大的动作。</li><li>K-摇臂赌博机(K-armed bandit)：单步强化学习任务对应的一个理论模型。<br><img src="https://img-blog.csdnimg.cn/20191014170305718.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="K-摇臂赌博机"></li><li>若仅为获知每个摇臂的期望奖赏，则可采用“仅探索”(exploration-only)法：将所有的尝试机会平均分配给每个摇臂(即轮流按下每个摇臂)，最后以每个摇壁各白的平均叶币概率作为其奖常期望的近似估计。<br>若仅为执行奖常最大的动作，则可采用“仅利用“（exploitation-only）法：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。<br>“仅探索”法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；<br>“仅利用”法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。</li></ol><h2 id="16-2-2-∈-贪心"><a href="#16-2-2-∈-贪心" class="headerlink" title="16.2.2  ∈-贪心"></a>16.2.2  ∈-贪心</h2><ol start="12"><li>∈-贪心法基于一个概率来对探索和利用进行折中：每次尝试时，以∈的概率进行探索，即以均匀概率随机选取一个摇臂；以1 -∈的概率进行利用，即选择当前平均奖赏最高的摇臂(若有多个，则随机选取一个)。</li><li>令Q(k)记录摇臂k的平均奖赏：若摇臂k被尝试了n次，得到的奖赏为<br><img src="https://img-blog.csdnimg.cn/20191014183552401.png" alt="平均奖赏"><h2 id="16-2-3-Softmax"><a href="#16-2-3-Softmax" class="headerlink" title="16.2.3 Softmax"></a>16.2.3 Softmax</h2></li></ol><h1 id="16-3-有模型学习"><a href="#16-3-有模型学习" class="headerlink" title="16.3 有模型学习"></a>16.3 有模型学习</h1><h2 id="16-3-1-策略评估"><a href="#16-3-1-策略评估" class="headerlink" title="16.3.1 策略评估"></a>16.3.1 策略评估</h2><h2 id="16-3-2-策略改进"><a href="#16-3-2-策略改进" class="headerlink" title="16.3.2 策略改进"></a>16.3.2 策略改进</h2><h2 id="16-3-3-策略迭代与值迭代"><a href="#16-3-3-策略迭代与值迭代" class="headerlink" title="16.3.3 策略迭代与值迭代"></a>16.3.3 策略迭代与值迭代</h2><h1 id="16-4-免模型学习"><a href="#16-4-免模型学习" class="headerlink" title="16.4 免模型学习"></a>16.4 免模型学习</h1><h2 id="16-4-1-蒙特卡罗强化学习"><a href="#16-4-1-蒙特卡罗强化学习" class="headerlink" title="16.4.1 蒙特卡罗强化学习"></a>16.4.1 蒙特卡罗强化学习</h2><h2 id="16-4-2-时序差分学习"><a href="#16-4-2-时序差分学习" class="headerlink" title="16.4.2 时序差分学习"></a>16.4.2 时序差分学习</h2><h1 id="16-5-值函数近似"><a href="#16-5-值函数近似" class="headerlink" title="16.5 值函数近似"></a>16.5 值函数近似</h1><h1 id="16-6-模仿学习"><a href="#16-6-模仿学习" class="headerlink" title="16.6 模仿学习"></a>16.6 模仿学习</h1><h2 id="16-6-1-直接模仿学习"><a href="#16-6-1-直接模仿学习" class="headerlink" title="16.6.1 直接模仿学习"></a>16.6.1 直接模仿学习</h2><h2 id="16-6-2-逆强化学习"><a href="#16-6-2-逆强化学习" class="headerlink" title="16.6.2 逆强化学习"></a>16.6.2 逆强化学习</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;16-1-任务与奖赏&quot;&gt;&lt;a href=&quot;#16-1-任务与奖赏&quot; class=&quot;headerlink&quot; title=&quot;16.1 任务与奖赏&quot;&gt;&lt;/a&gt;16.1 任务与奖赏&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;强化学习（再励学习）：在种瓜过程中不断摸索，总结出较好种瓜策略的
      
    
    </summary>
    
    
      <category term="西瓜书学习笔记" scheme="http://mengyashen.github.io/categories/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://mengyashen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="西瓜书" scheme="http://mengyashen.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书学习笔记 | 第15章  规则学习</title>
    <link href="http://mengyashen.github.io/a75704a9/"/>
    <id>http://mengyashen.github.io/a75704a9/</id>
    <published>2019-10-12T03:02:03.000Z</published>
    <updated>2019-10-14T06:35:15.611Z</updated>
    
    <content type="html"><![CDATA[<h1 id="15-1-基本概念"><a href="#15-1-基本概念" class="headerlink" title="15.1 基本概念"></a>15.1 基本概念</h1><ol><li>规则：机器学习中通常指语义明确、能描述数据分布所隐含的客观规律或领域概念、可写成“若……，则……”形式的逻辑规则。</li><li>规则学习（rule leaning）：是从训练数据中学习出一组能用来对未见实例进行判别的规则。</li><li>形式化地看，一条规则形如：<br><img src="https://img-blog.csdnimg.cn/20191012100345912.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="规则"></li><li>规则学习的优势：<br>（1）与神经网络、支持向量机这样的“黑箱模型”相比，规则学习具有更好的可解释性，能使用户更直观地对判别过程有所了解；<br>（2）规则学习能更自然地在学习过程中引入领域知识；<br>（3）逻辑规则的抽象描述能力在处理一些高度复杂的AI任务时具有显著的优势。</li><li>覆盖：<br>符合规则的样本称为被该规则覆盖。</li><li>冲突：<br>规则集合中的每条规则都可看作一一个子模型，规则集合是这些子模型的一个集成。当同一个示例被判别结果不同的多条规则覆盖时，称发生了“冲突”(confict)，解决冲突的办法称为“冲突消解”(conflict resolution).。</li><li>常用的冲突消解策略：<br>（1）投票法：将判别相同的规则数最多的结果作为最终结果。<br>（2）排序法：在规则集合上定义一个顺序，在发生冲突时使用排序最前的规则，相应的规则学习过程称为“带序规则”(orderedrule)学习或“优先级规则”(priority rule)学习。<br>（3）元规则法：根据领域知识事先设定-些“元规则”(meta-rule),，即关于规则的规则，例如“发生冲突时使用长度最小的规则”，然后根据元规则的指导来使用规则集。</li><li>规则学习算法通常会设置一条“默认规则”(default rule)，由它来处理规则集合未覆盖的样本。</li><li>从形式语言表达能力而言，规则可分为两类：<br>（1）“命题规则“(propositionalrule)：由“原子命题”(propositionalatom)和逻辑连接词“与”(∧)、“或” (V)、“非” (﹁)和“蕴含”(←)构成的简单陈述句；<br>（2）“一阶规则”(first- order rule)/关系型规则（relational rule）：基本成分是能描述事物的属性或关系的“原子公式”(atomic formula)。</li><li>从形式语言系统的角度看，命题规则是一阶规则的特例，一阶规则的学习要比命题规则复杂的多。</li></ol><h1 id="15-2-序贯覆盖"><a href="#15-2-序贯覆盖" class="headerlink" title="15.2 序贯覆盖"></a>15.2 序贯覆盖</h1><ol start="11"><li>规则学习的目标：产生一个能覆盖尽可能多的样例的规则集。</li><li>最直接的做法：“序贯覆盖”(sequential covering)，即逐条归纳：在训练集上每学到一条规则，就将该规则覆盖的训练样例去除，然后以剩下的训练样例组成训练集重复上述过程，由于每次只处理一部分数据，因此也被称为“分治”(separate and-conquer)策略。</li><li>序贯覆盖法的关键：如何从训练集学出单条规则。<br>对规则学习目标⊕，产生一条规则就是寻找最优的一组逻辑文字来构成规则体，这是一个搜索问题。</li><li>最简单的做法是从空规则“⊕←”开始，将正例类别作为规则头，再逐个遍历训练集中的每个属性及取值，尝试将其作为逻辑文字增加到规则体中，若能使当前规则体仅覆盖正例,则由此产生一条规则，然后去除已被覆盖的正例并基于剩余样本尝试生成下一条规则。</li><li>现实任务中一般有两种策略来产生规则：<br>（1）“自顶向下”(top-down)，即从比较一般的规则开始，逐渐添加新文字以缩小规则覆盖范围，直到满足预定条件为止；亦称为“生成-测试”(generate-then-test)法，是规则逐渐“特化”(specialization)的过程。<br>覆盖范围从大往小搜索规则，更容易产生泛化性能较好的规则。<br>（2）“自底向上”(bottom-up)，即从比较特殊的规则开始，逐渐删除文字以扩大规则覆盖范围，直到满足条件为止;亦称为“数据驱动”(data driven)法，是规则逐渐“泛化”(generalization)的过程。<br>覆盖范围从小往大搜索规则，适合于训练样本较少的情形。<br>前者对噪声的鲁棒性比后者要强得多。因此，在命题规则学习中通常使用第一种策略,而第二种策略在一阶规则学习这类假设空间非常复杂的任务上实用得多。    </li><li>“集束搜索”(beam search),：每轮保留最优的b个逻辑文字，在下一轮均用于构建候选集，再把候选集中最优的b个留待再下一轮使用。</li></ol><h1 id="15-3-剪枝优化"><a href="#15-3-剪枝优化" class="headerlink" title="15.3 剪枝优化"></a>15.3 剪枝优化</h1><ol start="17"><li>剪枝(pruning)：规则生成本质上是一个贪心搜索过程，需有一定的机制来缓解过拟合的风险，最常见的做法是剪枝(pruning)。</li><li>与决策树相似，剪枝可发生在规则生长过程中，即“预剪枝”，也可发生在规则产生后，即“后剪枝”。通常是基于某种性能度量指标来评估增/删逻辑文字前后的规则性能，或增/删规则前后的规则集性能,从而判断是否要进行剪枝。</li><li>剪枝还可借统计显著性检验来进行：<br>方法：CN2算法[Clark and Niblett,1989]：在预剪枝时，假设用规则集进行预测必须显著优于直接基于训练样例集后验概率分布进行预测。<br>CN2使用了似然率统计量(LikelihoodRatio Statistics,简称LRS)。<br><img src="https://img-blog.csdnimg.cn/20191014095348802.png" alt="LRS"><br>这实际上是一种信息量指标，衡量了规则(集)覆盖样例的分布与训练集经验分布的差别：<br>LRS越大，说明采用规则(集)进行预测与直接使用训练集正、反例比率进行猜测的差别越大；<br>LRS越小，说明规则(集)的效果越可能仅是偶然现象。<br>在数据量比较大的现实任务中，通常设置为在LRS很大(例如0.99)时CN2算法才停止规则(集)生长。</li><li>后剪枝最常用的策略是“减错剪枝”(Reduced Error Pruning,简称REP)：<br>其基本做法是：将样例集划分为训练集和验证集，从训练集上学得规则集R后进行多轮剪枝，在每一轮穷举所有 可能的剪枝操作，包括删除规则中某个文字、删除规则结尾文字、删除规则尾部多个文字、删除整条规则等，然后用验证集对剪枝产生的所有候选规则集进行评估保留最好的那个规则集进行下一轮剪枝，如此继续，直到无法通过剪枝提高验证集上的性能为止。</li><li>REP：复杂度是O(m^4)<br>IREP (Incremental REP) ：复杂度O(mlog2m)<br>做法：在生成每条规则前，先将当前样例集划分为训练集和验证集，在训练集上生成一条规则r，立即在验证集上对其进行REP剪枝，得到规则r，将r覆盖的样例去除，在更新后的样例集上重复上述过程。<br>REP是针对规则集进行剪枝，而IREP仅对单条规则进行剪枝，因此后者比前者更高效。</li><li>规则学习算法RIPPER：<br><img src="https://img-blog.csdnimg.cn/20191014100503395.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="规则学习算法RIPPER"></li><li>RIPPER中的后处理机 制是为了在剪枝的基础上进一步提升性能。对R中的每条规则ri,，RIPPER为它产生两个变体：<br><img src="https://img-blog.csdnimg.cn/20191014100837408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="RIPPER中的后处理机 制"></li></ol><h1 id="15-4-一阶规则学习"><a href="#15-4-一阶规则学习" class="headerlink" title="15.4 一阶规则学习"></a>15.4 一阶规则学习</h1><ol start="24"><li>关系数据：直接描述样例间的关系。</li><li>背景知识：由原样本属性转化而来的“色泽更深”“根蒂更蜷”等原子公式。</li><li>由样本类别转化而来的关于“更好”“﹁更好”的原子公式。</li><li>FOIL (First-Order Inductive Learner) ：是著名的一阶规则学习算法，它遵循序贯覆盖框架且采用自顶向下的规则归纳策略。</li><li>FOIL使用“FOIL增益”（FOIL gain）来选择文字：<br><img src="https://img-blog.csdnimg.cn/20191014103238284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="FOIL增益"></li></ol><h1 id="15-5-归纳逻辑程序设计"><a href="#15-5-归纳逻辑程序设计" class="headerlink" title="15.5 归纳逻辑程序设计"></a>15.5 归纳逻辑程序设计</h1><ol start="29"><li>归纳逻辑程序设计(Inductive Logic Programming,简称ILP)：<br>在一阶规则学习中引入了函数和逻辑表达式嵌套一方面，这使得机器学习系统具备了更为强大的表达能力；<br>另一方面，ILP可看作用机器学习技术来解决基于背景知识的逻辑程序(logic program)归纳，其学得的“规则”可被PROLOG等逻辑程序设计语言直接使用。<h2 id="15-5-1-最小一般泛化"><a href="#15-5-1-最小一般泛化" class="headerlink" title="15.5.1 最小一般泛化"></a>15.5.1 最小一般泛化</h2></li><li>归纳逻辑程序设计采用==自底向上==的规则生成策略，直接将一个或多个正例所对应的具体事实(grounded fact)作为初始规则，再对规则逐步进行泛化以增加其对样例的覆盖率。泛化操作可以是将规则中的常量替换为逻辑变量，也可以是删除规则体中的某个文字。</li><li>最小一般泛化(Least General Generalization,简称LGG) ：用于将“特殊”规则转变为更“一般”的规则。<h2 id="15-5-2-逆归结"><a href="#15-5-2-逆归结" class="headerlink" title="15.5.2 逆归结"></a>15.5.2 逆归结</h2></li><li>四种完备的逆归结操作：<br>以规则形式p←q等价地表达pV﹁q，并假定用小写字母表示逻辑文字、大写字母表示合取式组成的逻辑子句：<br><img src="https://img-blog.csdnimg.cn/20191014104323980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="逆归结"></li><li>“置换”(substitution)是用某些项来替换逻辑表达式中的变量。</li><li>“合一”(unification)是用一种变量置换令两个或多个逻辑表达式相等。</li><li>逆归结的一大特点：能够自动发明新谓词，这些谓词可能对应于样例属性和背景知识中不存在的新知识，对知识发现与精化有重要意义。</li><li>ILP系统通常先自底向上生成一组规则，然后再结合最小一般泛化与逆归结做进一步学习。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;15-1-基本概念&quot;&gt;&lt;a href=&quot;#15-1-基本概念&quot; class=&quot;headerlink&quot; title=&quot;15.1 基本概念&quot;&gt;&lt;/a&gt;15.1 基本概念&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;规则：机器学习中通常指语义明确、能描述数据分布所隐含的客观规律或领域概念
      
    
    </summary>
    
    
      <category term="西瓜书学习笔记" scheme="http://mengyashen.github.io/categories/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://mengyashen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="西瓜书" scheme="http://mengyashen.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书学习笔记 | 第14章  概率图模型</title>
    <link href="http://mengyashen.github.io/5028c23a/"/>
    <id>http://mengyashen.github.io/5028c23a/</id>
    <published>2019-10-11T02:56:51.000Z</published>
    <updated>2019-10-15T03:05:08.749Z</updated>
    
    <content type="html"><![CDATA[<p>概率图模型（probabilistic model）（变量关系图）：是一类用图来表达变量相关关系的概率模型。<br>一个结点：表示一个或一组随机变量。<br>边：表示变量间的概率相关关系。<br><img src="https://img-blog.csdnimg.cn/2019101020244613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="概率图模型"></p><h1 id="14-1-隐马尔科夫模型"><a href="#14-1-隐马尔科夫模型" class="headerlink" title="14.1 隐马尔科夫模型"></a>14.1 隐马尔科夫模型</h1><ol><li>概率模型：将学习任务归结于计算变量的概率分布。</li><li>推断：利用已知变量推测未知变量的条件分布。</li><li>“生成式”模型：考虑联合概率分布P(Y,R|O)。<br>“判别式”模型：考虑条件分布P(Y|O)。<br>（Y：变量集合O：可观测变量集合 R：其他变量的集合）</li><li>隐马尔科夫模型（Hidden Markov Model：简称HMM）：是结构最简单的动态贝叶斯网，有向图模型，==生成式模型==。</li><li>隐马尔科夫模型中的变量：状态变量（隐变量）、观测变量。<br><img src="https://img-blog.csdnimg.cn/20191011101045175.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="隐马尔科夫模型的图结构"></li><li>状态（离散空间）空间：状态变量的取值范围。</li><li>马尔科夫链（Markov chain）：系统下一时刻的状态仅有当前状态决定，不依赖于以往的任何状态。基于这种关系，所有变量的联合概率分布为：<br><img src="https://img-blog.csdnimg.cn/20191011101228817.png" alt="联合概率分布"></li><li>确定一个隐马尔科夫模型需要以下三组参数：<br><img src="https://img-blog.csdnimg.cn/20191011101513955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="隐马尔科夫模型的三组参数">)<img src="https://img-blog.csdnimg.cn/20191011101605221.png" alt="隐马尔科夫模型的三组参数"></li><li>产生观测序列的过程：<br><img src="https://img-blog.csdnimg.cn/20191011102205287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="产生观测序列的过程"></li><li>在实际应用中，人们常关注隐马尔科夫模型的三个基本问题：<br><img src="https://img-blog.csdnimg.cn/20191011102308760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="隐马尔科夫模型的三个基本问题"><h1 id="14-2-马尔可夫随机场"><a href="#14-2-马尔可夫随机场" class="headerlink" title="14.2 马尔可夫随机场"></a>14.2 马尔可夫随机场</h1></li><li>马尔可夫随机场（Markov Random Field，简称MRF）：是典型的马尔可夫网，无向图模型，==生成式模型==。</li><li>势函数（potential functions）：亦称”因子“（factor），是定义在变量子集上的非负实函数，主要用于定义概率分布函数。<br><img src="https://img-blog.csdnimg.cn/20191011103113776.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="马尔科夫随机场"></li><li>团（clique）：对于上图中节点的一个子集，若其中任意两结点间都有边连接，则该结点子集为一个“团”。</li><li>极大团（maximal clique）：在一个团中加入任何一个结点都不再形成团。不能被其他团所包含的团。</li><li>马尔可夫随机场中，多个变量之间的联合概率定义（基于团）：<br><img src="https://img-blog.csdnimg.cn/20191011104022220.png" alt="联合概率定义"><br>所有团构成的集合为C，团Q∈C。<br><img src="https://img-blog.csdnimg.cn/20191011104059826.png" alt="联合概率定义">)<img src="https://img-blog.csdnimg.cn/20191011104123786.png" alt="联合概率定义"></li><li>马尔可夫随机场中，多个变量之间的联合概率定义（基于极大团）：<br><img src="https://img-blog.csdnimg.cn/2019101111015679.png" alt="联合概率定义"><br><img src="https://img-blog.csdnimg.cn/20191011110249976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="联合概率定义"> </li><li>分离集（separating set）：<br><img src="https://img-blog.csdnimg.cn/20191011110440254.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="分离">如图，从结点集A中的结点到B中的结点都必须经过结点集C中的结点，则称结点集A和B被结点集C分离，C成为“分离集”。</li><li>全局马尔可夫性（global Markov property）：给定两个变量子集的分离集，则这两个变量子集条件独立。</li><li>由全局马尔可夫性得到的两个很有用的推论：<br>局部马尔可夫性（local Markov property）：给定某变量的邻接变量，则该变量条件独立于其他变量。<br>成对马尔可夫性（pairwise Markov property）：给定所有其他变量，两个非邻接变量条件独立。</li><li>马尔可夫随机场中的势函数：<br>作用：定量刻画变量集中变量之间的相关关系，它是非负函数，且在所偏好的变量取值上有较大函数值。<br>为了满足非负性，指数函数常被用于定义势函数：<br><img src="https://img-blog.csdnimg.cn/20191011111723215.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="势函数"><h1 id="14-3-条件随机场"><a href="#14-3-条件随机场" class="headerlink" title="14.3 条件随机场"></a>14.3 条件随机场</h1></li><li>条件随机场（Conditional Random Field，简称CRF）：是一种==判别式==无向图模型。</li><li>条件随机场试图对多个变量在给定观测值后的条件概率进行建模。在自然语言处理的词性标注任务中，观测数据为语句（即单词序列），标记为相应的词性序列，具有线性序列结构；在语法分析任务中，输出标记则是语法树，具有树形结构。<br><img src="https://img-blog.csdnimg.cn/20191011144549554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="自然语言处理词性标注任务和语法分析任务"></li><li>条件随机场定义：<br><img src="https://img-blog.csdnimg.cn/20191011144704353.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="条件随机场"></li><li>链式条件随机场（chain-structured CRF）：<br><img src="https://img-blog.csdnimg.cn/20191011145023958.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="链式条件随机场"><br>条件随机场使用势函数和图结构上的团来定义条件概率P（y|x）。<br>链式条件随机场主要包含两种关于标记变量的团：单个标记变量和相邻的标记变量。</li><li>在条件随机场中，通过选用指数势函数并引入特征函数，条件概率被定义为：<img src="https://img-blog.csdnimg.cn/20191011145548989.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="条件概率"></li><li>特征函数：特征函数通常是实值函数，以刻画数据的一些很可能成立或期望成立的经验特征。<br>例：14.5（a）的词性标注，若采用转移特征函数：<br><img src="https://img-blog.csdnimg.cn/2019101115015121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="特征函数"></li><li>条件随机场和马尔可夫随机场均使用团上的势函数定义概率，两者在形式上没有显著差别；但条件随机场处理的是条件概率，而马尔科夫随机场处理的是联合概率。</li></ol><h1 id="14-4-学习与推断"><a href="#14-4-学习与推断" class="headerlink" title="14.4 学习与推断"></a>14.4 学习与推断</h1><ol start="28"><li><p>边际分布：指对无关变量求和或积分后得到结果。</p></li><li><p>边际化：对联合分布中其他无关变量进行积分的过程。</p></li><li><p>参数估计（参数学习问题）：对概率图模型，确定具体分布的参数。通常使用极大似然估计或最大后验概率估计求解。</p></li><li><p>推断问题的目标和关键：<br><img src="https://img-blog.csdnimg.cn/20191011151601976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="推断问题的目标和关键"></p></li><li><p>概率图模型的推断方法大致可分为两类：<br>第一类：精确推断方法——变量消去、信念传播。<br>第二类：近似推断方法——MCMC采样、变分推断。</p><h2 id="14-4-1-变量消去"><a href="#14-4-1-变量消去" class="headerlink" title="14.4.1 变量消去"></a>14.4.1 变量消去</h2></li><li><p>精确推断的实质是一类动态规划算法，它利用图模型所描述的条件独立行来削减计算目标概率值所需要的计算量。</p></li><li><p>有向图模型：<br><img src="https://img-blog.csdnimg.cn/20191011152107749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="变量消去法及其对应的消息传递过程">)<img src="https://img-blog.csdnimg.cn/20191011152318518.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="变量消去法-有向图模型">)<img src="https://img-blog.csdnimg.cn/20191011152536381.png" alt="变量消去法-有向图模型"></p></li><li><p>无向图模型：上述方法对无向图模型同样适用，忽略掉图14.7（a）中的箭头，将其看作一个无向图模型，有<br><img src="https://img-blog.csdnimg.cn/20191011152723338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="变量消去法-无向图模型"></p></li><li><p>变量消去法有一个明显的缺点：若需计算多个边际分布，重复使用变量消去法将会造成大量的冗余计算。</p><h2 id="14-4-2-信念传播"><a href="#14-4-2-信念传播" class="headerlink" title="14.4.2 信念传播"></a>14.4.2 信念传播</h2></li><li><p>信念传播（Belief Propagation）算法：将变量消去法中的求和操作看作一个消息传递过程，较好地解决了求解多个边际分布时的重复计算问题。<br><img src="https://img-blog.csdnimg.cn/201910111532423.png" alt="信念传播算法"><br><img src="https://img-blog.csdnimg.cn/20191011153340998.png" alt="信念传播算法"></p></li><li><p>在信念传播算法中，一个结点仅在接收到来自其他所有结点的消息后才能向另一个结点发送消息，且结点的边际分布正比于它所接收的消息的乘积，即<br><img src="https://img-blog.csdnimg.cn/20191011153729293.png" alt="结点的边际分布"></p></li><li><p>若图结构中没有环，则信念传播算法经过两个步骤即可完成所有消息传递，进而能计算所有变量上的边际分布：</p><p>  ●指定一个根结点，从所有叶结点开始向根结点传递消息，直到根结点收到</p><p>  所有邻接结点的消息; </p><p>  ●从根结点开始向叶结点传递消息，直到所有叶结点均收到消息。<br>例：<img src="https://img-blog.csdnimg.cn/20191011153958170.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="信念传播算法图示"></p><h1 id="14-5-近似推断"><a href="#14-5-近似推断" class="headerlink" title="14.5 近似推断"></a>14.5 近似推断</h1></li><li><p>近似推断方法大致可分为两大类：<br>第一类：采样(sampling)，通过使用随机化方法完成近似；<br>第二类：使用确定性近似完成近似推断，典型代表为变分推断(variational inference)。</p><h2 id="14-5-1-MCMC采样"><a href="#14-5-1-MCMC采样" class="headerlink" title="14.5.1 MCMC采样"></a>14.5.1 MCMC采样</h2></li><li><p>采样思路：<br>假设我们的目标是计算函数f(x)在概率密度函数p(x)下的期望<br><img src="https://img-blog.csdnimg.cn/20191011154659942.png" alt="期望"><br><img src="https://img-blog.csdnimg.cn/2019101115480053.png" alt="均值">以此来近似目标期望E[f]。</p></li><li><p>概率图模型中最常用的采样技术是马尔可夫链蒙特卡罗(Markov ChainMonte Carlo,简称MCMC)方法。<br>给定连续变量x∈X的概率密度函数p(x)，x在区间A中的概率可计算为：<br><img src="https://img-blog.csdnimg.cn/20191011155112627.png" alt="概率"><br>若有函数f:X→R，则可计算f(x)的期望<br><img src="https://img-blog.csdnimg.cn/2019101115514724.png" alt="期望"></p></li><li><p>计算方法：MCMC先构造出服从p分布的独立同分布随机变量X1,2….. xN，再得到上式的无偏估计<br><img src="https://img-blog.csdnimg.cn/20191011155348570.png" alt="无偏估计"></p></li><li><p>MCMC方法的关键：通过构造“平稳分布为p的马尔可夫链”来产生样本：若马尔可夫链运行时间足够长(即收敛到平稳状态)，则此时产出的样本x近似服从于分布p。</p></li><li><p>如何判断马尔可夫链到达平稳状态呢？<br>假定平稳马尔可夫链T的状态转移概率(即从状态x转移到状态x’的概率)为T(x’ |x)，t时刻状态的分布为p(x*)，则若在某个时刻马尔可夫链满足平稳条件<br><img src="https://img-blog.csdnimg.cn/20191011155818295.png" alt="平稳条件"><br>则p(x)是该马尔可夫链的平稳分布，且马尔可夫链在满足该条件时已收敛到平稳状态。</p></li><li><p>MCMC方法：先设法构造一条马尔可夫链，使其收敛至平稳分布恰为待估计参数的后验分布，然后通过这条马尔可夫链来产生符合后验分布的样本，并基于这些样本来进行估计。</p></li><li><p>Metropolis Hastings (简称MH)算法（MCMC的重要代表）：它基于“拒绝采样”(reject sampling)来逼近平稳分布p。</p></li><li><p><img src="https://img-blog.csdnimg.cn/20191011160246440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="Metropolis Hastings 算法">为达到平稳状态，只需要将接受率设置为：<br><img src="https://img-blog.csdnimg.cn/20191011160445388.png" alt="接受率"></p></li><li><p>吉布斯采样(Gibbs sampling)：有时被视为MH算法的特例，它也使用马尔可夫链获取样本，而该马尔可夫链的平稳分布也是采样的目标分布p(x)。<br>采样步骤：<br>(1)随机或以某个次序选取某变量xi；<br>(2) 根据x中除xi外的变量的现有取值,计算条件概率p(xi | x)，其中<br>xq= {x1,W2…,Xi -1,Ti+1,…,xN} ；<br>(3)根据p(xi I xq)对变量xi采样，用采样值代替原值。</p></li></ol><h2 id="14-5-2-变分推断"><a href="#14-5-2-变分推断" class="headerlink" title="14.5.2 变分推断"></a>14.5.2 变分推断</h2><ol start="50"><li>变分推断：通过使用已知简单分布来逼近需推断的复杂分布，并通过限制近似分布的类型，从而得到种局部最优、但具有确定解的近似后验分布。</li><li>盘式记法(plate notation) ：概率图模型一种简洁的表示方法。<br><img src="https://img-blog.csdnimg.cn/20191011160955969.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZW5fX18=,size_16,color_FFFFFF,t_70" alt="盘式记法"><h1 id="14-6-话题模型"><a href="#14-6-话题模型" class="headerlink" title="14.6 话题模型"></a>14.6 话题模型</h1></li><li>话题模型(topic model)：是一族==生成式==有向图模型，主要用于处理离散型的数据(如文本集合)，在信息检索、自然语言处理等领域有广泛应用。</li><li>隐狄利克雷分配模型(Latent Dirichlet Allocation, 简称LDA)：话题模型的典型代表。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;概率图模型（probabilistic model）（变量关系图）：是一类用图来表达变量相关关系的概率模型。&lt;br&gt;一个结点：表示一个或一组随机变量。&lt;br&gt;边：表示变量间的概率相关关系。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2
      
    
    </summary>
    
    
      <category term="西瓜书学习笔记" scheme="http://mengyashen.github.io/categories/%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://mengyashen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="西瓜书" scheme="http://mengyashen.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>利用GitHub+hexo从零开始搭建个人博客</title>
    <link href="http://mengyashen.github.io/a13da74f/"/>
    <id>http://mengyashen.github.io/a13da74f/</id>
    <published>2019-10-08T09:49:53.000Z</published>
    <updated>2019-10-13T11:56:50.142Z</updated>
    
    <content type="html"><![CDATA[<p>作为一个技术小白，一直想搭建一个自己的博客，但是不知道从哪里开始。偶然间知道GitHub可以搭建博客，并且是免费的。所以抱着学习的态度，开始了自己第一个博客的搭建。整个搭建过程真的很简单，网上的很多教程也都说的非常详细，所以我的这篇博客主要来记录一下我在搭建过程中参考的一些文章，并且自己遇到的一些坑，下面就开始啦~</p><h2 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h2><p>1.注册一个GitHub账号<br>2.安装Git<br>3.安装Node.js<br>4.安装Hexo</p><h2 id="二、参考文章"><a href="#二、参考文章" class="headerlink" title="二、参考文章"></a>二、参考文章</h2><p>整个搭建过程可以参考：<a href="https://www.cnblogs.com/MuYunyun/p/5927491.html#_label6" target="_blank" rel="noopener">基于hexo+github搭建一个独立博客</a>.此博主写的非常详细，也填补了一些坑。<br>此外还有：</p><h3 id="1-GitHub相关："><a href="#1-GitHub相关：" class="headerlink" title="1.GitHub相关："></a>1.GitHub相关：</h3><p><a href="https://www.jianshu.com/p/4f56cf990bba" target="_blank" rel="noopener">使用Github搭建属于自己的博客</a>.<br> <a href="https://www.jianshu.com/p/af9737d770bb" target="_blank" rel="noopener">利用github搭建hexo个人静态博客网站</a>.<br><a href="https://blog.csdn.net/qq_32558577/article/details/79579286" target="_blank" rel="noopener">GitHub的注册与使用</a>.<br><a href="https://www.cnblogs.com/best/p/7474442.html" target="_blank" rel="noopener">一个小时学会Git</a>.</p><h3 id="2-Hexo相关："><a href="#2-Hexo相关：" class="headerlink" title="2.Hexo相关："></a>2.Hexo相关：</h3><p><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">hexo安装文档</a>.<br><a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">NexT使用文档</a>.<br><a href="https://blog.csdn.net/weixin_44815733/article/details/88786480" target="_blank" rel="noopener">使用hexo发布文章</a>.<br><a href="https://www.jianshu.com/p/e17711e44e00" target="_blank" rel="noopener">Hexo使用攻略-添加分类及标签</a>.<br><a href="https://segmentfault.com/a/1190000009009697" target="_blank" rel="noopener">hexo高阶教程：next主题优化之加入网易云音乐、网易云跟帖、炫酷动态背景、自定义样式，打造属于你自己的定制化博客</a>.<br><a href="https://blog.csdn.net/ganzhilin520/article/details/79047992" target="_blank" rel="noopener">hexo博客安装RSS插件</a>.<br><a href="https://blog.csdn.net/q2158798/article/details/82354154" target="_blank" rel="noopener">hexo博客常用插件及教程</a>.<br><a href="https://albenw.github.io/posts/be8242cc/" target="_blank" rel="noopener">hexo高级玩法</a>.</p><h3 id="3-SEO相关："><a href="#3-SEO相关：" class="headerlink" title="3.SEO相关："></a>3.SEO相关：</h3><p><a href="https://blog.csdn.net/weixin_43307658/article/details/86629586" target="_blank" rel="noopener">hexo搭建个人博客之seo优化</a>.<br><a href="https://www.jianshu.com/p/c20bb9df1867?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation" target="_blank" rel="noopener">Hexo博客之后续SEO优化</a>.</p><h3 id="4-Markdown相关："><a href="#4-Markdown相关：" class="headerlink" title="4.Markdown相关："></a>4.Markdown相关：</h3><p><a href="https://www.jianshu.com/p/191d1e21f7ed" target="_blank" rel="noopener">Markdown基本语法</a>.<br><a href="https://blog.csdn.net/qq_36759224/article/details/82229243" target="_blank" rel="noopener">最新主流 Markdown 编辑器推荐</a>.</p><h3 id="5-遇到的一些坑："><a href="#5-遇到的一些坑：" class="headerlink" title="5.遇到的一些坑："></a>5.遇到的一些坑：</h3><p><a href="https://blog.csdn.net/niu9799/article/details/80755895" target="_blank" rel="noopener">Git之右键没有Git Bash Here的解决办法</a>.<br><a href="https://blog.csdn.net/weixin_44815733/article/details/88786480" target="_blank" rel="noopener">hexo博客不蒜子统计不显示/无效</a>.<br><a href="https://blog.csdn.net/bohe_/article/details/98379247" target="_blank" rel="noopener">在搭建blog所遇到的问题</a>.<br><a href="https://www.jianshu.com/p/ba2ad6fb80e7" target="_blank" rel="noopener">Hexo生成sitemap步骤及duplicated mapping key 问题解决</a>.<br><a href="https://segmentfault.com/a/1190000009526905" target="_blank" rel="noopener">git warning: LF will be replaced by CRLF in 解决办法</a>.</p><h2 id="三、小小总结"><a href="#三、小小总结" class="headerlink" title="三、小小总结"></a>三、小小总结</h2><p>搭好博客后第一篇文章就到这里啦，其实也算不上什么技术原创文章，只是简单把自己看过的文章做个小小整理和记录。之所以搭建这个博客，主要还是想把自己的学习过程记录下来，也可以作为自己的小树洞，记录一下生活~既然有了自己的小博客，希望自己以后可以常常更新。技术小白也要更加努力的学技术，虽然现在的自己对未来还有一些迷茫，但是希望在学习过程中尽快找到方向，并且向大佬们看齐！加油^^</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作为一个技术小白，一直想搭建一个自己的博客，但是不知道从哪里开始。偶然间知道GitHub可以搭建博客，并且是免费的。所以抱着学习的态度，开始了自己第一个博客的搭建。整个搭建过程真的很简单，网上的很多教程也都说的非常详细，所以我的这篇博客主要来记录一下我在搭建过程中参考的一些
      
    
    </summary>
    
    
      <category term="GitHub" scheme="http://mengyashen.github.io/categories/GitHub/"/>
    
    
      <category term="GitHub" scheme="http://mengyashen.github.io/tags/GitHub/"/>
    
      <category term="Hexo" scheme="http://mengyashen.github.io/tags/Hexo/"/>
    
      <category term="Git" scheme="http://mengyashen.github.io/tags/Git/"/>
    
      <category term="Node" scheme="http://mengyashen.github.io/tags/Node/"/>
    
      <category term="SEO" scheme="http://mengyashen.github.io/tags/SEO/"/>
    
      <category term="Markdown" scheme="http://mengyashen.github.io/tags/Markdown/"/>
    
  </entry>
  
</feed>
